{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bea84e79-0997-4ab4-9696-fa47759f57fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This produces the dataframe for WR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116d1fd-1d2f-4aeb-9bcf-6d2db301ba5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904d5606-1a3f-47bb-8796-070d85227a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Notes on the NFL Library ##\n",
    "# the NFL python library seem to not work on Tuesday probably due to updates (not confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952dbb5-f535-4006-8759-671a554013b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca94a30-0ba3-49a4-9233-e4803a77d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REQUIRED ACTIONS - Include in a README doc ## \n",
    "# modify the number of weeks if the NFL adds regular season games to the schedule\n",
    "# Update the season start date each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e0f76d-0b7e-4232-b986-9577eece112c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5a58d-2211-4950-bf86-80be095213b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REQUIRED ACTIONS - Include in a README doc ## \n",
    "# ensure the output directory exists ./csv_files for the csv file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2e9736-3bb9-471e-8b2a-fd5da8c60a6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c74841-5502-416d-87a3-fd4ecaeed211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nfl_data_py in /home/rashawn/anaconda3/lib/python3.11/site-packages (0.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from nfl_data_py) (1.26.4)\n",
      "Requirement already satisfied: pandas<2.0,>=1.0 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from nfl_data_py) (1.5.3)\n",
      "Requirement already satisfied: appdirs>1 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from nfl_data_py) (1.4.4)\n",
      "Requirement already satisfied: fastparquet>0.5 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from nfl_data_py) (2024.11.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from fastparquet>0.5->nfl_data_py) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /home/rashawn/anaconda3/lib/python3.11/site-packages (from fastparquet>0.5->nfl_data_py) (2023.10.0)\n",
      "Requirement already satisfied: packaging in /home/rashawn/anaconda3/lib/python3.11/site-packages (from fastparquet>0.5->nfl_data_py) (23.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from pandas<2.0,>=1.0->nfl_data_py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from pandas<2.0,>=1.0->nfl_data_py) (2023.3.post1)\n",
      "Requirement already satisfied: six>=1.5 in /home/rashawn/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas<2.0,>=1.0->nfl_data_py) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "## Required installations\n",
    "!pip install nfl_data_py\n",
    "# Ensure all required packages are installed within the notebook\n",
    "# !pip install --quiet nfl_data_py\n",
    "!pip install --quiet rapidfuzz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab3e9b-564c-461c-babf-66d530847c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628f591e-8886-40c7-9890-cc493f84c094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime, timedelta\n",
    "import nfl_data_py as nfl\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from random import sample, uniform, seed\n",
    "import io\n",
    "from rapidfuzz import fuzz, process\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a188f-2afa-40d4-842d-5fed4cf7f904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2fd2a97-7c06-492a-be81-fb87c73a53a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Pandas options to display all columns in a single row without wrapping\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58770d68-03c0-4415-9dc3-0e436997e3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b990a51-33d5-411d-99f2-fdf868ba49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin: time calculators ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04be5836-83ab-43ca-beaa-ff9121c0d758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify the number of weeks if the NFL adds regular season games to the schedule\n",
    "# Update this each year\n",
    "season_start_date = datetime(2025, 9, 4)  \n",
    "REG_WEEKS = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b1744-40ba-4702-a55d-812649aafdcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "608a91f8-a4dc-45ab-b7fa-a2ddadb07874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_week(today=None):\n",
    "    if today is None:\n",
    "        today = datetime.now()\n",
    "    delta_days = (today.date() - season_start_date.date()).days\n",
    "    week_num = (delta_days // 7) + 1\n",
    "    return max(0, week_num)  # clamp to 0 for preseason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b51bde3-0c0d-405a-be91-4bf66c1a2a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68031520-4c3d-4791-bec9-aa0eb654c706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_year: 2025\n",
      "current_week: 0\n",
      "season_type: 1\n"
     ]
    }
   ],
   "source": [
    "# 1=preseason, 2=regular, 3=playoffs\n",
    "def get_season_type(current_week, reg_weeks=REG_WEEKS):\n",
    "    if current_week == 0:\n",
    "        return 1\n",
    "    elif current_week <= reg_weeks:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "current_year = season_start_date.year\n",
    "current_week = get_current_week()\n",
    "season_type = get_season_type(current_week, REG_WEEKS)\n",
    "\n",
    "print(\"current_year:\", current_year)\n",
    "print(\"current_week:\", current_week)\n",
    "print(\"season_type:\", season_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cadbf42-08d1-48fe-9965-254b73f7f074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67c3e0f5-e4a1-41b5-a9b5-a7aba520e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a list of years to pull.\n",
    "def get_year_range(current_year, current_week, start_year=2017, reg_weeks=18):\n",
    "    if current_week == 0:\n",
    "        return list(range(start_year, current_year))\n",
    "    else:\n",
    "        return list(range(start_year, current_year + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e5cd19-8c62-41f7-b6f3-204e544d186b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e4905a-5f1f-49cf-bf6f-2b3c0d7215d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds (year, week) pairs for scraping.\n",
    "# - 2017â€“2020: weeks 1â€“17\n",
    "# - 2021+: weeks 1â€“18\n",
    "def generate_year_week_combinations(start_year, end_year, current_year=None, current_week=None):\n",
    "    combos = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        max_regular = 17 if year <= 2020 else 18\n",
    "\n",
    "        # Handle the current year\n",
    "        if current_year is not None and year == current_year:\n",
    "            if current_week is None or current_week == 0:\n",
    "                # preseason: don't add any weeks for this year\n",
    "                continue\n",
    "            upper = min(max_regular, int(current_week))\n",
    "        else:\n",
    "            upper = max_regular\n",
    "\n",
    "        combos.extend([(year, wk) for wk in range(1, upper + 1)])\n",
    "    return combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53ffdd-bf0c-4c34-9e51-dbe7152ff9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d39b4cbc-1783-4084-a9a7-fd289b8160c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the year, week, and season type\n",
    "current_year = season_start_date.year\n",
    "current_week = get_current_week()\n",
    "season_type  = get_season_type(current_week, REG_WEEKS)\n",
    "\n",
    "years = get_year_range(current_year, current_week, start_year=2017)\n",
    "year_week_pairs = generate_year_week_combinations(\n",
    "    start_year=years[0] if years else 2017,\n",
    "    end_year=years[-1] if years else current_year - 1,\n",
    "    current_year=current_year,\n",
    "    current_week=current_week\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b762664-88b1-4f34-a736-1f2cefbf2b8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "095a8c4e-00ce-4229-9a1d-1de78665beca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years: [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "contains current_year? False\n",
      "years in pairs: [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "pairs count: 140\n",
      "first 5: [(2017, 1), (2017, 2), (2017, 3), (2017, 4), (2017, 5)]\n",
      "last 5: [(2024, 14), (2024, 15), (2024, 16), (2024, 17), (2024, 18)]\n",
      "week-cap violations: []\n",
      "current year present in pairs? False\n"
     ]
    }
   ],
   "source": [
    "# test years and weeks to pull\n",
    "\n",
    "# Years list should exclude current year during preseason\n",
    "print(\"years:\", years)                      # expect no 2025 when current_week == 0\n",
    "print(\"contains current_year?\", current_year in years)\n",
    "\n",
    "# Year-week pairs should have no current_year and valid week caps\n",
    "yrs_in_pairs = sorted({y for (y, _) in year_week_pairs})\n",
    "print(\"years in pairs:\", yrs_in_pairs)\n",
    "print(\"pairs count:\", len(year_week_pairs))\n",
    "print(\"first 5:\", year_week_pairs[:5])\n",
    "print(\"last 5:\", year_week_pairs[-5:])\n",
    "\n",
    "# Validate week caps per year (â‰¤17 for <=2020, â‰¤18 otherwise)\n",
    "violations = []\n",
    "for y in yrs_in_pairs:\n",
    "    max_reg = 17 if y <= 2020 else 18\n",
    "    max_week = max(w for (yy, w) in year_week_pairs if yy == y)\n",
    "    if max_week > max_reg:\n",
    "        violations.append((y, max_week, max_reg))\n",
    "print(\"week-cap violations:\", violations)   # expect []\n",
    "\n",
    "# Ensure current year is COMPLETELY absent during preseason\n",
    "has_current_year = any(yy == current_year for (yy, _) in year_week_pairs)\n",
    "print(\"current year present in pairs?\", has_current_year)  # expect False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b10d6339-c653-4173-b303-5126d978c807",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End: time calculators ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab46f29-61cc-42fb-952f-7e8b651b3b1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e8308e3-cd2c-4b45-8e37-6297a8dc15c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dataframe summarizing missing values for a given dataFrame.\n",
    "def check_nulls(df, name=None):\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_percent = (null_counts / len(df)).round(4)\n",
    "    summary = pd.DataFrame({\n",
    "        'Missing Count': null_counts,\n",
    "        'Missing %': null_percent\n",
    "    })\n",
    "    summary = summary[summary['Missing Count'] > 0].sort_values(by='Missing %', ascending=False)\n",
    "    \n",
    "    if name:\n",
    "        print(f\"\\nðŸ“Š Missing Value Summary for: {name}\")\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb4853-ebc5-4de7-8dce-2270088ec150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5615a35d-e100-4192-aedf-e506134f103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a dataFrame to the ./csv_files directory with the given filename\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(\"./csv_files\", exist_ok=True)\n",
    "\n",
    "def save_csv(df, filename, index=False, float_format=None):\n",
    "    if not filename.endswith('.csv'):\n",
    "        filename += '.csv'\n",
    "    path = os.path.join(\"./csv_files\", filename)\n",
    "    df.to_csv(path, index=index, float_format=float_format)\n",
    "    print(f\"Saved: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d44a4-e589-4a3f-b3c4-d5076deab859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7d23f7e-e735-40aa-935d-c8a775aedd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: Python NFL Library Dataframe ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd14ff10-c4d2-4e80-a11f-382f0a75f354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "years: [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "Downcasting floats.\n",
      "     season  week\n",
      "min    2017     1\n",
      "max    2024    22\n",
      "unique seasons: [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]\n",
      "week-cap violations: []\n"
     ]
    }
   ],
   "source": [
    "# Validate years to pull from the nfl library\n",
    "print(\"years:\", years)                         \n",
    "assert current_year not in years\n",
    "\n",
    "wr_weekly = nfl.import_weekly_data(years=years, downcast=True)\n",
    "print(wr_weekly[['season','week']].agg(['min','max']))\n",
    "print(\"unique seasons:\", sorted(wr_weekly['season'].unique()))\n",
    "\n",
    "# sanity: no week beyond league cap per year\n",
    "violations = []\n",
    "for y, g in wr_weekly.groupby('season'):\n",
    "    # Regular season cap: 17 weeks (<=2020) or 18 weeks (>=2021)\n",
    "    # Postseason cap: up to week 22 (including Super Bowl)\n",
    "    max_allowed = 22\n",
    "    max_week = int(g['week'].max())\n",
    "    if max_week > max_allowed:\n",
    "        violations.append((y, max_week, f\"> {max_allowed} not allowed\"))\n",
    "print(\"week-cap violations:\", violations)  # expect []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904829e8-ef1c-41ac-bdd6-02c80a21520b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3df3a358-b9ac-4627-bb94-fab97eb48a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['player_id', 'player_name', 'player_display_name', 'position', 'position_group', 'headshot_url', 'recent_team', 'season', 'week', 'season_type', 'opponent_team', 'completions', 'attempts', 'passing_yards', 'passing_tds', 'interceptions', 'sacks', 'sack_yards', 'sack_fumbles', 'sack_fumbles_lost', 'passing_air_yards', 'passing_yards_after_catch', 'passing_first_downs', 'passing_epa', 'passing_2pt_conversions', 'pacr', 'dakota', 'carries', 'rushing_yards', 'rushing_tds', 'rushing_fumbles', 'rushing_fumbles_lost', 'rushing_first_downs', 'rushing_epa', 'rushing_2pt_conversions', 'receptions', 'targets', 'receiving_yards', 'receiving_tds', 'receiving_fumbles', 'receiving_fumbles_lost', 'receiving_air_yards', 'receiving_yards_after_catch', 'receiving_first_downs', 'receiving_epa', 'receiving_2pt_conversions', 'racr', 'target_share', 'air_yards_share', 'wopr', 'special_teams_tds', 'fantasy_points', 'fantasy_points_ppr'], dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display all available columns in the nfl python API for weekly stats\n",
    "nfl.see_weekly_cols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d920848c-25a0-4f61-8bd6-d8b4d14dc318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fd3a4ad-f7ac-4134-b9e9-e56edc8b2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the base columns. \n",
    "base_columns = [\n",
    "    'season', 'season_type', 'week', 'player_id', 'player_name', \n",
    "    'position', 'position_group', 'recent_team',\n",
    "    'fantasy_points', 'fantasy_points_ppr'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f066437-8739-4511-b32e-77c30a9b60d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a38cb6-6a92-49ed-ae69-76bb27434bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the player IDs from nfl.import_ids() - without parameters\n",
    "ids_data = nfl.import_ids()\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "columns_to_drop = [\n",
    "    'position', 'team', 'birthdate', 'age', 'draft_year', \n",
    "    'draft_round', 'draft_pick', 'draft_ovr', 'twitter_username', \n",
    "    'height', 'weight', 'college', 'db_season'\n",
    "]\n",
    "ids_data = ids_data.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Display the resulting dataframe for review\n",
    "# print(f\"Columns after dropping unnecessary ones: {ids_data.columns.tolist()}\")\n",
    "# display(ids_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3146d4c-21d8-4bb3-b15d-46db6ad6736a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e044f79-7a6f-4970-b266-06488893226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downcasting floats.\n"
     ]
    }
   ],
   "source": [
    "# import the weekly data from nfl.import_weekly_data(years, columns, downcast)\n",
    "weekly_data = nfl.import_weekly_data(\n",
    "    years=years,\n",
    "    columns=base_columns\n",
    ")\n",
    "\n",
    "# display(weekly_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad9bdb-b6ec-474e-9e77-a0fc324ab258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ff9936-4693-4cf5-996e-caa401c461b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output: a dataframe of ALL NFL athletes info and ids since 2017\n",
    "\n",
    "# Merge the two dataframes on 'player_id' and 'gsis_id'\n",
    "# Align column names for merging\n",
    "ids_data = ids_data.rename(columns={'gsis_id': 'player_id'})  \n",
    "id_dataframe = pd.merge(weekly_data, ids_data, on='player_id', how='inner')\n",
    "\n",
    "# Assign the resulting dataframe to a variable\n",
    "all_players_id_data = id_dataframe\n",
    "\n",
    "# Display the resulting ID dataframe\n",
    "# display(all_players_id_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b1d937-3087-464c-91e8-52d3786630f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92caf86a-7090-438a-b523-2a38c3d79926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of merged dataframe: (17384, 31)\n"
     ]
    }
   ],
   "source": [
    "## Output: a dataframe of NFL WR info and ids since 2017\n",
    "# extract WR from the dataframe\n",
    "# Create a new dataframe with only wide receivers\n",
    "wide_receiver_ids = all_players_id_data[all_players_id_data['position'] == 'WR']\n",
    "\n",
    "# Display the resulting dataframe for review\n",
    "print(f\"Shape of merged dataframe: {wide_receiver_ids.shape}\")\n",
    "\n",
    "# Display the resulting dataframe for review\n",
    "# display(wide_receiver_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c552a-8713-46b2-9628-b0469a77dc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f1b7ee-dc7d-407b-aecc-fdff81adae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output: a dataframe of NFL WR info, ids, and stats since 2017\n",
    "# WR-specific columns (receiving-related)\n",
    "wr_columns = [\n",
    "    'receptions', 'targets', 'receiving_yards', 'receiving_tds',\n",
    "    'receiving_fumbles', 'receiving_fumbles_lost',\n",
    "    'receiving_air_yards', 'receiving_yards_after_catch',\n",
    "    'receiving_first_downs', 'receiving_epa',\n",
    "    'receiving_2pt_conversions', 'racr', 'target_share',\n",
    "    'air_yards_share', 'wopr'\n",
    "]\n",
    "\n",
    "# Pull WR-specific columns from weekly data\n",
    "wr_stats = nfl.import_weekly_data(\n",
    "    years=years,\n",
    "    columns=['player_id', 'season', 'week'] + wr_columns  # Include keys for merging\n",
    ")\n",
    "\n",
    "# Merge WR-specific stats with wide_receiver_ids\n",
    "wr_ids_weekly_stats_df = pd.merge(\n",
    "    wide_receiver_ids,\n",
    "    wr_stats,\n",
    "    on=['player_id', 'season', 'week'],  # Ensure correct alignment\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Display the resulting dataframe for review\n",
    "print(f\"Shape of merged dataframe: {wr_ids_weekly_stats_df.shape}\")\n",
    "\n",
    "# Row integrity check\n",
    "print(\n",
    "    f\"Row count matches: {wr_ids_weekly_stats_df.shape[0] == wide_receiver_ids.shape[0]}\"\n",
    ")\n",
    "\n",
    "# display the df\n",
    "display(wr_ids_weekly_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2862d7ef-b76e-45fc-b5a6-b30322ce968e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b749a0-20a0-4922-9ee5-9f03d5e5b661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv file\n",
    "# save_csv(wr_ids_weekly_stats_df, \"wr_ids_weekly_stats_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a40e20-8860-4839-80df-d82ccd09fc6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6c93aee-bad3-4c1c-ad35-a110f7a94ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Missing Value Summary for: WR Weekly Stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>racr</th>\n",
       "      <td>327</td>\n",
       "      <td>0.0188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving_epa</th>\n",
       "      <td>285</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>air_yards_share</th>\n",
       "      <td>285</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target_share</th>\n",
       "      <td>285</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wopr</th>\n",
       "      <td>285</td>\n",
       "      <td>0.0164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Missing Count  Missing %\n",
       "racr                       327     0.0188\n",
       "receiving_epa              285     0.0164\n",
       "air_yards_share            285     0.0164\n",
       "target_share               285     0.0164\n",
       "wopr                       285     0.0164"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for nulls\n",
    "# updated null value analysis using helper function\n",
    "null_summary_wr_ids_weekly = check_nulls(wr_ids_weekly_stats_df, name=\"WR Weekly Stats\")\n",
    "\n",
    "# Filter out columns containing '_id'\n",
    "null_summary_wr_ids_weekly = null_summary_wr_ids_weekly[~null_summary_wr_ids_weekly.index.str.contains('_id')]\n",
    "\n",
    "display(null_summary_wr_ids_weekly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3dd34-0e3d-4652-8067-0b7b18027270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4014cd1-125c-4606-b749-7fbfc00cf965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of NGS WR DataFrame after dropping columns: (8249, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>season_type</th>\n",
       "      <th>week</th>\n",
       "      <th>player_display_name</th>\n",
       "      <th>player_position</th>\n",
       "      <th>team_abbr</th>\n",
       "      <th>avg_cushion</th>\n",
       "      <th>avg_separation</th>\n",
       "      <th>avg_intended_air_yards</th>\n",
       "      <th>percent_share_of_intended_air_yards</th>\n",
       "      <th>receptions</th>\n",
       "      <th>targets</th>\n",
       "      <th>catch_percentage</th>\n",
       "      <th>yards</th>\n",
       "      <th>rec_touchdowns</th>\n",
       "      <th>avg_yac</th>\n",
       "      <th>avg_expected_yac</th>\n",
       "      <th>avg_yac_above_expectation</th>\n",
       "      <th>player_gsis_id</th>\n",
       "      <th>player_first_name</th>\n",
       "      <th>player_last_name</th>\n",
       "      <th>player_short_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1725</th>\n",
       "      <td>2017</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>Ryan Grant</td>\n",
       "      <td>WR</td>\n",
       "      <td>WAS</td>\n",
       "      <td>9.936667</td>\n",
       "      <td>2.894592</td>\n",
       "      <td>4.410000</td>\n",
       "      <td>7.154639</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.232500</td>\n",
       "      <td>10.072361</td>\n",
       "      <td>1.160139</td>\n",
       "      <td>00-0031068</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>Grant</td>\n",
       "      <td>R.Grant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1726</th>\n",
       "      <td>2017</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>Martavis Bryant</td>\n",
       "      <td>WR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>4.122054</td>\n",
       "      <td>12.688333</td>\n",
       "      <td>33.327496</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>4.098278</td>\n",
       "      <td>-3.943278</td>\n",
       "      <td>00-0031373</td>\n",
       "      <td>Martavis</td>\n",
       "      <td>Bryant</td>\n",
       "      <td>M.Bryant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>2017</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>Jamison Crowder</td>\n",
       "      <td>WR</td>\n",
       "      <td>WAS</td>\n",
       "      <td>7.655000</td>\n",
       "      <td>3.177793</td>\n",
       "      <td>10.540000</td>\n",
       "      <td>19.949707</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>1.631897</td>\n",
       "      <td>-0.181897</td>\n",
       "      <td>00-0031941</td>\n",
       "      <td>Jamison</td>\n",
       "      <td>Crowder</td>\n",
       "      <td>J.Crowder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>2017</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>Nelson Agholor</td>\n",
       "      <td>WR</td>\n",
       "      <td>PHI</td>\n",
       "      <td>7.423750</td>\n",
       "      <td>2.462620</td>\n",
       "      <td>10.463750</td>\n",
       "      <td>20.274656</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.611667</td>\n",
       "      <td>3.262470</td>\n",
       "      <td>2.349197</td>\n",
       "      <td>00-0031549</td>\n",
       "      <td>Nelson</td>\n",
       "      <td>Agholor</td>\n",
       "      <td>N.Agholor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>2017</td>\n",
       "      <td>REG</td>\n",
       "      <td>1</td>\n",
       "      <td>John Brown</td>\n",
       "      <td>WR</td>\n",
       "      <td>ARI</td>\n",
       "      <td>7.360000</td>\n",
       "      <td>2.751526</td>\n",
       "      <td>13.422222</td>\n",
       "      <td>28.208481</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.377500</td>\n",
       "      <td>0.961993</td>\n",
       "      <td>-1.339493</td>\n",
       "      <td>00-0031051</td>\n",
       "      <td>John</td>\n",
       "      <td>Brown</td>\n",
       "      <td>J.Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13323</th>\n",
       "      <td>2024</td>\n",
       "      <td>POST</td>\n",
       "      <td>23</td>\n",
       "      <td>Xavier Worthy</td>\n",
       "      <td>WR</td>\n",
       "      <td>KC</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>4.959113</td>\n",
       "      <td>14.276250</td>\n",
       "      <td>44.737358</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>157.0</td>\n",
       "      <td>2</td>\n",
       "      <td>6.250000</td>\n",
       "      <td>6.154624</td>\n",
       "      <td>0.095376</td>\n",
       "      <td>00-0039894</td>\n",
       "      <td>Xavier</td>\n",
       "      <td>Worthy</td>\n",
       "      <td>X.Worthy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13324</th>\n",
       "      <td>2024</td>\n",
       "      <td>POST</td>\n",
       "      <td>23</td>\n",
       "      <td>DeAndre Hopkins</td>\n",
       "      <td>WR</td>\n",
       "      <td>KC</td>\n",
       "      <td>7.676000</td>\n",
       "      <td>3.446231</td>\n",
       "      <td>11.974000</td>\n",
       "      <td>23.451761</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565000</td>\n",
       "      <td>0.798474</td>\n",
       "      <td>-0.233474</td>\n",
       "      <td>00-0030564</td>\n",
       "      <td>DeAndre</td>\n",
       "      <td>Hopkins</td>\n",
       "      <td>D.Hopkins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13325</th>\n",
       "      <td>2024</td>\n",
       "      <td>POST</td>\n",
       "      <td>23</td>\n",
       "      <td>DeVonta Smith</td>\n",
       "      <td>WR</td>\n",
       "      <td>PHI</td>\n",
       "      <td>7.470000</td>\n",
       "      <td>2.221577</td>\n",
       "      <td>14.752000</td>\n",
       "      <td>40.028219</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>69.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.600076</td>\n",
       "      <td>-0.260076</td>\n",
       "      <td>00-0036912</td>\n",
       "      <td>DeVonta</td>\n",
       "      <td>Smith</td>\n",
       "      <td>D.Smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13327</th>\n",
       "      <td>2024</td>\n",
       "      <td>POST</td>\n",
       "      <td>23</td>\n",
       "      <td>Marquise Brown</td>\n",
       "      <td>WR</td>\n",
       "      <td>KC</td>\n",
       "      <td>4.943333</td>\n",
       "      <td>3.302615</td>\n",
       "      <td>6.356667</td>\n",
       "      <td>14.939872</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>3.533891</td>\n",
       "      <td>-1.083891</td>\n",
       "      <td>00-0035662</td>\n",
       "      <td>Marquise</td>\n",
       "      <td>Brown</td>\n",
       "      <td>M.Brown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13328</th>\n",
       "      <td>2024</td>\n",
       "      <td>POST</td>\n",
       "      <td>23</td>\n",
       "      <td>A.J. Brown</td>\n",
       "      <td>WR</td>\n",
       "      <td>PHI</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>1.562996</td>\n",
       "      <td>15.044000</td>\n",
       "      <td>40.820535</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.120000</td>\n",
       "      <td>3.505311</td>\n",
       "      <td>0.614689</td>\n",
       "      <td>00-0035676</td>\n",
       "      <td>Arthur</td>\n",
       "      <td>Brown</td>\n",
       "      <td>A.Brown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8249 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season season_type  week player_display_name player_position team_abbr  avg_cushion  avg_separation  avg_intended_air_yards  percent_share_of_intended_air_yards  receptions  targets  catch_percentage  yards  rec_touchdowns    avg_yac  avg_expected_yac  avg_yac_above_expectation player_gsis_id player_first_name player_last_name player_short_name\n",
       "1725     2017         REG     1          Ryan Grant              WR       WAS     9.936667        2.894592                4.410000                             7.154639           4        6         66.666667   61.0               0  11.232500         10.072361                   1.160139     00-0031068              Ryan            Grant           R.Grant\n",
       "1726     2017         REG     1     Martavis Bryant              WR       PIT     8.300000        4.122054               12.688333                            33.327496           2        6         33.333333   14.0               0   0.155000          4.098278                  -3.943278     00-0031373          Martavis           Bryant          M.Bryant\n",
       "1729     2017         REG     1     Jamison Crowder              WR       WAS     7.655000        3.177793               10.540000                            19.949707           3        7         42.857143   14.0               0   1.450000          1.631897                  -0.181897     00-0031941           Jamison          Crowder         J.Crowder\n",
       "1732     2017         REG     1      Nelson Agholor              WR       PHI     7.423750        2.462620               10.463750                            20.274656           6        8         75.000000   86.0               1   5.611667          3.262470                   2.349197     00-0031549            Nelson          Agholor         N.Agholor\n",
       "1733     2017         REG     1          John Brown              WR       ARI     7.360000        2.751526               13.422222                            28.208481           4        9         44.444444   32.0               0  -0.377500          0.961993                  -1.339493     00-0031051              John            Brown           J.Brown\n",
       "...       ...         ...   ...                 ...             ...       ...          ...             ...                     ...                                  ...         ...      ...               ...    ...             ...        ...               ...                        ...            ...               ...              ...               ...\n",
       "13323    2024        POST    23       Xavier Worthy              WR        KC     8.160000        4.959113               14.276250                            44.737358           8        8        100.000000  157.0               2   6.250000          6.154624                   0.095376     00-0039894            Xavier           Worthy          X.Worthy\n",
       "13324    2024        POST    23     DeAndre Hopkins              WR        KC     7.676000        3.446231               11.974000                            23.451761           2        5         40.000000   18.0               1   0.565000          0.798474                  -0.233474     00-0030564           DeAndre          Hopkins         D.Hopkins\n",
       "13325    2024        POST    23       DeVonta Smith              WR       PHI     7.470000        2.221577               14.752000                            40.028219           4        5         80.000000   69.0               1   0.340000          0.600076                  -0.260076     00-0036912           DeVonta            Smith           D.Smith\n",
       "13327    2024        POST    23      Marquise Brown              WR        KC     4.943333        3.302615                6.356667                            14.939872           2        6         33.333333   15.0               0   2.450000          3.533891                  -1.083891     00-0035662          Marquise            Brown           M.Brown\n",
       "13328    2024        POST    23          A.J. Brown              WR       PHI     2.510000        1.562996               15.044000                            40.820535           3        5         60.000000   43.0               1   4.120000          3.505311                   0.614689     00-0035676            Arthur            Brown           A.Brown\n",
       "\n",
       "[8249 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output: imports the NFL next-generation stats from the nfl python library\n",
    "\n",
    "# import the next generation stats (NGS) from nfl.import_ngs_data()\n",
    "# note: ngs starts at week 0 (previous season totals) - not needed so drop those rows\n",
    "\n",
    "# Pull NGS receiving data for the specified years\n",
    "wr_ngs_df = nfl.import_ngs_data('receiving', years)\n",
    "\n",
    "# Exclude rows where 'week' == 0 and filter for 'WR' position in one step\n",
    "wr_ngs_df = wr_ngs_df[(wr_ngs_df['week'] != 0) & (wr_ngs_df['player_position'] == 'WR')]\n",
    "\n",
    "# Drop unnecessary columns (already in the nfl python baseline dataframe)\n",
    "wr_ngs_df = wr_ngs_df.drop(columns=['player_jersey_number'], errors='ignore')\n",
    "\n",
    "# Display the resulting dataframe\n",
    "print(f\"Shape of NGS WR DataFrame after dropping columns: {wr_ngs_df.shape}\")\n",
    "display(wr_ngs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3315c698-007a-47a1-be89-40660a38a58d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f8daec0-2b18-4b4a-8ab7-c8b03da80b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./csv_files/wr_ngs_df.csv\n"
     ]
    }
   ],
   "source": [
    "# csv file\n",
    "save_csv(wr_ngs_df, \"wr_ngs_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f53817-0ec9-4499-b397-99a0d2fdb847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7579432c-8adb-4828-a305-b33d641401a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'season_type', 'week', 'player_display_name', 'player_position', 'team_abbr', 'avg_cushion', 'avg_separation', 'avg_intended_air_yards', 'percent_share_of_intended_air_yards', 'receptions', 'targets', 'catch_percentage', 'yards', 'rec_touchdowns', 'avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation', 'player_gsis_id', 'player_first_name', 'player_last_name', 'player_short_name']\n"
     ]
    }
   ],
   "source": [
    "print(wr_ngs_df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a517dc98-83a7-4921-8b3c-348dbe76a87e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b389e45-3b3a-4683-b333-b68f248c17c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Missing Value Summary for: NGS WR Stats\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Count</th>\n",
       "      <th>Missing %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_expected_yac</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_yac_above_expectation</th>\n",
       "      <td>42</td>\n",
       "      <td>0.0051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_yac</th>\n",
       "      <td>33</td>\n",
       "      <td>0.0040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yards</th>\n",
       "      <td>28</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_cushion</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Missing Count  Missing %\n",
       "avg_expected_yac                      42     0.0051\n",
       "avg_yac_above_expectation             42     0.0051\n",
       "avg_yac                               33     0.0040\n",
       "yards                                 28     0.0034\n",
       "avg_cushion                            2     0.0002"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# updated null analysis using helper function\n",
    "wr_ngs_null_summary_df = check_nulls(wr_ngs_df, name=\"NGS WR Stats\")\n",
    "display(wr_ngs_null_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326207b-e0ca-4d53-bfc1-b0bbab5c2ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: Python NFL Library Dataframe ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cc5e56-ef8d-4df7-bd42-09362326caae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237be5b5-12c9-4758-8243-a582c50a6167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin:fantasypros webscraping ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea4c9d-6309-43a6-93e2-ed8f1e0bf92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a scraper function for a single (year, week) to test parsing logic\n",
    "def test_scraper_sample(scraper_func, year_week_pair=(2024, 1), **kwargs):\n",
    "    # Wrap the pair in a list so it matches the scraper signature\n",
    "    year_week_pairs = [year_week_pair]\n",
    "    \n",
    "    sample_df, sample_errors = scraper_func(\n",
    "        year_week_pairs=year_week_pairs,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    print(\"Sample shape:\", sample_df.shape)\n",
    "    print(\"team_abbr unique values:\", sample_df[\"team_abbr\"].unique()[:15])\n",
    "    display(sample_df.head(10))\n",
    "    \n",
    "    return sample_df, sample_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c1618-bab6-41f5-850d-092c6aae5c69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae54158-1c8d-405f-9772-26d0416a5c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape FantasyPros weekly WR basic stats \n",
    "def wr_scrape_fp_basic_stats(\n",
    "    year_week_pairs,\n",
    "    save_csv_path=None,              \n",
    "    sleep_range=(0.35, 0.85),\n",
    "    timeout=20\n",
    "):\n",
    "    \"\"\"\n",
    "    Scrape FantasyPros weekly WR *basic* stats for all (year, week) pairs provided.\n",
    "    Expects year_week_pairs from generate_year_week_combinations(...) so preseason is skipped\n",
    "    and in-season weeks are capped at current_week.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas.DataFrame\n",
    "    errors : list[dict]\n",
    "    \"\"\"\n",
    "\n",
    "    # BASIC stats page\n",
    "    url_tpl = \"https://www.fantasypros.com/nfl/stats/wr.php?year={y}&week={w}&range=week\"\n",
    "\n",
    "    sess = requests.Session()\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "\n",
    "    rows, errors = [], []\n",
    "\n",
    "    def _extract_fp_id(a_tag):\n",
    "        if not a_tag: return None\n",
    "        for cls in a_tag.get(\"class\", []):\n",
    "            m = re.match(r\"fp-id-(\\d+)\", cls)\n",
    "            if m: return m.group(1)\n",
    "        if a_tag.has_attr(\"data-player-id\"): return str(a_tag[\"data-player-id\"])\n",
    "        if a_tag.has_attr(\"href\"):\n",
    "            m = re.search(r\"(\\d+)(?:/|$)\", a_tag[\"href\"])\n",
    "            if m: return m.group(1)\n",
    "        return None\n",
    "\n",
    "    def _normalize_team(t):\n",
    "        t = (t or \"\").upper().strip()\n",
    "        alias = {\"JAX\":\"JAC\", \"WSH\":\"WAS\", \"LAR\":\"LA\", \"STL\":\"LA\", \"OAK\":\"LV\", \"SD\":\"LAC\"}\n",
    "        return alias.get(t, t)\n",
    "\n",
    "    for (year, week) in year_week_pairs:\n",
    "        url = url_tpl.format(y=year, w=week)\n",
    "        try:\n",
    "            resp = sess.get(url, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            table = soup.find(\"table\")\n",
    "            if table is None:\n",
    "                errors.append({\"year\": year, \"week\": week, \"url\": url, \"error\": \"no_table\"})\n",
    "                time.sleep(uniform(*sleep_range)); continue\n",
    "\n",
    "            thead = table.find(\"thead\")\n",
    "            headers = [h.get_text(strip=True) for h in (thead.find_all(\"th\") if thead else [])]\n",
    "\n",
    "            tbody = table.find(\"tbody\")\n",
    "            tr_list = tbody.find_all(\"tr\") if tbody else []\n",
    "            for tr in tr_list:\n",
    "                tds = tr.find_all([\"td\",\"th\"])\n",
    "                if not tds: continue\n",
    "\n",
    "                # locate player cell\n",
    "                player_idx = None\n",
    "                for i, h in enumerate(headers):\n",
    "                    if h.lower() == \"player\": player_idx = i; break\n",
    "                if player_idx is None:\n",
    "                    for i, td in enumerate(tds):\n",
    "                        if td.find(\"a\", class_=re.compile(r\"\\bfp-player-link\\b\")):\n",
    "                            player_idx = i; break\n",
    "                player_td = tds[player_idx] if player_idx is not None else tr\n",
    "\n",
    "                a = player_td.find(\"a\", class_=re.compile(r\"\\bfp-player-link\\b\"))\n",
    "                fantasypros_id = _extract_fp_id(a)\n",
    "                player_name = a.get_text(strip=True) if a else None\n",
    "\n",
    "                # --- TEAM EXTRACTION (mirrors advanced scraper) ---\n",
    "                team_abbr = None\n",
    "                # Attempt regex from player_td text like \"Jayden Reed(GB)\"\n",
    "                m = re.search(r\"\\(([A-Z]{2,4})\\)\", player_td.get_text(\" \", strip=True))\n",
    "                if m:\n",
    "                    team_abbr = m.group(1)\n",
    "\n",
    "                team_abbr = _normalize_team(team_abbr)\n",
    "\n",
    "                # record\n",
    "                cell_vals = [td.get_text(strip=True) for td in tds]\n",
    "                rec = {\n",
    "                    \"season\": year,\n",
    "                    \"season_type\": \"REG\",\n",
    "                    \"week\": week,\n",
    "                    \"fantasypros_id\": fantasypros_id,\n",
    "                    \"player_name\": player_name,\n",
    "                    \"team_abbr\": team_abbr,\n",
    "                }\n",
    "                for col, val in zip(headers, cell_vals):\n",
    "                    rec[col] = val\n",
    "                rows.append(rec)\n",
    "\n",
    "            time.sleep(uniform(*sleep_range))\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append({\"year\": year, \"week\": week, \"url\": url, \"error\": str(e)})\n",
    "            time.sleep(uniform(*sleep_range))\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if not df.empty:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"week\"]   = pd.to_numeric(df[\"week\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "        if \"fantasypros_id\" in df.columns:\n",
    "            df = df.drop_duplicates(subset=[\"season\",\"week\",\"fantasypros_id\"], keep=\"first\")\n",
    "        else:\n",
    "            df = df.drop_duplicates()\n",
    "\n",
    "    if save_csv_path:\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "\n",
    "    return df, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377de09c-4291-4b74-b553-95833d608dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0754fe28-4bb3-4277-8a6f-2cfc2800a86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull a sample of the scraped fantasypros data for visual inspection\n",
    "sample_df, sample_errors = test_scraper_sample(\n",
    "    wr_scrape_fp_basic_stats,\n",
    "    year_week_pair=(2024, 1),\n",
    "    save_csv_path=None  # prevent saving during test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f9e636-0be5-4ee7-bc07-07cdff71a13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c16e26-094f-4241-bcd5-35c6fc26fbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: a dataframe of WR basic stats\n",
    "wr_fp_basic_stats_df, fp_basic_errors = wr_scrape_fp_basic_stats(\n",
    "    year_week_pairs,\n",
    "    save_csv_path=\"wr_fp_basic_stats.csv\"\n",
    ")\n",
    "\n",
    "print(f\"Shape: {wr_fp_basic_stats_df.shape}\")\n",
    "display(wr_fp_basic_stats_df)\n",
    "\n",
    "display(check_nulls(wr_fp_basic_stats_df, name=\"FantasyPros WR Basic Stats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c3e8f-632c-4a03-a223-7849c644cac8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb440b9-947e-4528-a1db-63a00a16657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape FantasyPros weekly WR advanced stats\n",
    "def wr_scrape_fp_adv_stats(\n",
    "    year_week_pairs,\n",
    "    save_csv_path=None,              \n",
    "    sleep_range=(0.35, 0.85),\n",
    "    timeout=20\n",
    "):\n",
    "    url_tpl = \"https://www.fantasypros.com/nfl/advanced-stats-wr.php?year={y}&week={w}&range=week&type=reg&mode=pergame\"\n",
    "\n",
    "    sess = requests.Session()\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "\n",
    "    rows, errors = [], []\n",
    "    def _extract_fp_id(a_tag):\n",
    "        if not a_tag: return None\n",
    "        for cls in a_tag.get(\"class\", []):\n",
    "            m = re.match(r\"fp-id-(\\d+)\", cls)\n",
    "            if m: return m.group(1)\n",
    "        if a_tag.has_attr(\"data-player-id\"): return str(a_tag[\"data-player-id\"])\n",
    "        if a_tag.has_attr(\"href\"):\n",
    "            m = re.search(r\"(\\d+)(?:/|$)\", a_tag[\"href\"])\n",
    "            if m: return m.group(1)\n",
    "        return None\n",
    "\n",
    "    def _normalize_team(t):\n",
    "        t = (t or \"\").upper().strip()\n",
    "        alias = {\"JAX\": \"JAC\", \"WSH\": \"WAS\", \"LAR\": \"LA\", \"STL\": \"LA\", \"OAK\": \"LV\", \"SD\": \"LAC\"}\n",
    "        return alias.get(t, t)\n",
    "\n",
    "    for (year, week) in year_week_pairs:\n",
    "        url = url_tpl.format(y=year, w=week)\n",
    "        try:\n",
    "            resp = sess.get(url, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            table = soup.find(\"table\")\n",
    "            if table is None:\n",
    "                errors.append({\"year\": year, \"week\": week, \"url\": url, \"error\": \"no_table\"})\n",
    "                time.sleep(uniform(*sleep_range)); continue\n",
    "\n",
    "            thead = table.find(\"thead\")\n",
    "            headers = [h.get_text(strip=True) for h in (thead.find_all(\"th\") if thead else [])]\n",
    "\n",
    "            tbody = table.find(\"tbody\")\n",
    "            tr_list = tbody.find_all(\"tr\") if tbody else []\n",
    "            for tr in tr_list:\n",
    "                tds = tr.find_all([\"td\",\"th\"])\n",
    "                if not tds: continue\n",
    "\n",
    "                # Locate the player cell (by header name or anchor class)\n",
    "                player_idx = None\n",
    "                for i, h in enumerate(headers):\n",
    "                    if h.lower() == \"player\": player_idx = i; break\n",
    "                if player_idx is None:\n",
    "                    for i, td in enumerate(tds):\n",
    "                        if td.find(\"a\", class_=re.compile(r\"\\bfp-player-link\\b\")):\n",
    "                            player_idx = i; break\n",
    "                player_td = tds[player_idx] if player_idx is not None else tr\n",
    "\n",
    "                a = player_td.find(\"a\", class_=re.compile(r\"\\bfp-player-link\\b\"))\n",
    "                fantasypros_id = _extract_fp_id(a)\n",
    "                player_name = a.get_text(strip=True) if a else None\n",
    "\n",
    "                # --- TEAM EXTRACTION (priority order) ---\n",
    "                team_abbr = None\n",
    "                # 1) â€œTeamâ€ column if present\n",
    "                try:\n",
    "                    team_col_idx = headers.index(\"Team\")\n",
    "                    team_abbr = tds[team_col_idx].get_text(strip=True)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                # 2) small/span near player name like \"(MIA)\"\n",
    "                if not team_abbr:\n",
    "                    tag = player_td.select_one(\"small\") or player_td.select_one(\"span\") \\\n",
    "                          or player_td.select_one('span[class*=\"team\"]')\n",
    "                    if tag:\n",
    "                        txt = tag.get_text(strip=True)\n",
    "                        m = re.search(r\"\\(([A-Z]{2,4})\\)\", txt)\n",
    "                        team_abbr = m.group(1) if m else txt\n",
    "                # 3) logo alt/title\n",
    "                if not team_abbr and a:\n",
    "                    img = a.find_next(\"img\")\n",
    "                    if img:\n",
    "                        team_abbr = img.get(\"alt\") or img.get(\"title\")\n",
    "                # 4) regex fallback on full player cell text\n",
    "                if not team_abbr:\n",
    "                    m = re.search(r\"\\(([A-Z]{2,4})\\)\", player_td.get_text(\" \", strip=True))\n",
    "                    if m: team_abbr = m.group(1)\n",
    "\n",
    "                team_abbr = _normalize_team(team_abbr)\n",
    "\n",
    "                # Build row dict\n",
    "                cell_vals = [td.get_text(strip=True) for td in tds]\n",
    "                rec = {\n",
    "                    \"season\": year,\n",
    "                    \"season_type\": \"REG\",\n",
    "                    \"week\": week,\n",
    "                    \"fantasypros_id\": fantasypros_id,\n",
    "                    \"player_name\": player_name,\n",
    "                    \"team_abbr\": team_abbr,\n",
    "                }\n",
    "                for col, val in zip(headers, cell_vals):\n",
    "                    rec[col] = val\n",
    "                rows.append(rec)\n",
    "\n",
    "            time.sleep(uniform(*sleep_range))\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append({\"year\": year, \"week\": week, \"url\": url, \"error\": str(e)})\n",
    "            time.sleep(uniform(*sleep_range))\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if not df.empty:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"week\"] = pd.to_numeric(df[\"week\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        if \"fantasypros_id\" in df.columns:\n",
    "            df = df.drop_duplicates(subset=[\"season\", \"week\", \"fantasypros_id\"], keep=\"first\")\n",
    "        else:\n",
    "            df = df.drop_duplicates()\n",
    "\n",
    "    if save_csv_path:\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "\n",
    "    return df, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f9da5-ffb9-42eb-9221-28fe20e7cde8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8389e1-60f8-44ad-9ed0-d965a350453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull a sample of the scraped fantasypros for visual inspection\n",
    "sample_df, sample_errors = test_scraper_sample(\n",
    "    wr_scrape_fp_adv_stats,\n",
    "    year_week_pair=(2024, 1),\n",
    "    save_csv_path=None  # prevent saving during test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444c34a-7028-4ef1-8ac5-5873a6f6e7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ddbeb-c238-49b2-b412-0de81a6f66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: a dataframe of WR fantasypros advanced stats\n",
    "wr_fp_advanced_stats_df, fp_errors = wr_scrape_fp_adv_stats(\n",
    "    year_week_pairs,\n",
    "    save_csv_path=\"wr_fp_advanced_stats.csv\"\n",
    ")\n",
    "\n",
    "# âœ… Updated null analysis using helper function\n",
    "print(f\"Shape of FantasyPros WR Advanced Stats DataFrame: {wr_fp_advanced_stats_df.shape}\")\n",
    "\n",
    "display(wr_fp_advanced_stats_df.head(25))\n",
    "display(check_nulls(wr_fp_advanced_stats_df, name=\"FantasyPros WR Advanced Stats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91129ead-f260-435b-8410-7bca4e02ad74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7ce1ee-da50-418d-ae62-1282f1206892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape WR fantasypros redzone stats \n",
    "def wr_scrape_fp_rz_stats(\n",
    "    year_week_pairs,\n",
    "    save_csv_path=None,              \n",
    "    sleep_range=(0.35, 0.85),\n",
    "    timeout=20\n",
    "):\n",
    "    url_tpl = \"https://www.fantasypros.com/nfl/red-zone-stats/wr.php?year={y}&week={w}&range=week\"\n",
    "\n",
    "    sess = requests.Session()\n",
    "    sess.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \"\n",
    "                      \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    })\n",
    "\n",
    "    rows, errors = [], []\n",
    "\n",
    "    def _extract_fp_id(a_tag):\n",
    "        if not a_tag:\n",
    "            return None\n",
    "        for cls in a_tag.get(\"class\", []):\n",
    "            m = re.match(r\"fp-id-(\\d+)\", cls)\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "        if a_tag.has_attr(\"data-player-id\"):\n",
    "            return str(a_tag[\"data-player-id\"])\n",
    "        if a_tag.has_attr(\"href\"):\n",
    "            m = re.search(r\"(\\d+)(?:/|$)\", a_tag[\"href\"])\n",
    "            if m:\n",
    "                return m.group(1)\n",
    "        return None\n",
    "\n",
    "    def _normalize_team(t):\n",
    "        t = (t or \"\").upper().strip()\n",
    "        alias = {\"JAX\":\"JAC\", \"WSH\":\"WAS\", \"LAR\":\"LA\", \"STL\":\"LA\", \"OAK\":\"LV\", \"SD\":\"LAC\"}\n",
    "        if t in {\"FANTASYPROS\", \"FANTASY PROS\", \"FANTASY-PROS\", \"FP\", \"\"}:\n",
    "            return None\n",
    "        if t != \"FA\" and not (2 <= len(t) <= 4 and t.isalpha()):\n",
    "            return None\n",
    "        return alias.get(t, t)\n",
    "\n",
    "    for (year, week) in year_week_pairs:\n",
    "        url = url_tpl.format(y=year, w=week)\n",
    "        try:\n",
    "            resp = sess.get(url, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            table = soup.find(\"table\")\n",
    "            if table is None:\n",
    "                errors.append({\"year\": year, \"week\": week, \"url\": url, \"error\": \"no_table\"})\n",
    "                time.sleep(uniform(*sleep_range))\n",
    "                continue\n",
    "\n",
    "            thead = table.find(\"thead\")\n",
    "            headers = [h.get_text(strip=True) for h in (thead.find_all(\"th\") if thead else [])]\n",
    "\n",
    "            tbody = table.find(\"tbody\")\n",
    "            tr_list = tbody.find_all(\"tr\") if tbody else []\n",
    "            for tr in tr_list:\n",
    "                tds = tr.find_all([\"td\",\"th\"])\n",
    "                if not tds:\n",
    "                    continue\n",
    "\n",
    "                # locate player cell\n",
    "                player_idx = None\n",
    "                for i, h in enumerate(headers):\n",
    "                    if h.lower() == \"player\":\n",
    "                        player_idx = i\n",
    "                        break\n",
    "                if player_idx is None:\n",
    "                    for i, td in enumerate(tds):\n",
    "                        if td.find(\"a\", class_=re.compile(r\"\\bfp-player-link\\b\")):\n",
    "                            player_idx = i\n",
    "                            break\n",
    "                player_td = tds[player_idx] if player_idx is not None else tr\n",
    "\n",
    "                a = player_td.find(\"a\", class_=re.compile(r\"\\bfp-player-link\\b\"))\n",
    "                fantasypros_id = _extract_fp_id(a)\n",
    "                player_name = a.get_text(strip=True) if a else None\n",
    "\n",
    "                # --- TEAM extraction fix ---\n",
    "                team_abbr = None\n",
    "\n",
    "                # 1) Regex directly from full player cell text\n",
    "                m = re.search(r\"\\(([A-Z]{2,4})\\)\", player_td.get_text(\" \", strip=True))\n",
    "                if m:\n",
    "                    team_abbr = m.group(1)\n",
    "\n",
    "                # 2) 'Team' column if present\n",
    "                if not team_abbr:\n",
    "                    try:\n",
    "                        team_col_idx = headers.index(\"Team\")\n",
    "                        team_abbr = tds[team_col_idx].get_text(strip=True)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "\n",
    "                # 3) small/span near player name\n",
    "                if not team_abbr:\n",
    "                    tag = (player_td.select_one(\"small\")\n",
    "                           or player_td.select_one(\"span\")\n",
    "                           or player_td.select_one('span[class*=\"team\"]'))\n",
    "                    if tag:\n",
    "                        txt = tag.get_text(strip=True)\n",
    "                        m = re.search(r\"\\(([A-Z]{2,4})\\)\", txt)\n",
    "                        team_abbr = m.group(1) if m else txt\n",
    "\n",
    "                # 4) logo alt/title\n",
    "                if not team_abbr and a:\n",
    "                    img = a.find_next(\"img\")\n",
    "                    if img:\n",
    "                        team_abbr = img.get(\"alt\") or img.get(\"title\")\n",
    "\n",
    "                team_abbr = _normalize_team(team_abbr)\n",
    "\n",
    "                # record\n",
    "                cell_vals = [td.get_text(strip=True) for td in tds]\n",
    "                rec = {\n",
    "                    \"season\": year,\n",
    "                    \"season_type\": \"REG\",\n",
    "                    \"week\": week,\n",
    "                    \"fantasypros_id\": fantasypros_id,\n",
    "                    \"player_name\": player_name,\n",
    "                    \"team_abbr\": team_abbr,\n",
    "                }\n",
    "                for col, val in zip(headers, cell_vals):\n",
    "                    rec[col] = val\n",
    "                rows.append(rec)\n",
    "\n",
    "            time.sleep(uniform(*sleep_range))\n",
    "\n",
    "        except Exception as e:\n",
    "            errors.append({\"year\": year, \"week\": week, \"url\": url, \"error\": str(e)})\n",
    "            time.sleep(uniform(*sleep_range))\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if not df.empty:\n",
    "        df[\"season\"] = pd.to_numeric(df[\"season\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[\"week\"]   = pd.to_numeric(df[\"week\"],   errors=\"coerce\").astype(\"Int64\")\n",
    "        if \"fantasypros_id\" in df.columns:\n",
    "            df = df.drop_duplicates(subset=[\"season\",\"week\",\"fantasypros_id\"], keep=\"first\")\n",
    "        else:\n",
    "            df = df.drop_duplicates()\n",
    "\n",
    "    if save_csv_path:\n",
    "        df.to_csv(save_csv_path, index=False)\n",
    "\n",
    "    return df, errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb0bf05-4eaa-4590-a355-8172d97c78b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e69418d-d9d9-42da-a88c-945880121764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull a sample of the scraped fantasypros data for visual inspection\n",
    "sample_df, sample_errors = test_scraper_sample(\n",
    "    wr_scrape_fp_rz_stats,\n",
    "    year_week_pair=(2024, 1),\n",
    "    save_csv_path=None  # prevent saving during test\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b67317-7f4a-4cca-848d-91e2c4e90a5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17ef827-3065-4678-9138-5e11c451b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output: a dataframe of WR fantasypros advanced stats\n",
    "wr_fp_rz_stats_df, fp_rz_errors = wr_scrape_fp_rz_stats(\n",
    "    year_week_pairs,\n",
    "    save_csv_path=\"wr_fp_rz_stats.csv\"\n",
    ")\n",
    "\n",
    "# ðŸ“Š Display shape\n",
    "print(f\"Shape of FantasyPros WR Red Zone Stats DataFrame: {wr_fp_rz_stats_df.shape}\")\n",
    "\n",
    "# ðŸ‘€ Display first few rows\n",
    "display(wr_fp_rz_stats_df.head(25))\n",
    "\n",
    "# ðŸ” Display missing value summary\n",
    "display(check_nulls(wr_fp_rz_stats_df, name=\"FantasyPros WR Red Zone Stats\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1934222c-f33c-40ae-ada7-a35a7a136903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d88de8-53d6-4590-a0c7-175f51344eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing columns of all three FantasyPros dataframes\n",
    "basic_stats_cols = wr_fp_basic_stats_df.columns.tolist()\n",
    "advanced_stats_cols = wr_fp_advanced_stats_df.columns.tolist()\n",
    "redzone_stats_cols = wr_fp_rz_stats_df.columns.tolist()\n",
    "\n",
    "# Combine into a dataframe for comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Basic Stats\": pd.Series(basic_stats_cols),\n",
    "    \"Advanced Stats\": pd.Series(advanced_stats_cols),\n",
    "    \"Red Zone Stats\": pd.Series(redzone_stats_cols)\n",
    "})\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8a52ec-09cc-4fc0-9392-41d0bd373793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cdc840-6209-4f6c-822e-02f44ec70523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Display the shape of each dataframe before merging\n",
    "print(f\"ðŸ“Š **Shape of WR Basic Stats DataFrame:** {wr_fp_basic_stats_df.shape}\")\n",
    "print(f\"\\nðŸ“Š **Shape of WR Advanced Stats DataFrame:** {wr_fp_advanced_stats_df.shape}\")\n",
    "print(f\"ðŸ“Š **Shape of WR Red Zone Stats DataFrame:** {wr_fp_rz_stats_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8dda8e-0d30-424e-ba29-bc772bb663d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04342ab9-e794-49c1-ae3b-3e0fedac4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nulls\n",
    "# Apply helper function to each FantasyPros DataFrame\n",
    "basic_stats_nulls = check_nulls(wr_fp_basic_stats_df, \"FantasyPros Basic Stats\")\n",
    "advanced_stats_nulls = check_nulls(wr_fp_advanced_stats_df, \"FantasyPros Advanced Stats\")\n",
    "redzone_nulls = check_nulls(wr_fp_rz_stats_df, \"FantasyPros Red Zone Stats\")\n",
    "\n",
    "# Concatenate all results (only non-empty will be shown)\n",
    "combined_nulls = pd.concat(\n",
    "    [basic_stats_nulls, advanced_stats_nulls, redzone_nulls],\n",
    "    keys=[\"Basic Stats\", \"Advanced Stats\", \"Red Zone Stats\"]\n",
    ")\n",
    "combined_nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7bee76-7d2b-4a61-95c1-c1bb0470d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End:fantasypros webscraping ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738863d-e44d-42be-811a-031a7984e418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21298dab-c370-4d72-b1f2-da6681089480",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin: Build the dataframe for the DFS Fanduel and Draft Kings salary data from BigDataBall ##\n",
    "# ** Files must be in the local directory ** NFL-20xx-DFS-Dataset.xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09798d-43b2-4f32-9bde-7c34d3cbd928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper function to clean the the dfs salary data\n",
    "#    - Cleans and flattens multi-index column names for DFS salary Excel files:\n",
    "#    - Joins tuples if multi-index\n",
    "#    - Removes special characters\n",
    "#    - Normalizes spaces\n",
    "#    - Converts to lowercase for matching\n",
    "def clean_column_dfs(col):\n",
    "    if isinstance(col, tuple):\n",
    "        col = ' '.join(str(x) for x in col if x)\n",
    "\n",
    "    return (\n",
    "        str(col)\n",
    "        .replace('\\n', ' ')\n",
    "        .replace('(', '')\n",
    "        .replace(')', '')\n",
    "        .replace('\"', '')\n",
    "        .replace('#', '')\n",
    "        .replace('$', '')\n",
    "        .replace('/', '')\n",
    "        .replace('-', ' ')\n",
    "        .strip()\n",
    "        .lower()\n",
    "        .replace('  ', ' ')\n",
    "        .replace('   ', ' ')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5100916b-0409-46ad-9ce7-57c9aa586589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85df8233-1e52-4172-8448-783170151690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel files\n",
    "filepath = 'NFL-2024-DFS-Dataset.xlsx'\n",
    "dfs_raw = pd.read_excel(filepath, header=[0, 1])\n",
    "original_row_count = len(dfs_raw)\n",
    "\n",
    "dfs_raw.columns = [clean_column_dfs(col) for col in dfs_raw.columns]\n",
    "dfs_raw.head()  # Optional preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1881fd2d-fea1-46bb-a943-3a6d225772f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccef830e-cdf6-4856-8781-3c3b81452dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function does the following:\n",
    "# Fanduel and Draft Kings player salary data for all positions (QB, RB, TE, WR, DST)\n",
    "# creates and combines the dataframes for years 2017 - present \n",
    "# performs data validation checks\n",
    "\n",
    "def create_DFS_dataframe(filepath, year):\n",
    "    \n",
    "    # Step 1: Read and clean the headers\n",
    "    dfs_raw = pd.read_excel(filepath, header=[0, 1])\n",
    "    original_row_count = len(dfs_raw)\n",
    "    dfs_raw.columns = [clean_column_dfs(col) for col in dfs_raw.columns]\n",
    "\n",
    "    # âœ… Step 2: Extract only relevant columns using cleaned names\n",
    "    expected_cols = {\n",
    "        'player': 'game information player dst',\n",
    "        'week': 'game information week',\n",
    "        'date': 'game information date',\n",
    "        'player_id': 'game information player id',\n",
    "        'team': 'game information team', \n",
    "        'opponent': 'game information opponent',\n",
    "        'dk_position': 'position draftkings',\n",
    "        'fd_position': 'position fanduel',\n",
    "        'dk_salary': 'salary for draftkings classic contests',\n",
    "        'fd_salary': 'salary for fanduel full roster contests',\n",
    "        'dk_fpts': 'fantasy points scored draftkings',\n",
    "        'fd_fpts': 'fantasy points scored fanduel'\n",
    "    }\n",
    "\n",
    "    # Subset the dataframe using cleaned column names\n",
    "    dfs_subset = dfs_raw[list(expected_cols.values())].copy()\n",
    "\n",
    "    # Rename them to simple identifiers for internal use\n",
    "    dfs_subset.columns = list(expected_cols.keys())\n",
    "\n",
    "    \n",
    "    dfs_subset['date'] = pd.to_datetime(dfs_subset['date'])\n",
    "\n",
    "    team_abbreviation_mapping = {\n",
    "        'NWE': 'NE',\n",
    "        'SFO': 'SF',\n",
    "        'OAK': 'LV',\n",
    "        'KAN': 'KC',\n",
    "        'TAM': 'TB',\n",
    "        'NOR': 'NO',\n",
    "        'LAR': 'LA',\n",
    "        'GNB': 'GB'\n",
    "    }\n",
    "    mask_dst = dfs_subset['dk_position'] == 'DST'\n",
    "    dfs_subset.loc[mask_dst, 'player_id'] = dfs_subset.loc[mask_dst, 'player_id'].replace(team_abbreviation_mapping)\n",
    "\n",
    "    def fix_season(row):\n",
    "        game_year = row['date'].year\n",
    "        game_month = row['date'].month\n",
    "        game_week = row['week']\n",
    "        \n",
    "        if game_month in [1, 2]:\n",
    "            if (game_year <= 2020 and game_week >= 18):\n",
    "                return game_year - 1\n",
    "            elif (game_year >= 2021 and game_week >= 19):\n",
    "                return game_year - 1\n",
    "            elif (game_year >= 2021 and game_week == 18):\n",
    "                return game_year - 1\n",
    "        return game_year\n",
    "\n",
    "    dfs_subset['season'] = dfs_subset.apply(fix_season, axis=1)\n",
    "\n",
    "    # ðŸ”¥ Track NaNs before dropping\n",
    "    season_nulls_before = dfs_subset['season'].isna().sum()\n",
    "\n",
    "    dfs_subset = dfs_subset.dropna(subset=['season'])\n",
    "    dfs_subset['season'] = dfs_subset['season'].astype(int)\n",
    "\n",
    "    season_nulls_after = dfs_subset['season'].isna().sum()\n",
    "\n",
    "    print(f\"ðŸ”Ž Season NaN rows dropped: {season_nulls_before}\")\n",
    "    print(f\"Remaining NaN rows (should be 0): {season_nulls_after}\")\n",
    "\n",
    "    dfs_subset = dfs_subset.drop(columns=['date'])\n",
    "\n",
    "    dfs_subset['dk_salary'] = pd.to_numeric(dfs_subset['dk_salary'], errors='coerce')\n",
    "    dfs_subset['fd_salary'] = pd.to_numeric(dfs_subset['fd_salary'], errors='coerce')\n",
    "    dfs_subset = dfs_subset.dropna(subset=['dk_salary', 'fd_salary'])\n",
    "    dfs_subset['dk_salary'] = dfs_subset['dk_salary'].astype(int)\n",
    "    dfs_subset['fd_salary'] = dfs_subset['fd_salary'].astype(int)\n",
    "    dfs_subset['week'] = dfs_subset['week'].astype(int)\n",
    "    \n",
    "    dfs_subset = dfs_subset[['season', 'week', 'player_id', 'player', 'dk_position', 'fd_position', \n",
    "                             'team', 'opponent', 'dk_salary', 'fd_salary', 'dk_fpts', 'fd_fpts']]\n",
    "    \n",
    "    unique_weeks = dfs_subset['week'].nunique()\n",
    "    min_week = dfs_subset['week'].min()\n",
    "    max_week = dfs_subset['week'].max()\n",
    "    expected_weeks = 21 if int(year) <= 2020 else 22\n",
    "\n",
    "    print(f\"\\nProcessing file: {filepath}\")\n",
    "    print(f\"Original rows in xlsx file: {original_row_count}\")\n",
    "    print(f\"Number of players with no salary data found in xlsx: {original_row_count - len(dfs_subset)}\")\n",
    "    print(f\"Rows in csv file after dropping NaNs: {len(dfs_subset)}\")\n",
    "\n",
    "    if original_row_count - (original_row_count - len(dfs_subset)) == len(dfs_subset):\n",
    "        print(\"âœ… Salary Validation passed: Counts match after dropping NaNs.\")\n",
    "        salary_validation = 'Passed'\n",
    "    else:\n",
    "        print(\"âŒ Salary Validation failed: Counts mismatch!\")\n",
    "        salary_validation = 'Failed'\n",
    "\n",
    "    print(f\"Weeks detected: {min_week} to {max_week}\")\n",
    "    print(f\"Total unique weeks found: {unique_weeks}\")\n",
    "    print(\"ðŸ”” Reminder: Missing final playoff week (e.g., Super Bowl) is normal if no salary data exists.\")\n",
    "\n",
    "    if unique_weeks == expected_weeks or unique_weeks == expected_weeks - 1:\n",
    "        print(f\"âœ… Week Validation passed: {unique_weeks} weeks found (expected {expected_weeks}).\\n\")\n",
    "        week_validation = 'Passed'\n",
    "    else:\n",
    "        print(f\"âŒ Week Validation failed: {unique_weeks} weeks found, expected {expected_weeks}.\\n\")\n",
    "        week_validation = 'Failed'\n",
    "    \n",
    "    return dfs_subset, {\n",
    "        'year': int(year),\n",
    "        'original_rows': original_row_count,\n",
    "        'nan_rows': original_row_count - len(dfs_subset),\n",
    "        'rows_after_drop': len(dfs_subset),\n",
    "        'min_week': min_week,\n",
    "        'max_week': max_week,\n",
    "        'unique_weeks': unique_weeks,\n",
    "        'expected_weeks': expected_weeks,\n",
    "        'salary_validation': salary_validation,\n",
    "        'week_validation': week_validation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467d41e-cfbf-40a9-ba64-01515500ef1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f9a80c-b298-422f-ae2e-218a92d534b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** dataframe of Fanduel and Draft Kings Salaries FOR all positions ** \n",
    "\n",
    "# main control flow implements the helper function \n",
    "# output: combined dataframe and csv files of all seasons fanduel draft kings player salary data \n",
    "# output: data validation checks\n",
    "\n",
    "# Find all matching files\n",
    "file_list = sorted(glob.glob('NFL-*-DFS-Dataset.xlsx'))\n",
    "\n",
    "# Handle if no files found\n",
    "if not file_list:\n",
    "    print(\"âŒ No xlsx files detected.\\nPlease download and place the BigDataBall NFL DFS Excel files into the same directory as this Jupyter Notebook file.\")\n",
    "else:\n",
    "    # Process each file\n",
    "    all_years_dfs = []\n",
    "    validation_records = []\n",
    "    file_years = []\n",
    "\n",
    "    for file in file_list:\n",
    "        year = file.split('-')[1]  # Extract year from filename\n",
    "        file_years.append(int(year))\n",
    "        \n",
    "        year_df, validation_info = create_DFS_dataframe(file, year)\n",
    "\n",
    "        # ** csv file ***\n",
    "        # Save per-year CSV\n",
    "        # year_df.to_csv(f'nfl_fd_dk_salary_{year}.csv', index=False)\n",
    "        \n",
    "        # Append to master list\n",
    "        all_years_dfs.append(year_df)\n",
    "        validation_records.append(validation_info)\n",
    "\n",
    "    # Create validation summary DataFrame\n",
    "    validation_summary_df = pd.DataFrame(validation_records)\n",
    "    print(\"\\nðŸ“‹ Validation Summary:\")\n",
    "    display(validation_summary_df)\n",
    "\n",
    "    # Combine all years into one big dataframe\n",
    "    nfl_fd_dk_salary_combined = pd.concat(all_years_dfs, ignore_index=True)\n",
    "\n",
    "    # Determine latest season dynamically\n",
    "    current_season = max(file_years)\n",
    "\n",
    "    # Export final combined CSV\n",
    "    final_filename = f'nfl_fd_dk_salary_2017_{current_season}.csv'\n",
    "\n",
    "    # If the file already exists, create a backup\n",
    "    if os.path.exists(final_filename):\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        backup_filename = f'nfl_fd_dk_salary_2017_{current_season}_backup_{timestamp}.csv'\n",
    "        shutil.copy(final_filename, backup_filename)\n",
    "        print(f\"ðŸ›¡ï¸ Backup created: {backup_filename}\")\n",
    "\n",
    "\n",
    "    # *** csv file ***\n",
    "    # nfl_fd_dk_salary_combined.to_csv(final_filename, index=False)\n",
    "\n",
    "    print(f\"\\nâœ… Final combined CSV saved as: {final_filename}\")\n",
    "\n",
    "    # Display a quick preview\n",
    "    display(nfl_fd_dk_salary_combined.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30383fbe-c66b-473e-a56b-f76d3b67de20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4086f-35aa-4f6e-a4b3-cb54e089f258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** WR dataframe of Fanduel and Draft Kings player Salaries ** \n",
    "\n",
    "# Determine current season based on available data\n",
    "current_season = nfl_fd_dk_salary_combined['season'].max()\n",
    "\n",
    "# Extract WR players where DraftKings position is WR\n",
    "wr_fd_dk_salary_2017_current_df = nfl_fd_dk_salary_combined.loc[\n",
    "    nfl_fd_dk_salary_combined['dk_position'] == 'WR'\n",
    "]\n",
    "\n",
    "# *** csv file ***\n",
    "wr_csv_filename = f'wr_fd_dk_salary_2017_{current_season}.csv'\n",
    "wr_fd_dk_salary_2017_current_df.to_csv(wr_csv_filename, index=False)\n",
    "\n",
    "print(f\"âœ… WR DFS dataframe created and saved as {wr_csv_filename}\")\n",
    "\n",
    "# Optional: Display a quick preview\n",
    "display(wr_fd_dk_salary_2017_current_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605de75-abb4-4dcc-b173-c18ccf261cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End: Build the dataframe for the DFS Fanduel and Draft Kings salary data from BigDataBall ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c4fb01-be5f-4c39-a712-c5663ad5cccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4be2d9d-a19b-4884-acde-4919d92cb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All dataframes - no features and no salary\n",
    "# wr_ids_weekly_stats_df\n",
    "# wr_ngs_df\n",
    "# wr_fp_basic_stats_df\n",
    "# wr_fp_advanced_stats_df\n",
    "# wr_fp_rz_stats_df\n",
    "# wr_fd_dk_salary_2017_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b65ddc7-c36c-4ea3-94c5-98836069b1dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc8bb10-7ffd-406e-bda3-d5f125292fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin: team abbreviation standardization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca9788-5196-4db0-b3cc-fc08eba7788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List columns from each dataframe in memory\n",
    "ids_weekly_cols = wr_ids_weekly_stats_df.columns.tolist()\n",
    "ngs_cols = wr_ngs_df.columns.tolist()\n",
    "fp_basic_cols = wr_fp_basic_stats_df.columns.tolist()\n",
    "fp_adv_cols = wr_fp_advanced_stats_df.columns.tolist()\n",
    "fp_rz_cols = wr_fp_rz_stats_df.columns.tolist()\n",
    "dfs_fd_dk_cols = wr_fd_dk_salary_2017_current_df.columns.tolist()\n",
    "\n",
    "# Combine into a dataframe for side-by-side comparison\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"IDs & Weekly Stats\": pd.Series(ids_weekly_cols),\n",
    "    \"NGS Stats\": pd.Series(ngs_cols),\n",
    "    \"FantasyPros Basic\": pd.Series(fp_basic_cols),\n",
    "    \"FantasyPros Adv\": pd.Series(fp_adv_cols),\n",
    "    \"FantasyPros RZ\": pd.Series(fp_rz_cols),\n",
    "    \"DFS FD DK\": pd.Series(dfs_fd_dk_cols) \n",
    "})\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745aef9f-2d3e-456e-ba6a-cbb7562a3dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4a69df-bdfd-4267-a0bb-a5b07fdd0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_team_uniques():\n",
    "    def norm(s):\n",
    "        return (\n",
    "            s.astype('string')\n",
    "             .str.strip()\n",
    "             .str.upper()\n",
    "             .str.replace(\".\", \"\", regex=False)\n",
    "             .str.replace(\" \", \"\", regex=False)\n",
    "        )\n",
    "\n",
    "    datasets = {\n",
    "        \"wr_ids_weekly_stats_df.recent_team\": (wr_ids_weekly_stats_df, ['recent_team']),\n",
    "        \"wr_ngs_df.team_abbr\":               (wr_ngs_df,             ['team_abbr']),\n",
    "        \"wr_fp_basic_stats_df.team\":         (wr_fp_basic_stats_df,  ['team_abbr']),\n",
    "        \"wr_fp_advanced_stats_df.team\":      (wr_fp_advanced_stats_df,['team_abbr']),\n",
    "        \"wr_fp_rz_stats_df.team\":            (wr_fp_rz_stats_df,     ['team_abbr']),\n",
    "        \"wr_fd_dk_salary_2017_current_df.team\": (wr_fd_dk_salary_2017_current_df, ['team']),\n",
    "    }\n",
    "\n",
    "    for label, (df, candidates) in datasets.items():\n",
    "        team_col = next((c for c in candidates if c in df.columns), None)\n",
    "        print(f\"\\n{label}\")\n",
    "        if not team_col:\n",
    "            print(f\"  âš ï¸ No team column found in {candidates}\")\n",
    "            continue\n",
    "\n",
    "        vals = sorted(norm(df[team_col].dropna()).unique())\n",
    "        print(f\"  column: {team_col} | uniques ({len(vals)}):\")\n",
    "        print(vals)\n",
    "\n",
    "# Call to preview all six\n",
    "show_team_uniques()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffbb36-dbb6-4db4-a161-ddf9e7c77aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57e9f07-1613-4cd7-ba5d-2f9bbf6ae9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean & get unique values\n",
    "def get_team_set(df, col):\n",
    "    return set(df[col].dropna().astype(str).str.strip().str.upper())\n",
    "\n",
    "baseline_set = get_team_set(wr_ids_weekly_stats_df, 'recent_team')\n",
    "print(f\"Baseline (wr_ids_weekly_stats_df.recent_team) â€” {len(baseline_set)} uniques:\\n{sorted(baseline_set)}\\n\")\n",
    "\n",
    "# Dataframe -> column to compare\n",
    "compare_map = {\n",
    "    \"wr_ngs_df\": (\"team_abbr\" if \"team_abbr\" in wr_ngs_df.columns else None),\n",
    "    \"wr_fp_basic_stats_df\": (\"team\" if \"team\" in wr_fp_basic_stats_df.columns else None),\n",
    "    \"wr_fp_advanced_stats_df\": (\"team\" if \"team\" in wr_fp_advanced_stats_df.columns else None),\n",
    "    \"wr_fp_rz_stats_df\": (\"team\" if \"team\" in wr_fp_rz_stats_df.columns else None),\n",
    "    \"wr_fd_dk_salary_2017_current_df\": (\"team\" if \"team\" in wr_fd_dk_salary_2017_current_df.columns else None),\n",
    "}\n",
    "\n",
    "for name, col in compare_map.items():\n",
    "    if col and col in globals()[name].columns:\n",
    "        other_set = get_team_set(globals()[name], col)\n",
    "        diff_from_baseline = other_set - baseline_set\n",
    "        diff_in_baseline = baseline_set - other_set\n",
    "        print(f\"{name}.{col}:\")\n",
    "        print(f\"  Unique values: {len(other_set)}\")\n",
    "        print(f\"  In {name} but not in baseline: {sorted(diff_from_baseline) if diff_from_baseline else 'None'}\")\n",
    "        print(f\"  In baseline but not in {name}: {sorted(diff_in_baseline) if diff_in_baseline else 'None'}\\n\")\n",
    "    else:\n",
    "        print(f\"{name}: âš ï¸ No team column found or mismatch\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d6e0b-9373-4b15-8cf8-a0fde1d0dd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8dfca-4ede-4fe9-84f2-a41f8976fa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize team abbreviations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becda923-8e60-4cd4-a270-ea16ca6e9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# base mapping: full team names -> abbreviations\n",
    "full_name_to_abbr = {\n",
    "    'Arizona Cardinals': 'ARI', 'Atlanta Falcons': 'ATL', 'Baltimore Ravens': 'BAL',\n",
    "    'Buffalo Bills': 'BUF', 'Carolina Panthers': 'CAR', 'Chicago Bears': 'CHI',\n",
    "    'Cincinnati Bengals': 'CIN', 'Cleveland Browns': 'CLE', 'Dallas Cowboys': 'DAL',\n",
    "    'Denver Broncos': 'DEN', 'Detroit Lions': 'DET', 'Green Bay Packers': 'GB',\n",
    "    'Houston Texans': 'HOU', 'Indianapolis Colts': 'IND', 'Jacksonville Jaguars': 'JAX',\n",
    "    'Kansas City Chiefs': 'KC', 'Las Vegas Raiders': 'LV', 'Los Angeles Chargers': 'LAC',\n",
    "    'Los Angeles Rams': 'LA', 'Miami Dolphins': 'MIA', 'Minnesota Vikings': 'MIN',\n",
    "    'New England Patriots': 'NE', 'New Orleans Saints': 'NO', 'New York Giants': 'NYG',\n",
    "    'New York Jets': 'NYJ', 'Philadelphia Eagles': 'PHI', 'Pittsburgh Steelers': 'PIT',\n",
    "    'San Francisco 49ers': 'SF', 'Seattle Seahawks': 'SEA', 'Tampa Bay Buccaneers': 'TB',\n",
    "    'Tennessee Titans': 'TEN', 'Washington Commanders': 'WAS', \n",
    "\n",
    "    # legacy names\n",
    "    'St. Louis Rams': 'LAR',\n",
    "    'San Diego Chargers': 'LAC',\n",
    "    'Oakland Raiders': 'LV',\n",
    "    'Washington Football Team': 'WAS',\n",
    "    'Washington Redskins': 'WAS',\n",
    "    \n",
    "    # Free agent placeholder\n",
    "    'Free Agent': 'FA'\n",
    "    \n",
    "}\n",
    "\n",
    "# --- Start with exact-case mapping ---\n",
    "alias_map = {name.upper(): abbr for name, abbr in full_name_to_abbr.items()}\n",
    "\n",
    "# --- Add no-space/punctuation aliases ---\n",
    "for name, abbr in full_name_to_abbr.items():\n",
    "    no_space = re.sub(r'[^A-Z0-9]', '', name.upper())\n",
    "    alias_map[no_space] = abbr\n",
    "\n",
    "# --- Add free agent compressed form ---\n",
    "alias_map['FREEAGENT'] = 'FA'\n",
    "\n",
    "# --- Abbreviation fixups (site quirks, alternate short codes) ---\n",
    "abbr_fixes = {\n",
    "    'ARZ': 'ARI', 'TBB': 'TB', 'NEP': 'NE', 'GBP': 'GB',\n",
    "    'KCC': 'KC', 'SFF': 'SF', 'NOS': 'NO', 'JAC': 'JAX',\n",
    "    'LAR': 'LA', 'LVR': 'LV', 'WSH': 'WAS', 'WFT': 'WAS'\n",
    "}\n",
    "\n",
    "# Merge fixups into alias_map so one lookup covers all cases\n",
    "alias_map.update(abbr_fixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c097fc-70f2-4f19-b64e-8eae2ef522ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176adb8-8e6a-4199-a247-4eb149b49831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create team abbreviation mapping logic\n",
    "def standardize_team_abbr(df, col, mapping):\n",
    "    if col in df.columns:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.upper()\n",
    "            .str.replace(\".\", \"\", regex=False)\n",
    "            .str.replace(\" \", \"\", regex=False)\n",
    "            .replace(mapping)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231c6ba-944e-4792-abf4-74ab2d658c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90ca60-5dad-4401-9f12-8d9243118a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standardization to all relevant dataframes/columns in one go\n",
    "datasets_to_standardize = [\n",
    "    (wr_ids_weekly_stats_df, \"recent_team\"),\n",
    "    (wr_ngs_df, \"team_abbr\"),\n",
    "    (wr_fp_basic_stats_df, \"team_abbr\"),\n",
    "    (wr_fp_advanced_stats_df, \"team_abbr\"),\n",
    "    (wr_fp_rz_stats_df, \"team_abbr\"),\n",
    "    (wr_fd_dk_salary_2017_current_df, \"team\")\n",
    "]\n",
    "\n",
    "for df, col in datasets_to_standardize:\n",
    "    standardize_team_abbr(df, col, alias_map)\n",
    "\n",
    "# Quick check after standardization\n",
    "for df, col in datasets_to_standardize:\n",
    "    if col in df.columns:\n",
    "        print(f\"{col} uniques in dataframe:\")\n",
    "        print(sorted(df[col].dropna().unique()))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ebde2-3e66-437f-983a-ba43c1645846",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6832499d-713f-4716-8e1b-462f8bec047e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list the unique values for each specified team column for visual inspection\n",
    "dfs_and_cols = [\n",
    "    (\"IDs & Weekly\", wr_ids_weekly_stats_df, \"recent_team\"),\n",
    "    (\"NGS\", wr_ngs_df, \"team_abbr\"),\n",
    "    (\"FP Basic\", wr_fp_basic_stats_df, \"team_abbr\"),\n",
    "    (\"FP Advanced\", wr_fp_advanced_stats_df, \"team_abbr\"),\n",
    "    (\"FP RZ\", wr_fp_rz_stats_df, \"team_abbr\"),\n",
    "    (\"DFS DK\", wr_fd_dk_salary_2017_current_df, \"team\")\n",
    "]\n",
    "\n",
    "for label, df, col in dfs_and_cols:\n",
    "    if col in df.columns:\n",
    "        print(f\"\\n[{label}] {col} uniques ({len(df[col].dropna().unique())}):\")\n",
    "        print(sorted(df[col].dropna().unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9890fa-17a4-46a0-afa5-a0ecebe3b0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End: team abbreviation standardization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cc7e4b-e57f-49d3-92d7-727a5469d4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d9c1e-4079-4043-a48a-04d60eb29bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin: data type evaluation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3639d0c-a35f-45cf-968d-9efbbbc238c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save each dataframe to CSV for visual inspection\n",
    "wr_fp_basic_stats_df.to_csv('wr_fp_basic_stats.csv', index=False)\n",
    "wr_fp_advanced_stats_df.to_csv('wr_fp_advanced_stats.csv', index=False)\n",
    "wr_fp_rz_stats_df.to_csv('wr_fp_rz_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988468df-e48c-4f0a-a3fd-c359b197569d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772dbb1-e912-4331-b40b-96bc7f4fb2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_whitespace_columns(df):\n",
    "    df = df.copy()\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        try:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not strip column '{col}': {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa8c269-452b-4702-a944-897cc567b7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163e6fb0-db79-4c63-9d72-5a7b9d732bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase_id_columns(df, keys=['fantasypros_id', 'player_name']):\n",
    "    df = df.copy()\n",
    "    for key in keys:\n",
    "        if key in df.columns:\n",
    "            try:\n",
    "                df[key] = df[key].astype(str).str.lower()\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not lowercase column '{key}': {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d0b8ab-4b6e-4182-af98-d169f2c3832c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3796a0-4122-4dd6-8baf-80874fe84df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert a column to int32, coercing invalid entries to NaN\n",
    "# def clean_integer_column(df, column_name):\n",
    "#     df = df.copy()\n",
    "#     if column_name in df.columns:\n",
    "#         try:\n",
    "#             df[column_name] = (\n",
    "#                 pd.to_numeric(df[column_name], errors='coerce')\n",
    "#                 .astype('Int32')  # Pandas nullable integer\n",
    "#             )\n",
    "#         except Exception as e:\n",
    "#             print(f\"âš ï¸ Could not clean integer column '{column_name}': {e}\")\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41cb0dc-d6dc-4564-9f6c-65cff9c322f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526ddfb-f5e9-4d4c-a7c5-27ae6aaca819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a column to Int32, safely handling empty strings and non-numeric entries\n",
    "# *Note: int32 vs Int32 - Int32 can hanlde NaN\n",
    "def clean_integer_column(df, column_name):\n",
    "    df = df.copy()\n",
    "    if column_name in df.columns:\n",
    "        try:\n",
    "            df[column_name] = (\n",
    "                df[column_name]\n",
    "                .astype(str)                    # Ensure it's string type\n",
    "                .str.strip()                    # Remove extra whitespace\n",
    "                .replace('', np.nan)            # Replace empty string with NaN\n",
    "                .replace('nan', np.nan)         # Optional: if string \"nan\" exists\n",
    "            )\n",
    "            df[column_name] = (\n",
    "                pd.to_numeric(df[column_name], errors='coerce')  # Coerce invalids to NaN\n",
    "                .astype('Int32')                                 # Nullable integer\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Could not clean integer column '{column_name}': {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3aa26b4-8953-4f97-beb8-4eacc29d7f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c9c1e7-0d9f-4f6f-bfca-cc723cb613f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea12f08-c6b8-468f-af96-8e15913c4f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_percentage_columns(df, percent_cols):\n",
    "    df = df.copy()\n",
    "    for col in percent_cols:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = (\n",
    "                    df[col]\n",
    "                    .astype(str)\n",
    "                    .str.replace('%', '', regex=False)\n",
    "                    .str.strip()\n",
    "                    .replace('', np.nan)\n",
    "                    .astype(float) / 100\n",
    "                ).astype('float32')\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Could not convert column '{col}' to float32 percentage: {e}\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5970a3f-e184-4498-b0a8-3a48b97d1b26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b14b9-b115-45db-8132-4cb1373a3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_fp_basic_stats_type_map \n",
    "\n",
    "wr_fp_basic_stats_type_map = {\n",
    "    'season': 'Int32',\n",
    "    'season_type': 'str',\n",
    "    'week': 'Int32',\n",
    "    'fantasypros_id': 'str',\n",
    "    'player_name': 'str',\n",
    "    'team_abbr': 'str',\n",
    "    'Rank': 'Int32',\n",
    "    'Player': 'str',\n",
    "    'REC': 'Int32',\n",
    "    'TGT': 'Int32',\n",
    "    'YDS': 'Int32',\n",
    "    'Y/R': 'float32',\n",
    "    'LG': 'Int32',\n",
    "    '20+': 'Int32',\n",
    "    'TD': 'Int32',\n",
    "    'ATT': 'Int32',\n",
    "    'FL': 'Int32',\n",
    "    'G': 'Int32',\n",
    "    'FPTS': 'float32',\n",
    "    'FPTS/G': 'float32',\n",
    "    'ROST': 'float32'  # Already converted from %\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879d3c2-dc90-4ca0-b252-d660375a1197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050b8179-6fb5-4116-a4ad-8a5fa5df87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_fp_advanced_stats_type_map \n",
    "\n",
    "wr_fp_advanced_stats_type_map = {\n",
    "    'season': 'Int32',\n",
    "    'season_type': 'str',\n",
    "    'week': 'Int32',\n",
    "    'fantasypros_id': 'str',\n",
    "    'player_name': 'str',\n",
    "    'Player': 'str',\n",
    "    'team_abbr': 'str',\n",
    "    'Rank': 'Int32',\n",
    "    'G': 'Int32',\n",
    "    'REC': 'Int32',\n",
    "    'YDS': 'Int32',\n",
    "    'Y/R': 'float32',\n",
    "    'YBC': 'Int32',\n",
    "    'YBC/R': 'float32',\n",
    "    'AIR': 'Int32',\n",
    "    'AIR/R': 'float32',\n",
    "    'YAC': 'Int32',\n",
    "    'YAC/R': 'float32',\n",
    "    'YACON': 'Int32',\n",
    "    'YACON/R': 'float32',\n",
    "    'BRKTKL': 'Int32',\n",
    "    'TGT': 'Int32',\n",
    "    '% TM': 'float32',  # Already converted from %\n",
    "    'CATCHABLE': 'Int32',\n",
    "    'DROP': 'Int32',\n",
    "    'RZ TGT': 'Int32',\n",
    "    '10+ YDS': 'Int32',\n",
    "    '20+ YDS': 'Int32',\n",
    "    '30+ YDS': 'Int32',\n",
    "    '40+ YDS': 'Int32',\n",
    "    '50+ YDS': 'Int32',\n",
    "    'LNG': 'Int32'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc111d7-3b6f-4859-841d-e5fd439f5816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13991ad0-6604-416e-8f37-342f796d495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_fp_rz_stats_type_map \n",
    "\n",
    "wr_fp_rz_stats_type_map = {\n",
    "    'season': 'Int32',\n",
    "    'season_type': 'str',\n",
    "    'week': 'Int32',\n",
    "    'fantasypros_id': 'str',\n",
    "    'player_name': 'str',\n",
    "    'team_abbr': 'str',\n",
    "    'Rank': 'Int32',\n",
    "    'Player': 'str',\n",
    "    'REC': 'Int32',\n",
    "    'TGT': 'Int32',\n",
    "    'REC PCT': 'float32',  # Already converted from %\n",
    "    'YDS': 'Int32',\n",
    "    'Y/R': 'float32',\n",
    "    'TD': 'Int32',\n",
    "    'TGT PCT': 'float32',  # Already converted from %\n",
    "    'ATT': 'Int32',\n",
    "    'PCT': 'float32',      # Already converted from %\n",
    "    'FL': 'Int32',\n",
    "    'G': 'Int32',\n",
    "    'FPTS': 'float32',\n",
    "    'FPTS/G': 'float32',\n",
    "    'ROST %': 'float32'    # Already converted from %\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574eb25-0d4d-4f5c-9f18-0ce174bb9a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e7d057-93bc-41c0-b2c9-a238888dc8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cast_column_types(df, type_map):\n",
    "#     df = df.copy()\n",
    "#     for col, dtype in type_map.items():\n",
    "#         if col in df.columns:\n",
    "#             try:\n",
    "#                 df[col] = df[col].astype(dtype)\n",
    "#             except Exception as e:\n",
    "#                 print(f\"âš ï¸ Warning: could not convert column '{col}' to {dtype}. Reason: {e}\")\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6466a-5d0e-4587-b00d-d8aa3d5c4c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ee4967-e235-406e-a041-6402a4b8b1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast dataframe columns to specified types with error logging.\n",
    "def cast_column_types(df, type_map, df_name=\"DataFrame\", verbose=True):\n",
    "    df = df.copy()\n",
    "    for col, dtype in type_map.items():\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "                if verbose:\n",
    "                    print(f\"âœ… [{df_name}] {col} â†’ {dtype}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  [{df_name}] Failed to convert '{col}' to {dtype}: {e}\")\n",
    "        else:\n",
    "            print(f\"â„¹ï¸  [{df_name}] Column '{col}' not found â€” skipping.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a8785-7002-4973-ac47-c242d035a394",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e7f6b3-f19a-4529-84ba-e1ec3038d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cleaning and normalization to FantasyPros dataframes \n",
    "\n",
    "# Apply to Basic Stats\n",
    "wr_fp_basic_stats_df = wr_fp_basic_stats_df.copy()\n",
    "wr_fp_basic_stats_df = strip_whitespace_columns(wr_fp_basic_stats_df)\n",
    "wr_fp_basic_stats_df = lowercase_id_columns(wr_fp_basic_stats_df)\n",
    "wr_fp_basic_stats_df = convert_percentage_columns(wr_fp_basic_stats_df, ['ROST'])\n",
    "wr_fp_basic_stats_df = cast_column_types(wr_fp_basic_stats_df, wr_fp_basic_stats_type_map, df_name=\"Basic Stats\")\n",
    "print(f\"âœ… wr_fp_basic_stats_df shape: {wr_fp_basic_stats_df.shape}\")\n",
    "\n",
    "# Apply to Advanced Stats\n",
    "wr_fp_advanced_stats_df = wr_fp_advanced_stats_df.copy()\n",
    "wr_fp_advanced_stats_df = strip_whitespace_columns(wr_fp_advanced_stats_df)\n",
    "wr_fp_advanced_stats_df = lowercase_id_columns(wr_fp_advanced_stats_df)\n",
    "wr_fp_advanced_stats_df = convert_percentage_columns(wr_fp_advanced_stats_df, ['% TM'])\n",
    "wr_fp_advanced_stats_df = cast_column_types(wr_fp_advanced_stats_df, wr_fp_advanced_stats_type_map, df_name=\"Advanced Stats\")\n",
    "print(f\"âœ… wr_fp_advanced_stats_df shape: {wr_fp_advanced_stats_df.shape}\")\n",
    "\n",
    "# Apply to Red Zone Stats\n",
    "wr_fp_rz_stats_df = wr_fp_rz_stats_df.copy()\n",
    "wr_fp_rz_stats_df = strip_whitespace_columns(wr_fp_rz_stats_df)\n",
    "wr_fp_rz_stats_df = lowercase_id_columns(wr_fp_rz_stats_df)\n",
    "wr_fp_rz_stats_df = convert_percentage_columns(wr_fp_rz_stats_df, ['REC PCT', 'TGT PCT', 'PCT', 'ROST %'])\n",
    "wr_fp_rz_stats_df = cast_column_types(wr_fp_rz_stats_df, wr_fp_rz_stats_type_map, df_name=\"Red Zone Stats\")\n",
    "print(f\"âœ… wr_fp_rz_stats_df shape: {wr_fp_rz_stats_df.shape}\")\n",
    "\n",
    "\n",
    "# clean the 'G' column\n",
    "wr_fp_basic_stats_df = clean_integer_column(wr_fp_basic_stats_df, 'G')\n",
    "wr_fp_advanced_stats_df = clean_integer_column(wr_fp_advanced_stats_df, 'G')\n",
    "wr_fp_rz_stats_df = clean_integer_column(wr_fp_rz_stats_df, 'G')\n",
    "\n",
    "# csv files\n",
    "wr_fp_basic_stats_df.to_csv(\"wr_fp_basic_stats_cleaned.csv\", index=False)\n",
    "wr_fp_advanced_stats_df.to_csv(\"wr_fp_advanced_stats_cleaned.csv\", index=False)\n",
    "wr_fp_rz_stats_df.to_csv(\"wr_fp_rz_stats_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a1fdcb-7d0d-461f-ae24-2db3418e1b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de4726b-d454-4fa7-a2b5-8427fb8dcd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_fd_dk_salary_2017_current_df.info()\n",
    "wr_fd_dk_salary_2017_current_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ff425-b4f2-41bd-9a26-25c690abe74c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b98d89-cef7-48ef-aa80-69c9c6118a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_fd_dk_salary_type_map\n",
    "\n",
    "wr_fd_dk_salary_type_map = {\n",
    "    'season': 'Int32',\n",
    "    'week': 'Int32',\n",
    "    'player_id': 'str',\n",
    "    'player': 'str',\n",
    "    'dk_position': 'str',\n",
    "    'fd_position': 'str',\n",
    "    'team': 'str',\n",
    "    'opponent': 'str',\n",
    "    'dk_salary': 'Int32',\n",
    "    'fd_salary': 'Int32',\n",
    "    'dk_fpts': 'float32',\n",
    "    'fd_fpts': 'float32'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15882b8e-968c-4772-840a-de8553e11afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708a3721-c6a5-473f-b26f-1d0ec15079a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply type casting to wr_fd_dk_salary_2017_current_df\n",
    "wr_fd_dk_salary_2017_current_df = cast_column_types(\n",
    "    wr_fd_dk_salary_2017_current_df,\n",
    "    wr_fd_dk_salary_type_map\n",
    ")\n",
    "\n",
    "print(f\"âœ… wr_fd_dk_salary_2017_current_df shape: {wr_fd_dk_salary_2017_current_df.shape}\")\n",
    "wr_fd_dk_salary_2017_current_df.to_csv(\"wr_fd_dk_salary_2017_current_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43082784-b336-45e5-af2a-34df217005bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec29c6c-dee2-476e-af88-bdcc780cfe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_ids_weekly_stats_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf722f-3f97-42a7-a4c0-24be545019e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b4d600-7a74-41f8-ae8f-df7a45fb4e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_ids_weekly_stats type map\n",
    "\n",
    "wr_ids_weekly_stats_type_map = {\n",
    "    'season': 'Int32',\n",
    "    'season_type': 'str',\n",
    "    'week': 'Int32',\n",
    "    'player_id': 'str',\n",
    "    'player_name': 'str',\n",
    "    'position': 'str',\n",
    "    'position_group': 'str',\n",
    "    'recent_team': 'str',\n",
    "    'fantasy_points': 'float32',\n",
    "    'fantasy_points_ppr': 'float32',\n",
    "    'pff_id': 'str',\n",
    "    'nfl_id': 'str',\n",
    "    'name': 'str',\n",
    "    'stats_global_id': 'str',\n",
    "    'mfl_id': 'Int32',\n",
    "    'ff_id': 'str',\n",
    "    'cbs_id': 'str',\n",
    "    'fleaflicker_id': 'str',\n",
    "    'sportradar_id': 'str',\n",
    "    'rotoworld_id': 'str',\n",
    "    'sleeper_id': 'str',\n",
    "    'ktc_id': 'str',\n",
    "    'stats_id': 'str',\n",
    "    'fantasypros_id': 'str',\n",
    "    'merge_name': 'str',\n",
    "    'cbfref_id': 'str',\n",
    "    'fantasy_data_id': 'str',\n",
    "    'espn_id': 'str',\n",
    "    'swish_id': 'str',\n",
    "    'rotowire_id': 'str',\n",
    "    'yahoo_id': 'str',\n",
    "    'receptions': 'Int32',\n",
    "    'targets': 'Int32',\n",
    "    'receiving_yards': 'Int32',\n",
    "    'receiving_tds': 'Int32',\n",
    "    'receiving_fumbles': 'float32',\n",
    "    'receiving_fumbles_lost': 'float32',\n",
    "    'receiving_air_yards': 'float32',\n",
    "    'receiving_yards_after_catch': 'float32',\n",
    "    'receiving_first_downs': 'float32',\n",
    "    'receiving_epa': 'float32',\n",
    "    'receiving_2pt_conversions': 'Int32',\n",
    "    'racr': 'float32',\n",
    "    'target_share': 'float32',\n",
    "    'air_yards_share': 'float32',\n",
    "    'wopr': 'float32',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cd2c8-93b5-47a8-9561-ba8cb3aeebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c326e3e9-1aa1-4194-a89d-10b5eecc897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply type casting to wr_ids_weekly_stats_df\n",
    "wr_ids_weekly_stats_df = cast_column_types(\n",
    "    wr_ids_weekly_stats_df,\n",
    "    wr_ids_weekly_stats_type_map\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… wr_ids_weekly_stats_df shape: {wr_ids_weekly_stats_df.shape}\")\n",
    "for col, dtype in wr_ids_weekly_stats_type_map.items():\n",
    "    if col in wr_ids_weekly_stats_df.columns:\n",
    "        actual_dtype = str(wr_ids_weekly_stats_df[col].dtype)\n",
    "        checkmark = \"âœ…\" if actual_dtype == dtype.lower() or actual_dtype == dtype else \"âš ï¸\"\n",
    "        print(f\"{checkmark} [DataFrame] {col} â†’ {actual_dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0aed48-c427-4792-a139-ff2e601423fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be99414-96d0-4b49-a874-550590df2de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_ngs_df.info()\n",
    "wr_ngs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d5e05-d34e-4ca8-814b-2039e3575013",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2477197-1e9f-45a6-9cbc-68d12622b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wr_ngs_df type map\n",
    "\n",
    "wr_ngs_df_type_map = {\n",
    "    # int columns\n",
    "    'season': 'Int32',\n",
    "    'week': 'Int32',\n",
    "    'receptions': 'Int32',\n",
    "    'targets': 'Int32',\n",
    "    'rec_touchdowns': 'Int32',\n",
    "\n",
    "    # float columns\n",
    "    'avg_cushion': 'float32',\n",
    "    'avg_separation': 'float32',\n",
    "    'avg_intended_air_yards': 'float32',\n",
    "    'percent_share_of_intended_air_yards': 'float32',\n",
    "    'catch_percentage': 'float32',\n",
    "    'yards': 'float32',\n",
    "    'avg_yac': 'float32',\n",
    "    'avg_expected_yac': 'float32',\n",
    "    'avg_yac_above_expectation': 'float32',\n",
    "\n",
    "    # object â†’ str\n",
    "    'season_type': 'str',\n",
    "    'player_display_name': 'str',\n",
    "    'player_position': 'str',\n",
    "    'team_abbr': 'str',\n",
    "    'player_gsis_id': 'str',\n",
    "    'player_first_name': 'str',\n",
    "    'player_last_name': 'str',\n",
    "    'player_short_name': 'str'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab16dd5-b73e-46b3-81b5-0955a2c37668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ec53d-ee94-4a41-829a-6de045184194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply type casting to wr_ngs_df using the type map\n",
    "wr_ngs_df = cast_column_types(\n",
    "    wr_ngs_df,\n",
    "    wr_ngs_df_type_map\n",
    ")\n",
    "\n",
    "# Display verification summary\n",
    "print(f\"âœ… wr_ngs_df shape: {wr_ngs_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848424a1-1812-4a2d-a895-0447a113d5da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6743af7f-9e94-44ff-822b-af505d565aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataframes to CSV\n",
    "wr_fd_dk_salary_2017_current_df.to_csv(\"wr_fd_dk_salary_2017_current_cleaned.csv\", index=False)\n",
    "wr_ids_weekly_stats_df.to_csv(\"wr_ids_weekly_stats_cleaned.csv\", index=False)\n",
    "wr_ngs_df.to_csv(\"wr_ngs_df_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ecd35-fb6b-42b0-91bf-a5794b69f3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc53e3-4826-4771-8b28-97b349eecdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bb14c-9168-43b8-bd3f-8869ebbe4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next Tasks\n",
    "# minor refactor of code blocks to adjust where we output csv files\n",
    "# prep to merge with dfs (using modified names \"_dfs\")\n",
    "# there will be 10 dataframes total (dfs and non-dfs)\n",
    "# build the feature engineering list\n",
    "# conduct data normalization analysis in preparation to build the features\n",
    "# data normalization \n",
    "# build the features\n",
    "# eda analysis\n",
    "# monte carlo simulation dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8512a-b397-4518-82f1-3b7c9f872568",
   "metadata": {},
   "outputs": [],
   "source": [
    "## End: data type evaluation and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0880b-48cd-4276-bca9-2b1058f76293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59dc95f-2b82-44f0-a546-cee1bf982666",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: Merge Process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4502f2c0-1e6b-480d-9a8f-46336ef57301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6c20f2-54df-4f79-aa43-2d5c39d716f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a09749-a9a4-4646-bd7c-44a8a104a583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7c4ed9-e359-4380-ac61-00f9f4f6857f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c11453-7685-43e8-8644-f2e53b665d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816912e-6902-46d2-9542-b764d34f8b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfaeab9-39ed-4b22-b527-9c73590ff048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03a057-f85c-4046-9a82-bbd448caf8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: DMerge Process ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095c11f-c3dd-421b-8a01-c4539f9c7da0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1c04e-20bb-4d18-b9d7-101d2e3e40c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d1b57a-214e-4337-b405-fa7ea79b73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: Feature Engineering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b2d35-6966-45de-9295-101e6589d211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5918c54-024a-462b-a59a-f89edcef75b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5df770b-873a-4a36-89af-46751a717271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Rolling Averages and Aggregates - 3,5,and 7 week averages\n",
    "# Ouput: updated dataframe with aggregates (optional csv file)\n",
    "\n",
    "# Start from sorted copy of the main DF\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg = (\n",
    "    wr_nfl_py_fp_odds_salary_merged_mod_cols\n",
    "    .sort_values(by=['name', 'season', 'week'])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# Feature map: full column -> short prefix\n",
    "feature_map = {\n",
    "    'targets': 'tgt',\n",
    "    'receptions': 'rec',\n",
    "    'receiving_yards': 'rec_yds',\n",
    "    'receiving_air_yards': 'rec_air_yards',\n",
    "    'fpts': 'fpts'\n",
    "}\n",
    "\n",
    "windows = [3, 5, 7]\n",
    "\n",
    "# Apply rolling averages and lag features\n",
    "for full_col, short in feature_map.items():\n",
    "    # Group once\n",
    "    grouped = wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg.groupby(['name', 'season'])\n",
    "\n",
    "    # Rolling averages using apply (preserves group boundaries)\n",
    "    for window in windows:\n",
    "        col_name = f\"{short}_{window}wk_avg\"\n",
    "        wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg[col_name] = grouped[full_col].apply(\n",
    "            lambda x: x.rolling(window=window, min_periods=window).mean().shift(1)\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "    # Lag feature (1-game lookback)\n",
    "    lag_col = f\"{short}_lag_1\"\n",
    "    wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg[lag_col] = grouped[full_col].shift(1).reset_index(drop=True)\n",
    "\n",
    "# Final integrity check\n",
    "print(\"âœ… Final shape:\", wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg.shape)\n",
    "\n",
    "# ** csv file **\n",
    "# wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg.to_csv(\n",
    "#     \"wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg.csv\",\n",
    "#     index=False,\n",
    "#     float_format=\"%.2f\"\n",
    "# )\n",
    "# print(\"ðŸ“¤ Exported to: wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4172f1d-360b-4997-a216-fc07d9ad9e03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e7c5f-530b-49db-b42e-7c6b8d4698fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation tests of aggregates\n",
    "# output: there should be no aggregates prior to week 4\n",
    "\n",
    "# Structural Check â€” No aggregates in first 3 weeks of a season\n",
    "def check_early_aggregates(df, cols, earliest_week=4):\n",
    "    early = df[df['week'] < earliest_week]\n",
    "    violations = early[cols].notna().sum()\n",
    "    print(\"ðŸš¨ Aggregates present before week\", earliest_week)\n",
    "    print(violations[violations > 0])\n",
    "\n",
    "# Boundary Check â€” Rolling aggregates must reset per season\n",
    "def check_season_boundaries(df, col_prefix):\n",
    "    errors = []\n",
    "    for short in col_prefix:\n",
    "        col_name = f'{short}_3wk_avg'\n",
    "        season_transitions = df.groupby(['name'])['season'].diff().fillna(0)\n",
    "        cross_season_rows = df[season_transitions != 0]\n",
    "        if cross_season_rows[col_name].notna().any():\n",
    "            errors.append(col_name)\n",
    "    if errors:\n",
    "        print(\"âŒ Rolling values leaked across seasons:\", errors)\n",
    "    else:\n",
    "        print(\"âœ… No cross-season leakage detected.\")\n",
    "\n",
    "# Shape check\n",
    "def check_shape(df, expected_cols_added):\n",
    "    print(\"âœ… Final shape:\", df.shape)\n",
    "    print(\"âœ… Final columns:\", df.columns[-expected_cols_added:])\n",
    "\n",
    "# === Apply Checks ===\n",
    "rolling_cols = [f\"{short}_{w}wk_avg\" for short in ['tgt', 'rec', 'rec_yds', 'rec_air_yards', 'fpts'] for w in [3, 5, 7]]\n",
    "lag_cols = [f\"{short}_lag_1\" for short in ['tgt', 'rec', 'rec_yds', 'rec_air_yards', 'fpts']]\n",
    "check_early_aggregates(wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg, rolling_cols)\n",
    "check_season_boundaries(wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg, ['tgt', 'rec', 'rec_yds', 'rec_air_yards', 'fpts'])\n",
    "check_shape(wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg, expected_cols_added=len(rolling_cols + lag_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99601c20-0c44-4448-b9f1-0c8f93b959cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaac7b9-a788-4365-830f-d1fd6c134711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Trend Features (deltas) - recent performance over / under (3wk, 5wk, 7wk) averages\n",
    "# output: updated dataframe with deltas (optional csv file)\n",
    "\n",
    "# new dataframe\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend = wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg.copy()\n",
    "\n",
    "# Column map\n",
    "feature_map = {\n",
    "    'targets': 'tgt',\n",
    "    'receptions': 'rec',\n",
    "    'receiving_yards': 'rec_yds',\n",
    "    'receiving_air_yards': 'rec_air_yards',\n",
    "    'fpts': 'fpts'\n",
    "}\n",
    "\n",
    "windows = [3, 5, 7]\n",
    "\n",
    "# Create delta (deviation from trend) features\n",
    "for full_col, short in feature_map.items():\n",
    "    for window in windows:\n",
    "        avg_col = f\"{short}_{window}wk_avg\"\n",
    "        delta_col = f\"{short}_{window}wk_delta\"\n",
    "        wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend[delta_col] = (\n",
    "            wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend[full_col] -\n",
    "            wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend[avg_col]\n",
    "        )\n",
    "\n",
    "# Summary and export\n",
    "delta_cols = [f\"{short}_{w}wk_delta\" for short in feature_map.values() for w in windows]\n",
    "print(\"âœ… Added delta columns:\", delta_cols)\n",
    "print(\"âœ… Final shape:\", wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend.shape)\n",
    "\n",
    "# ** csv file **\n",
    "# wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend.to_csv(\n",
    "#     \"wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend.csv\",\n",
    "#     index=False,\n",
    "#     float_format=\"%.2f\"\n",
    "# )\n",
    "# print(\"ðŸ“¤ Exported to: wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6e9b3a-d17a-4a28-bd25-21ec152cd613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf88a393-c300-4b19-805e-51047e7d0689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean columns\n",
    "# output: updated dataframe with booleans (optional csv file)\n",
    "\n",
    "# Start from the previous trend-enhanced dataframe\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool = wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend.copy()\n",
    "\n",
    "# Define boolean columns as 0/1 integers\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['tgt_ge_5'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['targets'] >= 5).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['tgt_ge_7'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['targets'] >= 7).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['rec_ge_5'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['receptions'] >= 5).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['rec_ge_7'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['receptions'] >= 7).astype(int)\n",
    "\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['target_share_ge_20'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['target_share'] >= 0.2).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['target_share_ge_30'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['target_share'] >= 0.3).astype(int)\n",
    "\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['over_100_yds'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['receiving_yards'] >= 100).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['double_digit_targets'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['targets'] >= 10).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['boom_week'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['fpts'] >= 20).astype(int)\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['bust_week'] = (wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['fpts'] < 5).astype(int)\n",
    "\n",
    "# If 'home' is already boolean, convert to int\n",
    "wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['is_home_game'] = wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool['home'].astype(int)\n",
    "\n",
    "# Final shape and column check\n",
    "print(\"âœ… Final shape:\", wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool.shape)\n",
    "print(\"âœ… New boolean columns added.\")\n",
    "\n",
    "# ** csv file **\n",
    "# wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool.to_csv(\n",
    "#     \"wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool.csv\",\n",
    "#     index=False,\n",
    "#     float_format=\"%.2f\"\n",
    "# )\n",
    "# print(\"ðŸ“¤ Exported to: wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd716820-3e9f-44af-8316-6956ffed2276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0797c-d2f6-4e80-ba60-558ab67bf71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Dataframe: this can be used as the final dataframe but the filename is long ***\n",
    "\n",
    "# split the over / under column into two columns: o_u and total\n",
    "# output: updated dataframe with o_u and total columns (optional csv file) \n",
    "\n",
    "# Copy from final boolean-enriched dataframe\n",
    "wr_nfl_py_fp_odds_salary_features = wr_nfl_py_fp_odds_salary_merged_mod_cols_sort_agg_trend_bool.copy()\n",
    "\n",
    "# Extract 'O' or 'U' and map to \"over\"/\"under\"\n",
    "wr_nfl_py_fp_odds_salary_features['O_U'] = (\n",
    "    wr_nfl_py_fp_odds_salary_features['over_under']\n",
    "    .str[0]\n",
    "    .map({'O': 'over', 'U': 'under'})\n",
    ")\n",
    "\n",
    "# Extract the numeric total (handles int or float)\n",
    "wr_nfl_py_fp_odds_salary_features['Total'] = (\n",
    "    wr_nfl_py_fp_odds_salary_features['over_under']\n",
    "    .str.extract(r'(\\d+\\.?\\d*)')[0]\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "# Validation\n",
    "print(\"âœ… Final shape:\", wr_nfl_py_fp_odds_salary_features.shape)\n",
    "print(\"âœ… Sample 'O_U' values:\", wr_nfl_py_fp_odds_salary_features['O_U'].unique())\n",
    "print(\"âœ… Sample 'Total' values:\", wr_nfl_py_fp_odds_salary_features['Total'].dropna().unique()[:5])\n",
    "\n",
    "# ** csv file **\n",
    "# wr_nfl_py_fp_odds_salary_features.to_csv(\n",
    "#     \"wr_nfl_py_fp_odds_salary_features.csv\",\n",
    "#     index=False,\n",
    "#     float_format=\"%.2f\"\n",
    "# )\n",
    "# print(\"ðŸ“¤ Exported to: wr_nfl_py_fp_odds_salary_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ac6436-fb49-4166-811b-1380758a2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: Feature Engineering ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363e794f-b2aa-43e5-9e13-815bc671bbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162abe0-3554-405f-9b15-be14328fbc39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d51cd8-eb86-430e-97b1-621c9fb03aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b01e389-2ea9-4f9d-a803-c449744971b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d3e70-8f72-45d5-991f-59ffd18e9cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
