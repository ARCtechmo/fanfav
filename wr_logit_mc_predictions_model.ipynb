{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207224fc-7cea-4eee-b051-17e1fed63e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression classification modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e48ba7f-3a4d-4493-aedc-c6b8ef388528",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** INPUT REQUIRED **\n",
    "# add filtering option for backtest or live predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6d314e-7d7d-44e1-95ff-03200ab828a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f991fce9-8d29-4a10-a191-2adafc08a396",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from fpdf import FPDF\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba76926-8514-4148-aba3-40cba938a20b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "722c38cc-8141-4f30-b79b-386a38725396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "wr_df_raw = pd.read_csv(\"wr_nfl_df_sorted_new_features_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb9391-351f-47d0-8fd7-f250383772b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d36271-2b01-4f46-882c-3252ea5c2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create working copy\n",
    "wr_df = wr_df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9d393-b3e8-459b-880c-d2be8bcdd955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf665f96-fdd0-4f9e-9d4b-eb57efa86e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: feature engineering - imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5122be3-2e36-4606-8702-d5f3cf337422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6926e43-7d8a-41da-ac3f-8b7031973075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Columns confirmed to be dropped: ['receiving_fumbles', 'receiving_fumbles_lost', 'receiving_drop', 'receiving_drop_pct', 'g', 'rec', 'yds', 'ybc', 'air', 'yac', 'yacon', 'brktkl', 'tgt', 'catchable', 'rz tgt', '10+ yds', '20+ yds', '30+ yds', '40+ yds', '50+ yds', 'rec pct_rz', 'y/r_rz', 'tgt pct_rz', 'team_abbr_x', 'result', 'score', 'macro_tier_score_season', 'macro_tier_score_missing', 'macro_tier_season', 'over_under', 'O_U']\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering on fantasypros stats\n",
    "# imputation and drop unnecessary columns\n",
    "\n",
    "# Reapply 'drop' logic using 'receiving_drop' and 'targets'\n",
    "drop_mask = wr_df['drop'].isna() & wr_df['receiving_drop'].notna() & wr_df['targets'].notna() & (wr_df['targets'] > 0)\n",
    "zero_targets_mask = wr_df['targets'] == 0\n",
    "\n",
    "# If drop is missing and targets > 0, use receiving_drop\n",
    "wr_df.loc[drop_mask, 'drop'] = wr_df.loc[drop_mask, 'receiving_drop']\n",
    "\n",
    "# If targets == 0, set drop = 0\n",
    "wr_df.loc[zero_targets_mask, 'drop'] = 0\n",
    "\n",
    "# Failsafe: fill any remaining nulls with 0\n",
    "wr_df['drop'] = wr_df['drop'].fillna(0)\n",
    "\n",
    "# Drop confirmed redundant or low-value columns\n",
    "columns_to_drop = [\n",
    "    'receiving_fumbles', 'receiving_fumbles_lost', 'receiving_drop', 'receiving_drop_pct',\n",
    "    'g', 'rec', 'yds', 'ybc', 'air', 'yac', 'yacon', 'brktkl', 'tgt', 'catchable',\n",
    "    'rz tgt', '10+ yds', '20+ yds', '30+ yds', '40+ yds', '50+ yds',\n",
    "    'rec pct_rz', 'y/r_rz', 'tgt pct_rz', 'team_abbr_x', 'result', 'score', 'macro_tier_score_season', \n",
    "    'macro_tier_score_missing', 'macro_tier_season', 'over_under', 'O_U'\n",
    "\n",
    "]\n",
    "\n",
    "# Preview the actual columns that will be dropped\n",
    "to_drop_confirmed = [col for col in columns_to_drop if col in wr_df.columns]\n",
    "print(\"✅ Columns confirmed to be dropped:\", to_drop_confirmed)\n",
    "\n",
    "\n",
    "# Drop only if columns are present in the current frame\n",
    "wr_df.drop(columns=[col for col in columns_to_drop if col in wr_df.columns], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750e14d-886f-4fea-b106-0eabf96fa9df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57bfb086-cb3e-4be0-ac10-60c9cdf05182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['season',\n",
       " 'season_type',\n",
       " 'week',\n",
       " 'name',\n",
       " 'position',\n",
       " 'recent_team',\n",
       " 'player_display_name',\n",
       " 'rost',\n",
       " 'dk_salary',\n",
       " 'fd_salary',\n",
       " 'value_ratio_dk',\n",
       " 'value_ratio_fd',\n",
       " 'value_ratio_dk_log',\n",
       " 'value_ratio_fd_log',\n",
       " 'value_ratio_dk_log_z',\n",
       " 'value_ratio_fd_log_z',\n",
       " 'fpts',\n",
       " 'pos_avg_fpts',\n",
       " 'fpts_above_pos_avg',\n",
       " 'expected_fpts_dk',\n",
       " 'fpts_diff_dk',\n",
       " 'hit_value_dk',\n",
       " 'rolling_fpts_diff_dk',\n",
       " 'z_fpts_diff_dk',\n",
       " 'z_value_ratio_dk',\n",
       " 'expected_fpts_fd',\n",
       " 'fpts_diff_fd',\n",
       " 'hit_value_fd',\n",
       " 'rolling_fpts_diff_fd',\n",
       " 'z_fpts_diff_fd',\n",
       " 'z_value_ratio_fd',\n",
       " 'double_digit_targets',\n",
       " 'boom_week',\n",
       " 'bust_week',\n",
       " 'is_macro_high_tier',\n",
       " 'is_macro_mid_tier',\n",
       " 'is_macro_low_tier',\n",
       " 'opponent_abbr',\n",
       " 'home',\n",
       " 'role',\n",
       " 'spread',\n",
       " 'is_home_game',\n",
       " 'Total',\n",
       " 'receptions',\n",
       " 'receiving_yards',\n",
       " 'yards',\n",
       " 'receiving_yards_after_catch',\n",
       " 'targets',\n",
       " 'target_share',\n",
       " 'target_share_z',\n",
       " 'catch_percentage',\n",
       " 'catch_percentage_scaled',\n",
       " 'catch_percentage_scaled_z',\n",
       " 'avg_cushion',\n",
       " 'avg_separation',\n",
       " 'avg_yac',\n",
       " 'avg_expected_yac',\n",
       " 'avg_yac_above_expectation',\n",
       " 'receiving_broken_tackles',\n",
       " 'receiving_rat',\n",
       " 'receiving_air_yards',\n",
       " 'air_yards_share',\n",
       " 'avg_intended_air_yards',\n",
       " 'percent_share_of_intended_air_yards',\n",
       " 'receiving_tds',\n",
       " 'rec_touchdowns',\n",
       " 'receiving_first_downs',\n",
       " 'receiving_epa',\n",
       " 'receiving_2pt_conversions',\n",
       " 'racr',\n",
       " 'wopr',\n",
       " 'drop',\n",
       " 'rec_rz',\n",
       " 'tgt_rz',\n",
       " 'tgt_3wk_avg',\n",
       " 'tgt_5wk_avg',\n",
       " 'tgt_7wk_avg',\n",
       " 'tgt_lag_1',\n",
       " 'rec_3wk_avg',\n",
       " 'rec_5wk_avg',\n",
       " 'rec_7wk_avg',\n",
       " 'rec_lag_1',\n",
       " 'rec_yds_3wk_avg',\n",
       " 'rec_yds_5wk_avg',\n",
       " 'rec_yds_7wk_avg',\n",
       " 'rec_yds_lag_1',\n",
       " 'rec_air_yards_3wk_avg',\n",
       " 'rec_air_yards_5wk_avg',\n",
       " 'rec_air_yards_7wk_avg',\n",
       " 'rec_air_yards_lag_1',\n",
       " 'fpts_3wk_avg',\n",
       " 'fpts_5wk_avg',\n",
       " 'fpts_7wk_avg',\n",
       " 'fpts_lag_1',\n",
       " 'tgt_3wk_delta',\n",
       " 'tgt_5wk_delta',\n",
       " 'tgt_7wk_delta',\n",
       " 'rec_3wk_delta',\n",
       " 'rec_5wk_delta',\n",
       " 'rec_7wk_delta',\n",
       " 'rec_yds_3wk_delta',\n",
       " 'rec_yds_5wk_delta',\n",
       " 'rec_yds_7wk_delta',\n",
       " 'rec_air_yards_3wk_delta',\n",
       " 'rec_air_yards_5wk_delta',\n",
       " 'rec_air_yards_7wk_delta',\n",
       " 'fpts_3wk_delta',\n",
       " 'fpts_5wk_delta',\n",
       " 'fpts_7wk_delta',\n",
       " 'tgt_ge_5',\n",
       " 'tgt_ge_7',\n",
       " 'rec_ge_5',\n",
       " 'rec_ge_7',\n",
       " 'target_share_ge_20',\n",
       " 'target_share_ge_30',\n",
       " 'over_100_yds',\n",
       " 'rec_air_yards_7wk_avg_clipped',\n",
       " 'fpts_7wk_avg_z',\n",
       " 'rec_7wk_avg_z',\n",
       " 'tgt_7wk_avg_z',\n",
       " 'rec_yds_7wk_avg_z',\n",
       " 'rec_air_yards_7wk_avg_clipped_z',\n",
       " 'fpts_75th_percentile_1wk',\n",
       " 'fpts_3wk',\n",
       " 'fpts_games_played_3wk',\n",
       " 'fpts_75th_percentile_3wk',\n",
       " 'fpts_90th_percentile_3wk',\n",
       " 'fpts_95th_percentile_3wk',\n",
       " 'fpts_5wk',\n",
       " 'fpts_games_played_5wk',\n",
       " 'fpts_75th_percentile_5wk',\n",
       " 'fpts_90th_percentile_5wk',\n",
       " 'fpts_95th_percentile_5wk',\n",
       " 'fpts_7wk',\n",
       " 'fpts_games_played_7wk',\n",
       " 'fpts_75th_percentile_7wk',\n",
       " 'fpts_90th_percentile_7wk',\n",
       " 'fpts_95th_percentile_7wk',\n",
       " 'rec_75th_percentile_1wk',\n",
       " 'rec_3wk',\n",
       " 'rec_games_played_3wk',\n",
       " 'rec_75th_percentile_3wk',\n",
       " 'rec_90th_percentile_3wk',\n",
       " 'rec_95th_percentile_3wk',\n",
       " 'rec_5wk',\n",
       " 'rec_games_played_5wk',\n",
       " 'rec_75th_percentile_5wk',\n",
       " 'rec_90th_percentile_5wk',\n",
       " 'rec_95th_percentile_5wk',\n",
       " 'rec_7wk',\n",
       " 'rec_games_played_7wk',\n",
       " 'rec_75th_percentile_7wk',\n",
       " 'rec_90th_percentile_7wk',\n",
       " 'rec_95th_percentile_7wk',\n",
       " 'receiving_yards_75th_percentile_1wk',\n",
       " 'receiving_yards_3wk',\n",
       " 'receiving_yards_games_played_3wk',\n",
       " 'receiving_yards_75th_percentile_3wk',\n",
       " 'receiving_yards_90th_percentile_3wk',\n",
       " 'receiving_yards_95th_percentile_3wk',\n",
       " 'receiving_yards_5wk',\n",
       " 'receiving_yards_games_played_5wk',\n",
       " 'receiving_yards_75th_percentile_5wk',\n",
       " 'receiving_yards_90th_percentile_5wk',\n",
       " 'receiving_yards_95th_percentile_5wk',\n",
       " 'receiving_yards_7wk',\n",
       " 'receiving_yards_games_played_7wk',\n",
       " 'receiving_yards_75th_percentile_7wk',\n",
       " 'receiving_yards_90th_percentile_7wk',\n",
       " 'receiving_yards_95th_percentile_7wk',\n",
       " 'value_ratio_dk_75th_percentile_1wk',\n",
       " 'value_ratio_dk_3wk',\n",
       " 'value_ratio_dk_games_played_3wk',\n",
       " 'value_ratio_dk_75th_percentile_3wk',\n",
       " 'value_ratio_dk_90th_percentile_3wk',\n",
       " 'value_ratio_dk_95th_percentile_3wk',\n",
       " 'value_ratio_dk_5wk',\n",
       " 'value_ratio_dk_games_played_5wk',\n",
       " 'value_ratio_dk_75th_percentile_5wk',\n",
       " 'value_ratio_dk_90th_percentile_5wk',\n",
       " 'value_ratio_dk_95th_percentile_5wk',\n",
       " 'value_ratio_dk_7wk',\n",
       " 'value_ratio_dk_games_played_7wk',\n",
       " 'value_ratio_dk_75th_percentile_7wk',\n",
       " 'value_ratio_dk_90th_percentile_7wk',\n",
       " 'value_ratio_dk_95th_percentile_7wk',\n",
       " 'targets_75th_percentile_1wk',\n",
       " 'targets_3wk',\n",
       " 'targets_games_played_3wk',\n",
       " 'targets_75th_percentile_3wk',\n",
       " 'targets_90th_percentile_3wk',\n",
       " 'targets_95th_percentile_3wk',\n",
       " 'targets_5wk',\n",
       " 'targets_games_played_5wk',\n",
       " 'targets_75th_percentile_5wk',\n",
       " 'targets_90th_percentile_5wk',\n",
       " 'targets_95th_percentile_5wk',\n",
       " 'targets_7wk',\n",
       " 'targets_games_played_7wk',\n",
       " 'targets_75th_percentile_7wk',\n",
       " 'targets_90th_percentile_7wk',\n",
       " 'targets_95th_percentile_7wk',\n",
       " 'fpts_performance_bin',\n",
       " 'value_ratio_dk_log_performance_bin',\n",
       " 'value_ratio_fd_log_performance_bin',\n",
       " 'receptions_performance_bin',\n",
       " 'receiving_yards_performance_bin',\n",
       " 'targets_performance_bin',\n",
       " 'target_share_performance_bin',\n",
       " 'catch_percentage_performance_bin',\n",
       " 'avg_cushion_performance_bin',\n",
       " 'avg_separation_performance_bin',\n",
       " 'avg_intended_air_yards_performance_bin',\n",
       " 'percent_share_of_intended_air_yards_performance_bin']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the remaining columns after the drop\n",
    "remaining_columns = wr_df.columns.tolist()\n",
    "remaining_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369d3a17-b162-4224-a4b0-0ae1ab886afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "312fcf6d-202f-44bf-9237-12d2c3935364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step1_after_column_drop.csv\n"
     ]
    }
   ],
   "source": [
    "# csv output check\n",
    "# Export the dataframe after column drop to CSV\n",
    "wr_df.to_csv(\"step1_after_column_drop.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step1_after_column_drop.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcb218-0fea-42cc-93fd-c63a66e6014e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ea0e01b-739a-4c38-af5d-260f2e507686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed (true recorded stats)\n",
    "observed_stats = [\n",
    "    'targets', 'receptions', 'receiving_yards', 'receiving_yards_after_catch',\n",
    "    'receiving_air_yards', 'receiving_tds', 'rec_touchdowns',\n",
    "    'receiving_first_downs', 'receiving_epa', 'receiving_2pt_conversions',\n",
    "    'fpts', 'catch_percentage', 'avg_cushion', 'avg_separation',\n",
    "    'avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation',\n",
    "    'avg_intended_air_yards', 'percent_share_of_intended_air_yards',\n",
    "    'receiving_broken_tackles'\n",
    "]\n",
    "\n",
    "# Salary and roster\n",
    "salary_fields = ['dk_salary', 'fd_salary']\n",
    "roster_fields = ['rost']\n",
    "\n",
    "# Metadata (unchanged throughout pipeline)\n",
    "metadata = [\n",
    "    'season', 'season_type', 'week', 'name', 'player_display_name',\n",
    "    'position', 'recent_team', 'opponent_abbr', 'role', 'home', 'is_home_game'\n",
    "]\n",
    "\n",
    "# Dynamic categories\n",
    "booleans_flags = [col for col in wr_df.columns if col.startswith('is_') or col.endswith('_ge_5') or col.endswith('_ge_7') or col.startswith('boom_') or col.startswith('bust_') or col.startswith('double_digit') or col.endswith('over_100_yds')]\n",
    "performance_bins = [col for col in wr_df.columns if col.endswith('_performance_bin')]\n",
    "rolling_stats = [col for col in wr_df.columns if any(sub in col for sub in ['_avg', '_lag', '_delta', '_games_played'])]\n",
    "percentile_stats = [col for col in wr_df.columns if 'percentile' in col]\n",
    "\n",
    "# Derived = numeric columns not already categorized\n",
    "categorized_cols = set(observed_stats + salary_fields + roster_fields + metadata + booleans_flags + performance_bins + rolling_stats + percentile_stats)\n",
    "derived_features = [col for col in wr_df.columns if col not in categorized_cols and wr_df[col].dtype in ['float64', 'int64']]\n",
    "derived_features.extend(['hit_value_dk', 'hit_value_fd'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d0f75-605a-4109-916c-45c320ca724b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a54ce1f8-7505-4316-9f19-f6fc0b947942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'column_count_after_drop': 214,\n",
       " 'total_categorized_columns': 214,\n",
       " 'match': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm shape after cleaning\n",
    "column_count_after_drop = wr_df.shape[1]\n",
    "\n",
    "# Rebuild the category_lists dictionary from previous block\n",
    "category_lists = {\n",
    "    \"observed_stats\": observed_stats,\n",
    "    \"salary_fields\": salary_fields,\n",
    "    \"roster_fields\": roster_fields,\n",
    "    \"metadata\": metadata,\n",
    "    \"booleans_flags\": booleans_flags,\n",
    "    \"performance_bins\": performance_bins,\n",
    "    \"rolling_stats\": rolling_stats,\n",
    "    \"derived_features\": derived_features,\n",
    "    \"percentile_stats\": percentile_stats\n",
    "}\n",
    "\n",
    "# Flatten all categorized columns into one set\n",
    "all_categorized_columns = set().union(*category_lists.values())\n",
    "\n",
    "# Compare\n",
    "{\n",
    "    \"column_count_after_drop\": column_count_after_drop,\n",
    "    \"total_categorized_columns\": len(all_categorized_columns),\n",
    "    \"match\": column_count_after_drop == len(all_categorized_columns)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3e2409-c223-4d0b-a8eb-533c667d075e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbf22654-be9c-48dc-967d-7b356d05dba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Uncategorized columns: set()\n"
     ]
    }
   ],
   "source": [
    "# Reveal uncategorized columns\n",
    "uncategorized_columns = set(wr_df.columns) - all_categorized_columns\n",
    "print(\"🔍 Uncategorized columns:\", uncategorized_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13824bd-ccf3-4be9-946e-69fa8ae3a9b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aac8c345-6178-4324-ab0b-a09ff2132f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Rows</th>\n",
       "      <th>Active Rows</th>\n",
       "      <th>Inactive Rows</th>\n",
       "      <th>Active %</th>\n",
       "      <th>Inactive %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17449</td>\n",
       "      <td>17429</td>\n",
       "      <td>20</td>\n",
       "      <td>99.89</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Rows  Active Rows  Inactive Rows  Active %  Inactive %\n",
       "0       17449        17429             20     99.89        0.11"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define is_active using the refined logic\n",
    "wr_df['is_active'] = (\n",
    "    (wr_df['dk_salary'].fillna(0) > 0) |\n",
    "    (wr_df['fd_salary'].fillna(0) > 0) |\n",
    "    (wr_df['targets'].fillna(0) > 0) |\n",
    "    (wr_df['receptions'].fillna(0) > 0) |\n",
    "    (wr_df['receiving_yards'].fillna(0) > 0) |\n",
    "    (wr_df['fpts'].fillna(0) > 0)\n",
    ")\n",
    "\n",
    "# Count how many players are considered active\n",
    "active_count = wr_df['is_active'].sum()\n",
    "total_count = wr_df.shape[0]\n",
    "inactive_count = total_count - active_count\n",
    "\n",
    "# Display summary as a DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Total Rows\": [total_count],\n",
    "    \"Active Rows\": [active_count],\n",
    "    \"Inactive Rows\": [inactive_count],\n",
    "    \"Active %\": [round(100 * active_count / total_count, 2)],\n",
    "    \"Inactive %\": [round(100 * inactive_count / total_count, 2)]\n",
    "})\n",
    "\n",
    "summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc660a9-1e1a-4d58-b133-86ecbb131305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "834b3251-c490-456f-aba9-e84cc2a43b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero out or False-out specified columns for rows where is_active is False.\n",
    "def apply_default_zeros(df, column_groups):\n",
    "\n",
    "    for group_name, cols in column_groups.items():\n",
    "        for col in cols:\n",
    "            if col in df.columns:\n",
    "                if df[col].dtype == 'bool':\n",
    "                    df.loc[~df['is_active'], col] = False\n",
    "                else:\n",
    "                    df.loc[~df['is_active'], col] = 0\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6348e70a-8483-4fbd-889c-8296ae09c714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04de6822-8734-43f6-b618-228898d787ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Begin block-by-block imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8287e5b4-489a-4f66-a2bc-85b54d5bd34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing 'player_display_name' using 'name'\n",
    "wr_df['player_display_name'] = wr_df['player_display_name'].fillna(wr_df['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3f060-d306-44aa-86ac-f8d2af860e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a312ed87-436a-46b3-a6ac-a7ae3d34f1f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       "          name player_display_name\n",
       " 0  A.J. Green          A.J. Green\n",
       " 1  A.J. Green          A.J. Green\n",
       " 2  A.J. Green          A.J. Green\n",
       " 3  A.J. Green          A.J. Green\n",
       " 4  A.J. Green          A.J. Green\n",
       " 5  A.J. Green          A.J. Green\n",
       " 6  A.J. Green          A.J. Green\n",
       " 7  A.J. Green          A.J. Green\n",
       " 8  A.J. Green          A.J. Green\n",
       " 9  A.J. Green          A.J. Green)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many missing values remain in 'player_display_name' after the imputation\n",
    "missing_display_name = wr_df['player_display_name'].isna().sum()\n",
    "\n",
    "# Display a few rows where the original imputation was applied (i.e., name and player_display_name were previously not equal)\n",
    "imputed_rows = wr_df[wr_df['player_display_name'] == wr_df['name']][['name', 'player_display_name']].head(10)\n",
    "\n",
    "missing_display_name, imputed_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15674ab-afca-4a92-b12f-510856e6590f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5bf12247-8f42-4e35-a222-316098a79bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step2_after_player_display_name_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing 'player_display_name'\n",
    "wr_df.to_csv(\"step2_after_player_display_name_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step2_after_player_display_name_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a35ced-180d-4679-8c78-b0b2281d9321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "056d5a02-2787-4129-acdb-f7ea327159ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>catch_percentage</th>\n",
       "      <td>8199.0</td>\n",
       "      <td>63.015331</td>\n",
       "      <td>19.083549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>77.8</td>\n",
       "      <td>100.0</td>\n",
       "      <td>9250</td>\n",
       "      <td>53.011634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch_percentage_scaled</th>\n",
       "      <td>8199.0</td>\n",
       "      <td>0.633138</td>\n",
       "      <td>0.195842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9250</td>\n",
       "      <td>53.011634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catch_percentage_scaled_z</th>\n",
       "      <td>8199.0</td>\n",
       "      <td>-0.006403</td>\n",
       "      <td>1.004702</td>\n",
       "      <td>-3.3</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.9</td>\n",
       "      <td>9250</td>\n",
       "      <td>53.011634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count       mean        std  min   25%   50%  \\\n",
       "catch_percentage           8199.0  63.015331  19.083549  0.0  50.0  62.5   \n",
       "catch_percentage_scaled    8199.0   0.633138   0.195842  0.0   0.5   0.6   \n",
       "catch_percentage_scaled_z  8199.0  -0.006403   1.004702 -3.3  -0.7   0.0   \n",
       "\n",
       "                            75%    max  missing_count  missing_percent  \n",
       "catch_percentage           77.8  100.0           9250        53.011634  \n",
       "catch_percentage_scaled     0.8    1.0           9250        53.011634  \n",
       "catch_percentage_scaled_z   0.8    1.9           9250        53.011634  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Stats - catch_percentage\n",
    "\n",
    "# Get statistical summary of 'catch_percentage' and its scaled versions\n",
    "catch_pct_cols = ['catch_percentage', 'catch_percentage_scaled', 'catch_percentage_scaled_z']\n",
    "catch_pct_stats = wr_df[catch_pct_cols].describe().T\n",
    "\n",
    "# Add missing value counts\n",
    "catch_pct_stats['missing_count'] = wr_df[catch_pct_cols].isna().sum()\n",
    "catch_pct_stats['missing_percent'] = wr_df[catch_pct_cols].isna().mean() * 100\n",
    "\n",
    "catch_pct_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bcc7c9-ee2e-4f86-909e-226252d44a88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca06d739-07b2-4901-81be-5107c7eda83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8964"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many rows have missing catch_percentage but have valid receptions and targets\n",
    "\n",
    "# Check how many valid values we would get using 'receptions' and 'targets'\n",
    "correct_mask = (\n",
    "    wr_df['catch_percentage'].isna() &\n",
    "    wr_df['receptions'].notna() &\n",
    "    wr_df['targets'].notna() &\n",
    "    (wr_df['targets'] > 0)\n",
    ")\n",
    "\n",
    "correct_count = correct_mask.sum()\n",
    "\n",
    "# recalculate number of rows fields\n",
    "correct_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4a678-ae9d-43cc-8f9a-7a2c83ded31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c05c3300-a548-4620-b1e5-0e6ab619242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in catch_percentage receptions / targets\n",
    "\n",
    "# Apply the corrected calculation\n",
    "wr_df.loc[correct_mask, 'catch_percentage'] = (\n",
    "    wr_df.loc[correct_mask, 'receptions'] / wr_df.loc[correct_mask, 'targets']\n",
    ") * 100\n",
    "\n",
    "# Recompute the scaled version\n",
    "wr_df['catch_percentage_scaled'] = wr_df['catch_percentage'] / 100\n",
    "\n",
    "# Recompute the z-score version\n",
    "scaled_mean = wr_df['catch_percentage_scaled'].mean(skipna=True)\n",
    "scaled_std = wr_df['catch_percentage_scaled'].std(skipna=True)\n",
    "wr_df['catch_percentage_scaled_z'] = (wr_df['catch_percentage_scaled'] - scaled_mean) / scaled_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65503bb4-02ea-4fc1-955f-91958f65ca2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e640b9e6-3940-482d-8bf9-81f8c381c44c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catch_percentage             286\n",
       "catch_percentage_scaled      286\n",
       "catch_percentage_scaled_z    286\n",
       "dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify catch_percentage results\n",
    "# Re-check how many missing values remain in the three catch_percentage-related columns\n",
    "final_missing_summary = wr_df[['catch_percentage', 'catch_percentage_scaled', 'catch_percentage_scaled_z']].isna().sum()\n",
    "final_missing_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37410ea-56b9-4a5d-ad1a-5a430b74a61a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "58e87b4a-a3b5-42d9-b48b-de49f693030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>receptions</th>\n",
       "      <th>targets</th>\n",
       "      <th>catch_percentage</th>\n",
       "      <th>catch_percentage_scaled</th>\n",
       "      <th>catch_percentage_scaled_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5917</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.055820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16682</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.397307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7442</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.261205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5059</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>-0.951250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11055</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.261205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12587</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.261205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3542</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>72.700000</td>\n",
       "      <td>0.727000</td>\n",
       "      <td>0.355657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11884</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.155530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.950145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4592</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>77.800000</td>\n",
       "      <td>0.778000</td>\n",
       "      <td>0.524826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       receptions  targets  catch_percentage  catch_percentage_scaled  \\\n",
       "5917            0        2          0.000000                 0.000000   \n",
       "16682           1        2         50.000000                 0.500000   \n",
       "7442            5        5        100.000000                 1.000000   \n",
       "5059            2        6         33.300000                 0.333000   \n",
       "11055           1        1        100.000000                 1.000000   \n",
       "12587           4        4        100.000000                 1.000000   \n",
       "3542            8       11         72.700000                 0.727000   \n",
       "11884           2        3         66.666667                 0.666667   \n",
       "3609            1        3         33.333333                 0.333333   \n",
       "4592            7        9         77.800000                 0.778000   \n",
       "\n",
       "       catch_percentage_scaled_z  \n",
       "5917                   -2.055820  \n",
       "16682                  -0.397307  \n",
       "7442                    1.261205  \n",
       "5059                   -0.951250  \n",
       "11055                   1.261205  \n",
       "12587                   1.261205  \n",
       "3542                    0.355657  \n",
       "11884                   0.155530  \n",
       "3609                   -0.950145  \n",
       "4592                    0.524826  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check df\n",
    "# Sample a few rows with valid values to verify the imputation and scaling logic\n",
    "check_rows = wr_df[\n",
    "    wr_df['catch_percentage'].notna() &\n",
    "    wr_df['catch_percentage_scaled'].notna() &\n",
    "    wr_df['catch_percentage_scaled_z'].notna()\n",
    "][['receptions', 'targets', 'catch_percentage', 'catch_percentage_scaled', 'catch_percentage_scaled_z']].sample(10)\n",
    "\n",
    "display(check_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537ce020-5e29-4cac-a5af-d3cc631c90a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a628495-dcba-4ecb-b20c-e92e2d974f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>receptions</th>\n",
       "      <th>targets</th>\n",
       "      <th>catch_percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Alex Erickson</td>\n",
       "      <td>8</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>ArDarius Stewart</td>\n",
       "      <td>5</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>ArDarius Stewart</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Bernard Reedy</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Bernard Reedy</td>\n",
       "      <td>7</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Bobo Wilson</td>\n",
       "      <td>13</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Bobo Wilson</td>\n",
       "      <td>17</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Braxton Miller</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Chad Williams</td>\n",
       "      <td>14</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Curtis Samuel</td>\n",
       "      <td>6</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name  week  season  receptions  targets  catch_percentage\n",
       "81      Alex Erickson     8    2017           0        0               NaN\n",
       "174  ArDarius Stewart     5    2017           0        0               NaN\n",
       "177  ArDarius Stewart    13    2017           0        0               NaN\n",
       "198     Bernard Reedy     6    2017           0        0               NaN\n",
       "199     Bernard Reedy     7    2017           0        0               NaN\n",
       "201       Bobo Wilson    13    2017           0        0               NaN\n",
       "203       Bobo Wilson    17    2017           0        0               NaN\n",
       "268    Braxton Miller    10    2017           0        0               NaN\n",
       "333     Chad Williams    14    2017           0        0               NaN\n",
       "460     Curtis Samuel     6    2017           0        0               NaN"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show 10 sample rows where catch_percentage is still missing\n",
    "remaining_na_rows = wr_df[wr_df['catch_percentage'].isna()][\n",
    "    ['name', 'week', 'season', 'receptions', 'targets', 'catch_percentage']\n",
    "].head(10)\n",
    "\n",
    "remaining_na_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02a9696-0fcf-4d67-b033-a1f43b75e7f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "51c96fd5-bb70-4df0-ab9c-ee2e579268c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Imputation for remaining missing values\n",
    "# Define masks for remaining missing values\n",
    "mask_catch_pct = wr_df['catch_percentage'].isna()\n",
    "\n",
    "# Impute dummy values\n",
    "wr_df.loc[mask_catch_pct, 'catch_percentage'] = -10.0\n",
    "wr_df.loc[mask_catch_pct, 'catch_percentage_scaled'] = -0.1\n",
    "wr_df.loc[mask_catch_pct, 'catch_percentage_scaled_z'] = -4.0\n",
    "\n",
    "# Add boolean flags for each\n",
    "wr_df['is_missing_catch_pct'] = mask_catch_pct.astype(int)\n",
    "wr_df['is_missing_catch_pct_scaled'] = mask_catch_pct.astype(int)\n",
    "wr_df['is_missing_catch_pct_z'] = mask_catch_pct.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d671b3-73e5-4ff7-952c-b3cdf7fae6fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b83ac3a9-449f-41c8-88bc-c7b4798ac1ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catch_percentage             0\n",
       "catch_percentage_scaled      0\n",
       "catch_percentage_scaled_z    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify no missing values\n",
    "# Re-check how many missing values remain in the catch_percentage feature group\n",
    "final_check = wr_df[\n",
    "    ['catch_percentage', 'catch_percentage_scaled', 'catch_percentage_scaled_z']\n",
    "].isna().sum()\n",
    "\n",
    "final_check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec78b9-290a-4334-9335-2fe6d938bf22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b3d6c926-9aad-4513-b9f4-09376ad1438a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>receptions</th>\n",
       "      <th>targets</th>\n",
       "      <th>catch_percentage</th>\n",
       "      <th>catch_percentage_scaled</th>\n",
       "      <th>catch_percentage_scaled_z</th>\n",
       "      <th>is_missing_catch_pct</th>\n",
       "      <th>is_missing_catch_pct_scaled</th>\n",
       "      <th>is_missing_catch_pct_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16559</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11085</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6091</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9720</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12485</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5024</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10454</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       receptions  targets  catch_percentage  catch_percentage_scaled  \\\n",
       "16559           0        0             -10.0                     -0.1   \n",
       "11085           0        0             -10.0                     -0.1   \n",
       "6091            0        0             -10.0                     -0.1   \n",
       "9720            0        0             -10.0                     -0.1   \n",
       "12485           0        0             -10.0                     -0.1   \n",
       "5495            0        0             -10.0                     -0.1   \n",
       "7995            0        0             -10.0                     -0.1   \n",
       "5024            0        0             -10.0                     -0.1   \n",
       "14367           0        0             -10.0                     -0.1   \n",
       "10454           0        0             -10.0                     -0.1   \n",
       "\n",
       "       catch_percentage_scaled_z  is_missing_catch_pct  \\\n",
       "16559                       -4.0                     1   \n",
       "11085                       -4.0                     1   \n",
       "6091                        -4.0                     1   \n",
       "9720                        -4.0                     1   \n",
       "12485                       -4.0                     1   \n",
       "5495                        -4.0                     1   \n",
       "7995                        -4.0                     1   \n",
       "5024                        -4.0                     1   \n",
       "14367                       -4.0                     1   \n",
       "10454                       -4.0                     1   \n",
       "\n",
       "       is_missing_catch_pct_scaled  is_missing_catch_pct_z  \n",
       "16559                            1                       1  \n",
       "11085                            1                       1  \n",
       "6091                             1                       1  \n",
       "9720                             1                       1  \n",
       "12485                            1                       1  \n",
       "5495                             1                       1  \n",
       "7995                             1                       1  \n",
       "5024                             1                       1  \n",
       "14367                            1                       1  \n",
       "10454                            1                       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check df\n",
    "# Confirm that all dummy rows were flagged and values assigned correctly\n",
    "check_dummy_rows = wr_df[wr_df['is_missing_catch_pct'] == 1][[\n",
    "    'receptions', 'targets', 'catch_percentage',\n",
    "    'catch_percentage_scaled', 'catch_percentage_scaled_z',\n",
    "    'is_missing_catch_pct', 'is_missing_catch_pct_scaled', 'is_missing_catch_pct_z'\n",
    "]].sample(10)\n",
    "\n",
    "display(check_dummy_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06fd25e-1d92-41e6-b93d-75c5eacfe144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9449515d-beee-4edb-8107-f211ce301fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step3_after_catch_percentage_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing catch_percentage and related fields\n",
    "wr_df.to_csv(\"step3_after_catch_percentage_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step3_after_catch_percentage_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d020e6c-1757-4fa8-aed9-7413c1b309c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d6c7f947-3da6-44b8-a902-7edb97e35ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_cushion</th>\n",
       "      <td>8197.0</td>\n",
       "      <td>6.044809</td>\n",
       "      <td>1.505897</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>14.4</td>\n",
       "      <td>9252</td>\n",
       "      <td>53.023096</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count      mean       std  min  25%  50%  75%   max  \\\n",
       "avg_cushion  8197.0  6.044809  1.505897  1.9  5.0  6.0  7.1  14.4   \n",
       "\n",
       "             missing_count  missing_percent  \n",
       "avg_cushion           9252        53.023096  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Stats - avg_cushion\n",
    "\n",
    "# Get statistical summary of 'avg_cushion'\n",
    "cushion_stats = wr_df[['avg_cushion']].describe().T\n",
    "\n",
    "# Add missing count and percent\n",
    "cushion_stats['missing_count'] = wr_df['avg_cushion'].isna().sum()\n",
    "cushion_stats['missing_percent'] = wr_df['avg_cushion'].isna().mean() * 100\n",
    "\n",
    "cushion_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dafa1d-10b7-4d3e-9b56-dadb7422c053",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ce7b578e-58d2-45fe-b117-1b9d33a92cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Imputation for avg_cushion\n",
    "\n",
    "# Identify missing values\n",
    "mask_avg_cushion = wr_df['avg_cushion'].isna()\n",
    "\n",
    "# Impute with dummy value\n",
    "wr_df.loc[mask_avg_cushion, 'avg_cushion'] = -1.0\n",
    "\n",
    "# Add boolean flag\n",
    "wr_df['is_missing_avg_cushion'] = mask_avg_cushion.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf600c-0b01-4dcf-bb2d-d46a54934b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95fabd26-e8dd-43b3-a88f-b20083f1c7c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify no missing values remain in 'avg_cushion'\n",
    "missing_avg_cushion = wr_df['avg_cushion'].isna().sum()\n",
    "missing_avg_cushion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad1ab9d-30d5-4fcf-8162-14bab5e73437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bcb4b9ef-5e81-42ac-a18c-f0e4c93dad69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_cushion</th>\n",
       "      <th>is_missing_avg_cushion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14795</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10937</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1270</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3286</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13795</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8606</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_cushion  is_missing_avg_cushion\n",
       "14795         -1.0                       1\n",
       "10937         -1.0                       1\n",
       "1270          -1.0                       1\n",
       "3286          -1.0                       1\n",
       "4838          -1.0                       1\n",
       "4860          -1.0                       1\n",
       "6693          -1.0                       1\n",
       "13795         -1.0                       1\n",
       "8606          -1.0                       1\n",
       "8333          -1.0                       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check df\n",
    "check_cushion = wr_df[wr_df['is_missing_avg_cushion'] == 1][['avg_cushion', 'is_missing_avg_cushion']].sample(10)\n",
    "display(check_cushion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39ed36-b5a7-4847-bfd2-5d9a66dd6c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5e551aea-d767-4683-966a-6e9827157676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step4_after_avg_cushion_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing avg_cushion\n",
    "wr_df.to_csv(\"step4_after_avg_cushion_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step4_after_avg_cushion_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a269bddf-f9f8-42b5-99b6-0e0f82fc251c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "193457a9-1a2a-4381-8435-376d9a0d403d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_separation</th>\n",
       "      <td>8199.0</td>\n",
       "      <td>2.862666</td>\n",
       "      <td>0.937429</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>8.7</td>\n",
       "      <td>9250</td>\n",
       "      <td>53.011634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count      mean       std  min  25%  50%  75%  max  \\\n",
       "avg_separation  8199.0  2.862666  0.937429  0.6  2.2  2.8  3.4  8.7   \n",
       "\n",
       "                missing_count  missing_percent  \n",
       "avg_separation           9250        53.011634  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Stats - avg_separation\n",
    "\n",
    "# check for the next feature: 'avg_separation'\n",
    "separation_stats = wr_df[['avg_separation']].describe().T\n",
    "separation_stats['missing_count'] = wr_df['avg_separation'].isna().sum()\n",
    "separation_stats['missing_percent'] = wr_df['avg_separation'].isna().mean() * 100\n",
    "\n",
    "separation_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35ed8d-a6cd-4c44-ae21-ccb5bcf85bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85945036-618b-4610-aeb5-66024600715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Imputation for avg_separation\n",
    "\n",
    "# Identify missing values\n",
    "mask_avg_separation = wr_df['avg_separation'].isna()\n",
    "\n",
    "# Impute with dummy value\n",
    "wr_df.loc[mask_avg_separation, 'avg_separation'] = -1.0\n",
    "\n",
    "# Add boolean flag\n",
    "wr_df['is_missing_avg_separation'] = mask_avg_separation.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc008f4d-f68d-4de5-9c1b-38b14ef95d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "76cd64e2-8b3f-477c-a42a-88fa4ad0ee70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify that 'avg_separation' has no remaining missing values\n",
    "missing_avg_separation = wr_df['avg_separation'].isna().sum()\n",
    "missing_avg_separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e271c8cc-c02e-4c8c-aa5c-2a49696f5046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "517533ca-94cd-441b-a628-1a390d48263f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_separation</th>\n",
       "      <th>is_missing_avg_separation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17282</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12321</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11570</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3373</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16782</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7818</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11984</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8067</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4255</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_separation  is_missing_avg_separation\n",
       "4781             -1.0                          1\n",
       "17282            -1.0                          1\n",
       "12321            -1.0                          1\n",
       "11570            -1.0                          1\n",
       "3373             -1.0                          1\n",
       "16782            -1.0                          1\n",
       "7818             -1.0                          1\n",
       "11984            -1.0                          1\n",
       "8067             -1.0                          1\n",
       "4255             -1.0                          1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check df\n",
    "check_separation = wr_df[wr_df['is_missing_avg_separation'] == 1][['avg_separation', 'is_missing_avg_separation']].sample(10)\n",
    "display(check_separation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24eabc7c-fbdf-416f-8f83-3ee1636a6ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e3ee848c-64c7-4d73-b722-fff2bb67e7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step5_after_avg_separation_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing avg_separation\n",
    "wr_df.to_csv(\"step5_after_avg_separation_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step5_after_avg_separation_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ce88dc-fea5-472d-81be-1877a5930906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ed1d02f3-b0d0-4442-ba19-1d7de9672299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>avg_yac</th>\n",
       "      <td>8167.0</td>\n",
       "      <td>4.492580</td>\n",
       "      <td>3.456574</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>43.2</td>\n",
       "      <td>9282</td>\n",
       "      <td>53.195026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_expected_yac</th>\n",
       "      <td>8158.0</td>\n",
       "      <td>3.916082</td>\n",
       "      <td>2.313900</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>19.1</td>\n",
       "      <td>9291</td>\n",
       "      <td>53.246604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>avg_yac_above_expectation</th>\n",
       "      <td>8158.0</td>\n",
       "      <td>0.578500</td>\n",
       "      <td>2.335047</td>\n",
       "      <td>-9.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>37.7</td>\n",
       "      <td>9291</td>\n",
       "      <td>53.246604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            count      mean       std  min  25%  50%  75%  \\\n",
       "avg_yac                    8167.0  4.492580  3.456574 -2.6  2.3  3.8  5.9   \n",
       "avg_expected_yac           8158.0  3.916082  2.313900  0.1  2.3  3.5  5.1   \n",
       "avg_yac_above_expectation  8158.0  0.578500  2.335047 -9.9 -0.6  0.2  1.2   \n",
       "\n",
       "                            max  missing_count  missing_percent  \n",
       "avg_yac                    43.2           9282        53.195026  \n",
       "avg_expected_yac           19.1           9291        53.246604  \n",
       "avg_yac_above_expectation  37.7           9291        53.246604  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive stats for NGS-related features\n",
    "next_ngs_features = ['avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation']\n",
    "next_ngs_stats = wr_df[next_ngs_features].describe().T\n",
    "next_ngs_stats['missing_count'] = wr_df[next_ngs_features].isna().sum()\n",
    "next_ngs_stats['missing_percent'] = wr_df[next_ngs_features].isna().mean() * 100\n",
    "\n",
    "next_ngs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb9a4e7-a8a4-4c67-8b48-996d6832c23b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6d5784b5-dc3b-4531-8083-11f730956b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - apply imputation for YAC-related Next Gen Stats\n",
    "\n",
    "# Identify missing masks for each column\n",
    "mask_yac = wr_df['avg_yac'].isna()\n",
    "mask_exp_yac = wr_df['avg_expected_yac'].isna()\n",
    "mask_yac_diff = wr_df['avg_yac_above_expectation'].isna()\n",
    "\n",
    "# Apply smart imputation values\n",
    "wr_df.loc[mask_yac, 'avg_yac'] = -5.0\n",
    "wr_df.loc[mask_exp_yac, 'avg_expected_yac'] = -1.0\n",
    "wr_df.loc[mask_yac_diff, 'avg_yac_above_expectation'] = -10.0\n",
    "\n",
    "# Add boolean flags\n",
    "wr_df['is_missing_avg_yac'] = mask_yac.astype(int)\n",
    "wr_df['is_missing_avg_expected_yac'] = mask_exp_yac.astype(int)\n",
    "wr_df['is_missing_avg_yac_above_expectation'] = mask_yac_diff.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558abf92-cbf3-4858-9531-cb8dff8eee20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "84118c74-756c-4c28-9128-8d0681568c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_yac                      0\n",
       "avg_expected_yac             0\n",
       "avg_yac_above_expectation    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final verification: confirm no missing values remain in the YAC-related fields\n",
    "yac_verification = wr_df[\n",
    "    ['avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation']\n",
    "].isna().sum()\n",
    "\n",
    "yac_verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c3e3af-323c-4f90-a961-378e7235fd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b903124a-ae4d-4f5d-a8a6-11e7de70213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>avg_yac</th>\n",
       "      <th>avg_expected_yac</th>\n",
       "      <th>avg_yac_above_expectation</th>\n",
       "      <th>is_missing_avg_yac</th>\n",
       "      <th>is_missing_avg_expected_yac</th>\n",
       "      <th>is_missing_avg_yac_above_expectation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7155</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10091</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14646</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9186</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10380</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5229</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11247</th>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       avg_yac  avg_expected_yac  avg_yac_above_expectation  \\\n",
       "5567      -5.0              -1.0                      -10.0   \n",
       "12119     -5.0              -1.0                      -10.0   \n",
       "7155      -5.0              -1.0                      -10.0   \n",
       "10091     -5.0              -1.0                      -10.0   \n",
       "14646     -5.0              -1.0                      -10.0   \n",
       "9186      -5.0              -1.0                      -10.0   \n",
       "10380     -5.0              -1.0                      -10.0   \n",
       "106       -5.0              -1.0                      -10.0   \n",
       "5229      -5.0              -1.0                      -10.0   \n",
       "11247     -5.0              -1.0                      -10.0   \n",
       "\n",
       "       is_missing_avg_yac  is_missing_avg_expected_yac  \\\n",
       "5567                    1                            1   \n",
       "12119                   1                            1   \n",
       "7155                    1                            1   \n",
       "10091                   1                            1   \n",
       "14646                   1                            1   \n",
       "9186                    1                            1   \n",
       "10380                   1                            1   \n",
       "106                     1                            1   \n",
       "5229                    1                            1   \n",
       "11247                   1                            1   \n",
       "\n",
       "       is_missing_avg_yac_above_expectation  \n",
       "5567                                      1  \n",
       "12119                                     1  \n",
       "7155                                      1  \n",
       "10091                                     1  \n",
       "14646                                     1  \n",
       "9186                                      1  \n",
       "10380                                     1  \n",
       "106                                       1  \n",
       "5229                                      1  \n",
       "11247                                     1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check df\n",
    "check_yac = wr_df[wr_df['is_missing_avg_yac'] == 1][[\n",
    "    'avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation',\n",
    "    'is_missing_avg_yac', 'is_missing_avg_expected_yac', 'is_missing_avg_yac_above_expectation'\n",
    "]].sample(10)\n",
    "display(check_yac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e9319a-4b1d-4f06-afed-2e08d6c9fcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5b2ed7b0-fe90-406e-a953-0ba65a1b693a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step6_after_yac_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing YAC-related fields\n",
    "wr_df.to_csv(\"step6_after_yac_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step6_after_yac_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a39634c-527e-4504-a81b-159910033a82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5de8ba8b-5065-4f03-aa9d-6b3694372d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>receiving_broken_tackles</th>\n",
       "      <td>14675.0</td>\n",
       "      <td>0.164634</td>\n",
       "      <td>0.480238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2774</td>\n",
       "      <td>15.897759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving_air_yards</th>\n",
       "      <td>17449.0</td>\n",
       "      <td>52.960399</td>\n",
       "      <td>45.542857</td>\n",
       "      <td>-32.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>334.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving_epa</th>\n",
       "      <td>17163.0</td>\n",
       "      <td>1.139678</td>\n",
       "      <td>3.896251</td>\n",
       "      <td>-23.1</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>23.6</td>\n",
       "      <td>286</td>\n",
       "      <td>1.639062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving_2pt_conversions</th>\n",
       "      <td>17449.0</td>\n",
       "      <td>0.011920</td>\n",
       "      <td>0.108531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving_first_downs</th>\n",
       "      <td>17449.0</td>\n",
       "      <td>1.882973</td>\n",
       "      <td>1.778176</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receiving_tds</th>\n",
       "      <td>17449.0</td>\n",
       "      <td>0.237836</td>\n",
       "      <td>0.499024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec_touchdowns</th>\n",
       "      <td>8199.0</td>\n",
       "      <td>0.377607</td>\n",
       "      <td>0.605227</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9250</td>\n",
       "      <td>53.011634</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             count       mean        std   min   25%   50%  \\\n",
       "receiving_broken_tackles   14675.0   0.164634   0.480238   0.0   0.0   0.0   \n",
       "receiving_air_yards        17449.0  52.960399  45.542857 -32.0  17.0  43.0   \n",
       "receiving_epa              17163.0   1.139678   3.896251 -23.1  -1.0   0.8   \n",
       "receiving_2pt_conversions  17449.0   0.011920   0.108531   0.0   0.0   0.0   \n",
       "receiving_first_downs      17449.0   1.882973   1.778176   0.0   1.0   1.0   \n",
       "receiving_tds              17449.0   0.237836   0.499024   0.0   0.0   0.0   \n",
       "rec_touchdowns              8199.0   0.377607   0.605227   0.0   0.0   0.0   \n",
       "\n",
       "                            75%    max  missing_count  missing_percent  \n",
       "receiving_broken_tackles    0.0    7.0           2774        15.897759  \n",
       "receiving_air_yards        78.0  334.0              0         0.000000  \n",
       "receiving_epa               3.2   23.6            286         1.639062  \n",
       "receiving_2pt_conversions   0.0    1.0              0         0.000000  \n",
       "receiving_first_downs       3.0   14.0              0         0.000000  \n",
       "receiving_tds               0.0    4.0              0         0.000000  \n",
       "rec_touchdowns              1.0    4.0           9250        53.011634  "
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define important and secondary NGS features for evaluation\n",
    "important_ngs = [\n",
    "    'receiving_broken_tackles', 'receiving_air_yards',\n",
    "    'receiving_epa', 'receiving_2pt_conversions'\n",
    "]\n",
    "\n",
    "potential_duplicates = [\n",
    "    'receiving_first_downs', 'receiving_tds', 'rec_touchdowns'\n",
    "]\n",
    "\n",
    "all_ngs = important_ngs + potential_duplicates\n",
    "\n",
    "# Generate descriptive statistics and missing value summary\n",
    "ngs_stats = wr_df[all_ngs].describe().T\n",
    "ngs_stats['missing_count'] = wr_df[all_ngs].isna().sum()\n",
    "ngs_stats['missing_percent'] = wr_df[all_ngs].isna().mean() * 100\n",
    "\n",
    "ngs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6e951-2e8c-4cb4-9aed-27e05397a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "47239b22-b598-4495-94e1-27b0375c3361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rec_touchdowns = receiving_tds if missing\n",
    "\n",
    "# Identify rows where rec_touchdowns is missing but receiving_tds is available\n",
    "mask_rec_td_missing = wr_df['rec_touchdowns'].isna() & wr_df['receiving_tds'].notna()\n",
    "\n",
    "# Fill missing rec_touchdowns using receiving_tds\n",
    "wr_df.loc[mask_rec_td_missing, 'rec_touchdowns'] = wr_df.loc[mask_rec_td_missing, 'receiving_tds']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e990f5-e40c-4eb4-8b58-8b0c6c3427b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "59db4f7b-82c0-4046-b29b-eef6c9d54990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step7_after_rec_touchdowns_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing rec_touchdowns using receiving_tds\n",
    "wr_df.to_csv(\"step7_after_rec_touchdowns_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step7_after_rec_touchdowns_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebcb9b-7351-40ea-805c-81870f86ec70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9ea7b605-3386-4c73-9bff-9cdf13913150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - imputation for receiving_air_yards \n",
    "\n",
    "# Identify masks for missing values\n",
    "mask_air_yards = wr_df['receiving_air_yards'].isna()\n",
    "mask_epa = wr_df['receiving_epa'].isna()\n",
    "\n",
    "# Apply smart imputation logic for air yards\n",
    "# If receptions or targets are 0, set air yards to 0\n",
    "zero_air_mask = (\n",
    "    (wr_df['receptions'] == 0) | (wr_df['targets'] == 0)\n",
    ") & mask_air_yards\n",
    "\n",
    "# For remaining missing air yards (actual NGS gaps), use -10.0\n",
    "missing_air_yards_mask = mask_air_yards & ~zero_air_mask\n",
    "\n",
    "# Apply both types of imputations\n",
    "wr_df.loc[zero_air_mask, 'receiving_air_yards'] = 0\n",
    "wr_df.loc[missing_air_yards_mask, 'receiving_air_yards'] = -10.0\n",
    "\n",
    "# Add flag for imputed (non-zero) air yards only\n",
    "wr_df['is_missing_receiving_air_yards'] = missing_air_yards_mask.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616f915-349c-450e-aaa8-93dc2a067bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b9068c6d-c1d2-430e-aa2e-75f132c08695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step8_after_receiving_air_yards_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing receiving_air_yards\n",
    "wr_df.to_csv(\"step8a_after_receiving_air_yards_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step8_after_receiving_air_yards_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bdaf5f-6ed4-4271-bbdd-eb8360a3a19c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "14db5f19-b61a-462d-9876-c5ca57855c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing in receiving_epa: 0\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - imputation receiving_epa\n",
    "\n",
    "# Identify missing values\n",
    "mask_epa = wr_df['receiving_epa'].isna()\n",
    "\n",
    "# Impute with a distinct dummy value (-30.0 falls well below the observed min of -23.1)\n",
    "wr_df.loc[mask_epa, 'receiving_epa'] = -30.0\n",
    "\n",
    "# Add boolean flag for rows that were imputed\n",
    "wr_df['is_missing_receiving_epa'] = mask_epa.astype(int)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(\"Remaining missing in receiving_epa:\", wr_df['receiving_epa'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4f745-547f-4952-9b5f-364d3f994524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "30075b85-90e1-4901-bfee-bb2c1520b9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step9_after_receiving_epa_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing receiving_epa\n",
    "wr_df.to_csv(\"step9_after_receiving_epa_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step9_after_receiving_epa_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98794478-011f-48f1-94aa-4e126b93b6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e7fe0117-64fc-4e02-b52e-47c04ff7056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation for receiving_broken_tackles\n",
    "\n",
    "# Identify missing values\n",
    "mask_broken_tackles = wr_df['receiving_broken_tackles'].isna()\n",
    "\n",
    "# Impute with dummy value (sentinel)\n",
    "wr_df.loc[mask_broken_tackles, 'receiving_broken_tackles'] = -1\n",
    "\n",
    "# Add boolean flag — always add, even if currently no missing\n",
    "wr_df['is_missing_receiving_broken_tackles'] = mask_broken_tackles.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a394af-e5a6-40c3-8940-98740652d26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b4aacd33-d5a5-4799-80de-553f8e1cd9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confirm no missing values remain in 'receiving_broken_tackles'\n",
    "missing_broken_tackles = wr_df['receiving_broken_tackles'].isna().sum()\n",
    "missing_broken_tackles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3217b5c-296e-4b11-b4be-4be5ebd96fed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c42107ff-3c13-4aeb-816f-575a09b14462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step10_after_receiving_broken_tackles_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after imputing receiving_broken_tackles\n",
    "wr_df.to_csv(\"step10_after_receiving_broken_tackles_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step10_after_receiving_broken_tackles_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beed31f-5d1b-4798-9cbe-4382f5ada235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "409c07b9-f2e1-461e-98fe-c3fcc1050fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "catch_percentage                        0\n",
       "is_missing_avg_yac                      0\n",
       "receiving_broken_tackles                0\n",
       "receiving_tds                           0\n",
       "rec_touchdowns                          0\n",
       "receiving_2pt_conversions               0\n",
       "is_missing_receiving_epa                0\n",
       "receiving_epa                           0\n",
       "is_missing_receiving_air_yards          0\n",
       "receiving_air_yards                     0\n",
       "is_missing_avg_yac_above_expectation    0\n",
       "is_missing_avg_expected_yac             0\n",
       "avg_yac_above_expectation               0\n",
       "catch_percentage_scaled                 0\n",
       "avg_expected_yac                        0\n",
       "avg_yac                                 0\n",
       "is_missing_avg_separation               0\n",
       "avg_separation                          0\n",
       "is_missing_avg_cushion                  0\n",
       "avg_cushion                             0\n",
       "is_missing_catch_pct_z                  0\n",
       "is_missing_catch_pct_scaled             0\n",
       "is_missing_catch_pct                    0\n",
       "catch_percentage_scaled_z               0\n",
       "is_missing_receiving_broken_tackles     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify imputations for NGS stats\n",
    "# Gather all columns that were either imputed or created as flags\n",
    "imputation_columns = [\n",
    "    \n",
    "    # Catch percentage trio + flags\n",
    "    'catch_percentage', 'catch_percentage_scaled', 'catch_percentage_scaled_z',\n",
    "    'is_missing_catch_pct', 'is_missing_catch_pct_scaled', 'is_missing_catch_pct_z',\n",
    "    \n",
    "    # Spatial stats + flags\n",
    "    'avg_cushion', 'is_missing_avg_cushion',\n",
    "    'avg_separation', 'is_missing_avg_separation',\n",
    "    \n",
    "    # YAC stats + flags\n",
    "    'avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation',\n",
    "    'is_missing_avg_yac', 'is_missing_avg_expected_yac', 'is_missing_avg_yac_above_expectation',\n",
    "    \n",
    "    # Other receiving stats + flags\n",
    "    'receiving_air_yards', 'is_missing_receiving_air_yards',\n",
    "    'receiving_epa', 'is_missing_receiving_epa',\n",
    "    'receiving_2pt_conversions', 'rec_touchdowns', 'receiving_tds',\n",
    "\n",
    "    # broken tackles\n",
    "    'receiving_broken_tackles',\n",
    "    'is_missing_receiving_broken_tackles'\n",
    "]\n",
    "\n",
    "# Extract sample rows for visual verification\n",
    "imputed_preview = wr_df[imputation_columns].head(10)\n",
    "\n",
    "# Summary of missing values in all imputed columns\n",
    "missing_summary = wr_df[imputation_columns].isna().sum().sort_values(ascending=False)\n",
    "\n",
    "missing_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390b35ef-452a-4db8-bf81-6561003af864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "67d44523-238e-4f7a-b873-d1b8e8731df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>week</th>\n",
       "      <th>season</th>\n",
       "      <th>catch_percentage</th>\n",
       "      <th>catch_percentage_scaled</th>\n",
       "      <th>catch_percentage_scaled_z</th>\n",
       "      <th>avg_cushion</th>\n",
       "      <th>avg_separation</th>\n",
       "      <th>avg_yac</th>\n",
       "      <th>avg_expected_yac</th>\n",
       "      <th>...</th>\n",
       "      <th>is_missing_catch_pct_scaled</th>\n",
       "      <th>is_missing_catch_pct_z</th>\n",
       "      <th>is_missing_avg_cushion</th>\n",
       "      <th>is_missing_avg_separation</th>\n",
       "      <th>is_missing_avg_yac</th>\n",
       "      <th>is_missing_avg_expected_yac</th>\n",
       "      <th>is_missing_avg_yac_above_expectation</th>\n",
       "      <th>is_missing_receiving_air_yards</th>\n",
       "      <th>is_missing_receiving_epa</th>\n",
       "      <th>is_missing_receiving_broken_tackles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>Tyler Boyd</td>\n",
       "      <td>16</td>\n",
       "      <td>2022</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.431949</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906</th>\n",
       "      <td>Travis Rudolph</td>\n",
       "      <td>17</td>\n",
       "      <td>2017</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-1.226564</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5928</th>\n",
       "      <td>Taylor Gabriel</td>\n",
       "      <td>11</td>\n",
       "      <td>2019</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.397307</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14258</th>\n",
       "      <td>Kayshon Boutte</td>\n",
       "      <td>12</td>\n",
       "      <td>2023</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.397307</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9061</th>\n",
       "      <td>Dee Eskridge</td>\n",
       "      <td>14</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.055820</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14339</th>\n",
       "      <td>Lil'Jordan Humphrey</td>\n",
       "      <td>1</td>\n",
       "      <td>2023</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.261205</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>Kalif Raymond</td>\n",
       "      <td>9</td>\n",
       "      <td>2019</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.155530</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16100</th>\n",
       "      <td>Jalen Brooks</td>\n",
       "      <td>2</td>\n",
       "      <td>2024</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-0.950145</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>Tyron Johnson</td>\n",
       "      <td>13</td>\n",
       "      <td>2020</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.261205</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8510</th>\n",
       "      <td>Ben Skowronek</td>\n",
       "      <td>14</td>\n",
       "      <td>2021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.055820</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  week  season  catch_percentage  \\\n",
       "12741           Tyler Boyd    16    2022         75.000000   \n",
       "1906        Travis Rudolph    17    2017         25.000000   \n",
       "5928        Taylor Gabriel    11    2019         50.000000   \n",
       "14258       Kayshon Boutte    12    2023         50.000000   \n",
       "9061          Dee Eskridge    14    2021          0.000000   \n",
       "14339  Lil'Jordan Humphrey     1    2023        100.000000   \n",
       "5237         Kalif Raymond     9    2019         66.666667   \n",
       "16100         Jalen Brooks     2    2024         33.333333   \n",
       "8238         Tyron Johnson    13    2020        100.000000   \n",
       "8510         Ben Skowronek    14    2021          0.000000   \n",
       "\n",
       "       catch_percentage_scaled  catch_percentage_scaled_z  avg_cushion  \\\n",
       "12741                 0.750000                   0.431949         -1.0   \n",
       "1906                  0.250000                  -1.226564         -1.0   \n",
       "5928                  0.500000                  -0.397307          7.5   \n",
       "14258                 0.500000                  -0.397307         -1.0   \n",
       "9061                  0.000000                  -2.055820         -1.0   \n",
       "14339                 1.000000                   1.261205         -1.0   \n",
       "5237                  0.666667                   0.155530         -1.0   \n",
       "16100                 0.333333                  -0.950145         -1.0   \n",
       "8238                  1.000000                   1.261205         -1.0   \n",
       "8510                  0.000000                  -2.055820         -1.0   \n",
       "\n",
       "       avg_separation  avg_yac  avg_expected_yac  ...  \\\n",
       "12741            -1.0     -5.0              -1.0  ...   \n",
       "1906             -1.0     -5.0              -1.0  ...   \n",
       "5928              2.6      3.3               3.4  ...   \n",
       "14258            -1.0     -5.0              -1.0  ...   \n",
       "9061             -1.0     -5.0              -1.0  ...   \n",
       "14339            -1.0     -5.0              -1.0  ...   \n",
       "5237             -1.0     -5.0              -1.0  ...   \n",
       "16100            -1.0     -5.0              -1.0  ...   \n",
       "8238             -1.0     -5.0              -1.0  ...   \n",
       "8510             -1.0     -5.0              -1.0  ...   \n",
       "\n",
       "       is_missing_catch_pct_scaled  is_missing_catch_pct_z  \\\n",
       "12741                            0                       0   \n",
       "1906                             0                       0   \n",
       "5928                             0                       0   \n",
       "14258                            0                       0   \n",
       "9061                             0                       0   \n",
       "14339                            0                       0   \n",
       "5237                             0                       0   \n",
       "16100                            0                       0   \n",
       "8238                             0                       0   \n",
       "8510                             0                       0   \n",
       "\n",
       "       is_missing_avg_cushion  is_missing_avg_separation  is_missing_avg_yac  \\\n",
       "12741                       1                          1                   1   \n",
       "1906                        1                          1                   1   \n",
       "5928                        0                          0                   0   \n",
       "14258                       1                          1                   1   \n",
       "9061                        1                          1                   1   \n",
       "14339                       1                          1                   1   \n",
       "5237                        1                          1                   1   \n",
       "16100                       1                          1                   1   \n",
       "8238                        1                          1                   1   \n",
       "8510                        1                          1                   1   \n",
       "\n",
       "       is_missing_avg_expected_yac  is_missing_avg_yac_above_expectation  \\\n",
       "12741                            1                                     1   \n",
       "1906                             1                                     1   \n",
       "5928                             0                                     0   \n",
       "14258                            1                                     1   \n",
       "9061                             1                                     1   \n",
       "14339                            1                                     1   \n",
       "5237                             1                                     1   \n",
       "16100                            1                                     1   \n",
       "8238                             1                                     1   \n",
       "8510                             1                                     1   \n",
       "\n",
       "       is_missing_receiving_air_yards  is_missing_receiving_epa  \\\n",
       "12741                               0                         0   \n",
       "1906                                0                         0   \n",
       "5928                                0                         0   \n",
       "14258                               0                         0   \n",
       "9061                                0                         0   \n",
       "14339                               0                         0   \n",
       "5237                                0                         0   \n",
       "16100                               0                         0   \n",
       "8238                                0                         0   \n",
       "8510                                0                         0   \n",
       "\n",
       "       is_missing_receiving_broken_tackles  \n",
       "12741                                    0  \n",
       "1906                                     1  \n",
       "5928                                     1  \n",
       "14258                                    0  \n",
       "9061                                     0  \n",
       "14339                                    0  \n",
       "5237                                     0  \n",
       "16100                                    0  \n",
       "8238                                     0  \n",
       "8510                                     0  \n",
       "\n",
       "[10 rows x 28 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check df\n",
    "# Combine all boolean imputation flags\n",
    "flags = [col for col in wr_df.columns if col.startswith('is_missing_')]\n",
    "\n",
    "# Select rows where at least one flag is triggered\n",
    "imputed_rows = wr_df[wr_df[flags].sum(axis=1) > 0]\n",
    "\n",
    "# Display selected columns from imputed rows\n",
    "cols_to_check = [\n",
    "    'name', 'week', 'season'\n",
    "] + [col for col in imputation_columns if not col.startswith('is_missing_')] + flags\n",
    "\n",
    "# Show a sample for visual inspection\n",
    "display(imputed_rows[cols_to_check].sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7d0e3-7999-4434-8ace-d69aa52fda18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "9af187ea-a725-4100-8661-455df0fb2b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering - Imputation for the value ratio group\n",
    "# Identify columns for imputation\n",
    "value_ratio_cols = [\n",
    "    'value_ratio_dk', 'value_ratio_fd',\n",
    "    'value_ratio_dk_log', 'value_ratio_fd_log',\n",
    "    'value_ratio_dk_log_z'\n",
    "]\n",
    "\n",
    "# Impute value_ratio and log variants with 0.0\n",
    "wr_df['value_ratio_dk'] = wr_df['value_ratio_dk'].fillna(0.0)\n",
    "wr_df['value_ratio_fd'] = wr_df['value_ratio_fd'].fillna(0.0)\n",
    "wr_df['value_ratio_dk_log'] = wr_df['value_ratio_dk_log'].fillna(0.0)\n",
    "wr_df['value_ratio_fd_log'] = wr_df['value_ratio_fd_log'].fillna(0.0)\n",
    "\n",
    "# Impute z-score variant with an extreme low and flag\n",
    "z_mask = wr_df['value_ratio_dk_log_z'].isna()\n",
    "wr_df.loc[z_mask, 'value_ratio_dk_log_z'] = -4.0\n",
    "wr_df['is_missing_value_ratio_dk_log_z'] = z_mask.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5d9ad-e61d-4871-b576-ab5cd4cd7533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "826f9212-0c98-4439-aa8f-9a47c656f314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "value_ratio_dk                     0\n",
       "value_ratio_fd                     0\n",
       "value_ratio_dk_log                 0\n",
       "value_ratio_fd_log                 0\n",
       "value_ratio_dk_log_z               0\n",
       "is_missing_value_ratio_dk_log_z    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spot-check value ratio columns\n",
    "wr_df[\n",
    "    [\n",
    "        'value_ratio_dk', 'value_ratio_fd',\n",
    "        'value_ratio_dk_log', 'value_ratio_fd_log',\n",
    "        'value_ratio_dk_log_z', 'is_missing_value_ratio_dk_log_z'\n",
    "    ]\n",
    "].sample(10)\n",
    "\n",
    "# And confirm missing values\n",
    "wr_df[\n",
    "    [\n",
    "        'value_ratio_dk', 'value_ratio_fd',\n",
    "        'value_ratio_dk_log', 'value_ratio_fd_log',\n",
    "        'value_ratio_dk_log_z', 'is_missing_value_ratio_dk_log_z'\n",
    "    ]\n",
    "].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425a7dab-d097-4cc4-920b-946661730c37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "fe94d611-4a08-44df-8a53-a6e9e481fd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.]\n",
      "float64\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check the actual value types and examples\n",
    "print(wr_df['value_ratio_dk'].unique()[:10])\n",
    "print(wr_df['value_ratio_dk'].dtype)\n",
    "\n",
    "# Count how many entries are actually zero, empty string, or 'nan'\n",
    "print((wr_df['value_ratio_dk'] == '').sum())  # empty string\n",
    "print((wr_df['value_ratio_dk'] == 'nan').sum())  # string 'nan'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e61ae12-84ec-441a-8c06-0892c35504dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bd9e491f-d033-452f-874d-7fbc8cad8bfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2833"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many rows had original missing values before fill?\n",
    "# Check how many were flagged\n",
    "wr_df['is_missing_value_ratio_dk_log_z'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a781e01-b8a4-460c-a2f6-043a0519d9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "f462509d-0c33-4239-b517-555793aee882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_ratio_dk\n",
      "0.0    17449\n",
      "Name: count, dtype: int64\n",
      "NaN count: 0\n",
      "Empty string count: 0\n",
      "Data type: float64\n"
     ]
    }
   ],
   "source": [
    "# Inspect actual value distribution and types\n",
    "print(wr_df['value_ratio_dk'].value_counts(dropna=False).head(10))\n",
    "\n",
    "# See how many are truly NaN (np.nan)\n",
    "print(\"NaN count:\", wr_df['value_ratio_dk'].isna().sum())\n",
    "\n",
    "# See how many are empty strings\n",
    "print(\"Empty string count:\", (wr_df['value_ratio_dk'] == '').sum())\n",
    "\n",
    "# See dtype\n",
    "print(\"Data type:\", wr_df['value_ratio_dk'].dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc062464-47a4-4901-9efb-439b9f0b5b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9b9a49c8-7b42-45a1-97a3-5a031b637d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "# Drop or overwrite old calculations to ensure clean slate\n",
    "cols_to_reset = [\n",
    "    'value_ratio_dk', 'value_ratio_dk_log', 'value_ratio_dk_log_z', 'is_missing_value_ratio_dk_log_z'\n",
    "]\n",
    "wr_df.drop(columns=[col for col in cols_to_reset if col in wr_df.columns], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571f541-2099-441a-b58e-c842db13e0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6dc4c814-af9c-479b-b584-4c5cb8053e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "# Recalculate safely only for valid rows\n",
    "mask_valid_dk = (wr_df['dk_salary'].notna()) & (wr_df['dk_salary'] != 0)\n",
    "wr_df.loc[mask_valid_dk, 'value_ratio_dk'] = wr_df.loc[mask_valid_dk, 'fpts'] / (wr_df.loc[mask_valid_dk, 'dk_salary'] / 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd0e213-ec04-4247-b40b-683a78538a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "fea6bcb0-5ac6-4824-9003-99ca84d7e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "# Use log1p and clip to avoid errors on invalid/missing values\n",
    "wr_df['value_ratio_dk_log'] = np.log1p(wr_df['value_ratio_dk'].clip(lower=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "089db1bb-a580-40f4-99f0-0bd2f1f8b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "# Use mean and std on the log-transformed version\n",
    "mean_log = wr_df['value_ratio_dk_log'].mean(skipna=True)\n",
    "std_log = wr_df['value_ratio_dk_log'].std(skipna=True)\n",
    "wr_df['value_ratio_dk_log_z'] = (wr_df['value_ratio_dk_log'] - mean_log) / std_log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20127655-1f9d-42cb-9df0-890b7626cfcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b5edb448-aa13-4b70-86f8-a718483eb885",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "z_mask = wr_df['value_ratio_dk_log_z'].isna()\n",
    "wr_df.loc[z_mask, 'value_ratio_dk_log_z'] = -4.0\n",
    "wr_df['is_missing_value_ratio_dk_log_z'] = z_mask.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383beb7a-3078-42e8-885d-114a4390ad9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b9e0fd05-e2d3-430a-b25f-76d1151a4c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step11_after_value_ratio_dk_recalculation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after recalculating value_ratio_dk and related features\n",
    "wr_df.to_csv(\"step11_after_value_ratio_dk_recalculation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step11_after_value_ratio_dk_recalculation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44af7d2-8f47-4cbc-85f9-c86b96df2599",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b0c4933f-7962-4e50-ac0e-72b3d7dce5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "cols_to_reset = [\n",
    "    'value_ratio_fd', 'value_ratio_fd_log', 'value_ratio_fd_log_z', 'is_missing_value_ratio_fd_log_z'\n",
    "]\n",
    "wr_df.drop(columns=[col for col in cols_to_reset if col in wr_df.columns], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecb5181-074a-4cea-8d57-b9e443709464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "ea8e920e-02eb-4979-938f-caddc5700549",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "mask_valid_fd = (wr_df['fd_salary'].notna()) & (wr_df['fd_salary'] != 0)\n",
    "wr_df.loc[mask_valid_fd, 'value_ratio_fd'] = wr_df.loc[mask_valid_fd, 'fpts'] / (wr_df.loc[mask_valid_fd, 'fd_salary'] / 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be91d58-ae32-428a-b47b-dae85c1b1fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0b18f27d-601c-434b-a5f8-854015760a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "wr_df['value_ratio_fd_log'] = np.log1p(wr_df['value_ratio_fd'].clip(lower=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d59ddb-6c26-4c69-86b8-cf6f5c63c20e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9a329b86-74be-410b-b592-686d20c8a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dataframe: correct columns ##\n",
    "\n",
    "mean_log_fd = wr_df['value_ratio_fd_log'].mean(skipna=True)\n",
    "std_log_fd = wr_df['value_ratio_fd_log'].std(skipna=True)\n",
    "wr_df['value_ratio_fd_log_z'] = (wr_df['value_ratio_fd_log'] - mean_log_fd) / std_log_fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97fb10d-a7a9-4938-9013-5a097341f131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4eedc9cb-0ee6-4886-b098-4aac11d790fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the missing mask and fallback imputation\n",
    "z_mask_fd = wr_df['value_ratio_fd_log_z'].isna()\n",
    "wr_df.loc[z_mask_fd, 'value_ratio_fd_log_z'] = -4.0\n",
    "wr_df['is_missing_value_ratio_fd_log_z'] = z_mask_fd.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ccf368eb-5fa2-4b1f-a59c-4ef4c26da12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe after recalculating value_ratio_fd and related features\n",
    "wr_df.to_csv(\"step12_fixed_value_ratio_fd_flags.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c7219-48e4-478e-9859-baab9e2d4ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ccf028a9-aca5-47ba-bbaf-604090e1fd37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value_ratio_dk_log_z</th>\n",
       "      <th>is_missing_value_ratio_dk_log_z</th>\n",
       "      <th>value_ratio_dk</th>\n",
       "      <th>value_ratio_fd</th>\n",
       "      <th>value_ratio_dk_log</th>\n",
       "      <th>value_ratio_fd_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9857</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13702</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12426</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13782</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2284</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9776</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6819</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10040</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12066</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7090</th>\n",
       "      <td>-4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       value_ratio_dk_log_z  is_missing_value_ratio_dk_log_z  value_ratio_dk  \\\n",
       "9857                   -4.0                                1             NaN   \n",
       "13702                  -4.0                                1             NaN   \n",
       "12426                  -4.0                                1             NaN   \n",
       "13782                  -4.0                                1             NaN   \n",
       "2284                   -4.0                                1             NaN   \n",
       "9776                   -4.0                                1             NaN   \n",
       "6819                   -4.0                                1             NaN   \n",
       "10040                  -4.0                                1             NaN   \n",
       "12066                  -4.0                                1             NaN   \n",
       "7090                   -4.0                                1             NaN   \n",
       "\n",
       "       value_ratio_fd  value_ratio_dk_log  value_ratio_fd_log  \n",
       "9857              NaN                 NaN                 NaN  \n",
       "13702             NaN                 NaN                 NaN  \n",
       "12426             NaN                 NaN                 NaN  \n",
       "13782             NaN                 NaN                 NaN  \n",
       "2284              NaN                 NaN                 NaN  \n",
       "9776              NaN                 NaN                 NaN  \n",
       "6819              NaN                 NaN                 NaN  \n",
       "10040             NaN                 NaN                 NaN  \n",
       "12066             NaN                 NaN                 NaN  \n",
       "7090              NaN                 NaN                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_value_ratio_z = wr_df[wr_df['is_missing_value_ratio_dk_log_z'] == 1][\n",
    "    ['value_ratio_dk_log_z', 'is_missing_value_ratio_dk_log_z',\n",
    "     'value_ratio_dk', 'value_ratio_fd', 'value_ratio_dk_log', 'value_ratio_fd_log']\n",
    "]\n",
    "\n",
    "if check_value_ratio_z.shape[0] > 0:\n",
    "    display(check_value_ratio_z.sample(10))\n",
    "else:\n",
    "    print(\"✅ No rows were imputed with -4.0 for value_ratio_dk_log_z — all values were originally valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec7611-6a75-4daa-b110-5bd1e826d50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "970245c5-c2f9-4793-befc-38f4b84050d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_rz                                   : 16330\n",
      "rec_rz                                   : 16330\n",
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_delta                           : 10664\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "fpts_7wk_avg                             : 10268\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "tgt_7wk_avg                              : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "rec_7wk_avg                              : 9961\n",
      "rec_air_yards_7wk_delta                  : 9961\n",
      "rec_yds_7wk_avg                          : 9961\n",
      "rec_air_yards_7wk_avg                    : 9961\n",
      "rec_yds_7wk_delta                        : 9961\n",
      "tgt_7wk_delta                            : 9961\n",
      "rec_7wk_avg_z                            : 9961\n",
      "rec_7wk_delta                            : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "yards                                    : 9277\n",
      "avg_cushion_performance_bin              : 9252\n",
      "catch_percentage_performance_bin         : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "percent_share_of_intended_air_yards_performance_bin : 9250\n",
      "avg_intended_air_yards_performance_bin   : 9250\n",
      "avg_separation_performance_bin           : 9250\n",
      "fpts_5wk_delta                           : 8256\n",
      "fpts_5wk_avg                             : 7843\n",
      "rec_5wk_avg                              : 7525\n",
      "tgt_5wk_avg                              : 7525\n",
      "rec_5wk_delta                            : 7525\n",
      "rec_yds_5wk_avg                          : 7525\n",
      "tgt_5wk_delta                            : 7525\n",
      "rec_air_yards_5wk_delta                  : 7525\n",
      "rec_air_yards_5wk_avg                    : 7525\n",
      "rec_yds_5wk_delta                        : 7525\n",
      "fpts_3wk_delta                           : 5568\n",
      "fpts_3wk_avg                             : 5141\n",
      "rec_3wk_delta                            : 4822\n",
      "tgt_3wk_delta                            : 4822\n",
      "rec_yds_3wk_avg                          : 4822\n",
      "rec_air_yards_3wk_delta                  : 4822\n",
      "rec_3wk_avg                              : 4822\n",
      "rec_air_yards_3wk_avg                    : 4822\n",
      "rec_yds_3wk_delta                        : 4822\n",
      "tgt_3wk_avg                              : 4822\n",
      "value_ratio_dk                           : 2833\n",
      "value_ratio_dk_log                       : 2833\n",
      "value_ratio_fd_log_performance_bin       : 2833\n",
      "value_ratio_fd                           : 2833\n",
      "value_ratio_dk_log_performance_bin       : 2833\n",
      "value_ratio_fd_log                       : 2833\n",
      "z_fpts_diff_fd                           : 2833\n",
      "fpts_diff_dk                             : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "z_value_ratio_fd                         : 2833\n",
      "z_value_ratio_dk                         : 2833\n",
      "z_fpts_diff_dk                           : 2833\n",
      "receiving_rat                            : 2774\n",
      "expected_fpts_dk                         : 2193\n",
      "fd_salary                                : 2193\n",
      "dk_salary                                : 2193\n",
      "expected_fpts_fd                         : 2193\n",
      "fpts_lag_1                               : 2075\n",
      "tgt_lag_1                                : 1748\n",
      "rec_lag_1                                : 1748\n",
      "rec_yds_lag_1                            : 1748\n",
      "rec_air_yards_lag_1                      : 1748\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "rost                                     : 780\n",
      "fpts_performance_bin                     : 780\n",
      "fpts                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share_performance_bin             : 323\n",
      "wopr                                     : 286\n",
      "target_share                             : 286\n",
      "air_yards_share                          : 286\n",
      "target_share_z                           : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 91\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "\n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354e510-49f4-4011-b43e-021a5d0ec453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3a52574-b316-45b7-9b8e-876f295af4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_ratio_dk           : 2833 missing\n",
      "value_ratio_fd           : 2833 missing\n",
      "value_ratio_dk_log       : 2833 missing\n",
      "value_ratio_fd_log       : 2833 missing\n",
      "value_ratio_dk_log_z     : 0 missing\n",
      "value_ratio_fd_log_z     : 0 missing\n"
     ]
    }
   ],
   "source": [
    "# columns already imputed with remaining a NaNs - will impute with 0\n",
    "value_ratio_cols = [\n",
    "    'value_ratio_dk', 'value_ratio_fd',\n",
    "    'value_ratio_dk_log', 'value_ratio_fd_log',\n",
    "    'value_ratio_dk_log_z', 'value_ratio_fd_log_z'\n",
    "]\n",
    "\n",
    "for col in value_ratio_cols:\n",
    "    print(f\"{col:<25}: {wr_df[col].isna().sum()} missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aea123-5f63-483a-a553-cde9be584969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "00cac66f-2877-4705-b68f-e12be6a91798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns already imputed with remaining a NaNs - will impute with 0\n",
    "\n",
    "# Fill missing base and log columns for modeling compatibility\n",
    "wr_df['value_ratio_dk'] = wr_df['value_ratio_dk'].fillna(0.0)\n",
    "wr_df['value_ratio_fd'] = wr_df['value_ratio_fd'].fillna(0.0)\n",
    "wr_df['value_ratio_dk_log'] = wr_df['value_ratio_dk_log'].fillna(0.0)\n",
    "wr_df['value_ratio_fd_log'] = wr_df['value_ratio_fd_log'].fillna(0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09605f98-5efb-4bae-856c-c7a16f608e29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c3797d9c-bfec-4a97-9a58-51246ba86bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "value_ratio_dk        0\n",
      "value_ratio_fd        0\n",
      "value_ratio_dk_log    0\n",
      "value_ratio_fd_log    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# columns already imputed with remaining a NaNs - will impute with 0\n",
    "\n",
    "# Confirm all cleared\n",
    "print(wr_df[['value_ratio_dk', 'value_ratio_fd', 'value_ratio_dk_log', 'value_ratio_fd_log']].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f56d8-c2ae-4c45-83c5-8a43fe11b46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "7ea1b781-aa45-48a0-bc61-f7822bab8038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avg_cushion                  0\n",
       "avg_separation               0\n",
       "avg_yac                      0\n",
       "avg_expected_yac             0\n",
       "avg_yac_above_expectation    0\n",
       "receiving_air_yards          0\n",
       "receiving_epa                0\n",
       "rec_touchdowns               0\n",
       "catch_percentage             0\n",
       "catch_percentage_scaled      0\n",
       "catch_percentage_scaled_z    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns already imputed with remaining a NaNs - will impute with 0\n",
    "\n",
    "# Columns to fill with 0.0 that were already imputed but still contain NaNs\n",
    "# Exclude value_ratio_dk and value_ratio_fd (and their variants)\n",
    "columns_to_fill = [\n",
    "    'avg_cushion', 'avg_separation',\n",
    "    'avg_yac', 'avg_expected_yac', 'avg_yac_above_expectation',\n",
    "    'receiving_air_yards', 'receiving_epa',\n",
    "    'rec_touchdowns',\n",
    "    'catch_percentage', 'catch_percentage_scaled', 'catch_percentage_scaled_z'\n",
    "]\n",
    "\n",
    "# Apply fillna(0.0) to those columns\n",
    "wr_df[columns_to_fill] = wr_df[columns_to_fill].fillna(0.0)\n",
    "\n",
    "# Confirm cleanup\n",
    "nan_summary_post_fill = wr_df[columns_to_fill].isna().sum()\n",
    "nan_summary_post_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b49493-fe5e-45b3-850c-856f2ac541b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "98dd2b47-e716-420c-a875-5e3cec60edea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "output_path = \"step13_after_1st_round_imputations_zero_fill.csv\"\n",
    "wr_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037430a-18e6-4d6f-a8b9-07b5c7cd2001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "36c0828f-4357-4889-aee4-794b1d132296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_rz                                   : 16330\n",
      "tgt_rz                                   : 16330\n",
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_delta                           : 10664\n",
      "fpts_7wk_avg                             : 10268\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "rec_air_yards_7wk_avg                    : 9961\n",
      "rec_yds_7wk_avg                          : 9961\n",
      "tgt_7wk_delta                            : 9961\n",
      "rec_7wk_avg                              : 9961\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "tgt_7wk_avg                              : 9961\n",
      "rec_7wk_delta                            : 9961\n",
      "rec_yds_7wk_delta                        : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "rec_air_yards_7wk_delta                  : 9961\n",
      "rec_7wk_avg_z                            : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "yards                                    : 9277\n",
      "avg_cushion_performance_bin              : 9252\n",
      "percent_share_of_intended_air_yards_performance_bin : 9250\n",
      "catch_percentage_performance_bin         : 9250\n",
      "avg_separation_performance_bin           : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "avg_intended_air_yards_performance_bin   : 9250\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "fpts_5wk_delta                           : 8256\n",
      "fpts_5wk_avg                             : 7843\n",
      "rec_air_yards_5wk_delta                  : 7525\n",
      "tgt_5wk_avg                              : 7525\n",
      "rec_5wk_avg                              : 7525\n",
      "rec_yds_5wk_delta                        : 7525\n",
      "tgt_5wk_delta                            : 7525\n",
      "rec_yds_5wk_avg                          : 7525\n",
      "rec_5wk_delta                            : 7525\n",
      "rec_air_yards_5wk_avg                    : 7525\n",
      "fpts_3wk_delta                           : 5568\n",
      "fpts_3wk_avg                             : 5141\n",
      "rec_3wk_delta                            : 4822\n",
      "rec_yds_3wk_delta                        : 4822\n",
      "rec_air_yards_3wk_avg                    : 4822\n",
      "rec_yds_3wk_avg                          : 4822\n",
      "rec_3wk_avg                              : 4822\n",
      "rec_air_yards_3wk_delta                  : 4822\n",
      "tgt_3wk_avg                              : 4822\n",
      "tgt_3wk_delta                            : 4822\n",
      "fpts_diff_dk                             : 2833\n",
      "z_fpts_diff_dk                           : 2833\n",
      "z_value_ratio_dk                         : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "value_ratio_fd_log_performance_bin       : 2833\n",
      "z_fpts_diff_fd                           : 2833\n",
      "z_value_ratio_fd                         : 2833\n",
      "value_ratio_dk_log_performance_bin       : 2833\n",
      "receiving_rat                            : 2774\n",
      "expected_fpts_dk                         : 2193\n",
      "dk_salary                                : 2193\n",
      "expected_fpts_fd                         : 2193\n",
      "fd_salary                                : 2193\n",
      "fpts_lag_1                               : 2075\n",
      "rec_air_yards_lag_1                      : 1748\n",
      "rec_yds_lag_1                            : 1748\n",
      "rec_lag_1                                : 1748\n",
      "tgt_lag_1                                : 1748\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts_performance_bin                     : 780\n",
      "rost                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share_performance_bin             : 323\n",
      "wopr                                     : 286\n",
      "target_share                             : 286\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_7wk                                 : 19\n",
      "fpts_5wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 87\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155e91d6-08a1-4140-9138-8b03f5f0edf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "d1cea856-b3b5-47d7-98cb-3160047e8ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Summary of remaining nulls in rolling average columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tgt_3wk_avg                0\n",
       "tgt_5wk_avg                0\n",
       "fpts_5wk_delta             0\n",
       "fpts_3wk_delta             0\n",
       "rec_air_yards_7wk_delta    0\n",
       "rec_air_yards_5wk_delta    0\n",
       "rec_air_yards_3wk_delta    0\n",
       "rec_yds_7wk_delta          0\n",
       "rec_yds_5wk_delta          0\n",
       "rec_yds_3wk_delta          0\n",
       "rec_7wk_delta              0\n",
       "rec_5wk_delta              0\n",
       "rec_3wk_delta              0\n",
       "tgt_7wk_delta              0\n",
       "tgt_5wk_delta              0\n",
       "tgt_3wk_delta              0\n",
       "fpts_7wk_avg               0\n",
       "fpts_5wk_avg               0\n",
       "fpts_3wk_avg               0\n",
       "rec_air_yards_7wk_avg      0\n",
       "rec_air_yards_5wk_avg      0\n",
       "rec_air_yards_3wk_avg      0\n",
       "rec_yds_7wk_avg            0\n",
       "rec_yds_5wk_avg            0\n",
       "rec_yds_3wk_avg            0\n",
       "rec_7wk_avg                0\n",
       "rec_5wk_avg                0\n",
       "rec_3wk_avg                0\n",
       "tgt_7wk_avg                0\n",
       "fpts_7wk_delta             0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👀 Sample rows where fallback expanding mean likely applied:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>tgt_3wk_avg</th>\n",
       "      <th>tgt_5wk_avg</th>\n",
       "      <th>tgt_7wk_avg</th>\n",
       "      <th>rec_3wk_avg</th>\n",
       "      <th>rec_5wk_avg</th>\n",
       "      <th>rec_7wk_avg</th>\n",
       "      <th>rec_yds_3wk_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>rec_7wk_delta</th>\n",
       "      <th>rec_yds_3wk_delta</th>\n",
       "      <th>rec_yds_5wk_delta</th>\n",
       "      <th>rec_yds_7wk_delta</th>\n",
       "      <th>rec_air_yards_3wk_delta</th>\n",
       "      <th>rec_air_yards_5wk_delta</th>\n",
       "      <th>rec_air_yards_7wk_delta</th>\n",
       "      <th>fpts_3wk_delta</th>\n",
       "      <th>fpts_5wk_delta</th>\n",
       "      <th>fpts_7wk_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15225</th>\n",
       "      <td>Zay Flowers</td>\n",
       "      <td>2023</td>\n",
       "      <td>20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>6.600000</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>61.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-20.700000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>-14.100000</td>\n",
       "      <td>-3.300000</td>\n",
       "      <td>-16.400000</td>\n",
       "      <td>-16.900000</td>\n",
       "      <td>2.856250</td>\n",
       "      <td>2.856250</td>\n",
       "      <td>2.856250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13401</th>\n",
       "      <td>Darnell Mooney</td>\n",
       "      <td>2023</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7249</th>\n",
       "      <td>Jamison Crowder</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>89.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-63.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-71.300000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-2.300000</td>\n",
       "      <td>5.720000</td>\n",
       "      <td>5.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8675</th>\n",
       "      <td>Chase Claypool</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>1.766667</td>\n",
       "      <td>1.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11262</th>\n",
       "      <td>DeAndre Hopkins</td>\n",
       "      <td>2022</td>\n",
       "      <td>8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>10.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2575</th>\n",
       "      <td>D.J. Moore</td>\n",
       "      <td>2018</td>\n",
       "      <td>13</td>\n",
       "      <td>7.3</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.700000</td>\n",
       "      <td>6.300000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>89.300000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-45.300000</td>\n",
       "      <td>-30.800000</td>\n",
       "      <td>-22.000000</td>\n",
       "      <td>81.700000</td>\n",
       "      <td>80.400000</td>\n",
       "      <td>84.400000</td>\n",
       "      <td>-4.800000</td>\n",
       "      <td>-3.800000</td>\n",
       "      <td>-1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14847</th>\n",
       "      <td>Ronnie Bell</td>\n",
       "      <td>2023</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>5.300000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-2.700000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-2.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>John Brown</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>8.7</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>7.600000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.300000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>67.700000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>56.400000</td>\n",
       "      <td>55.100000</td>\n",
       "      <td>18.800000</td>\n",
       "      <td>17.200000</td>\n",
       "      <td>17.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1147</th>\n",
       "      <td>Kenny Britt</td>\n",
       "      <td>2017</td>\n",
       "      <td>13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>31.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-21.700000</td>\n",
       "      <td>-20.800000</td>\n",
       "      <td>-20.000000</td>\n",
       "      <td>-7.300000</td>\n",
       "      <td>-26.000000</td>\n",
       "      <td>-35.000000</td>\n",
       "      <td>-4.200000</td>\n",
       "      <td>-3.300000</td>\n",
       "      <td>-3.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9619</th>\n",
       "      <td>K.J. Osborn</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name  season  week  tgt_3wk_avg  tgt_5wk_avg  tgt_7wk_avg  \\\n",
       "15225      Zay Flowers    2023    20          6.0     7.200000     6.600000   \n",
       "13401   Darnell Mooney    2023     4          1.0     1.000000     1.000000   \n",
       "7249   Jamison Crowder    2020     9         11.0     1.000000     1.000000   \n",
       "8675    Chase Claypool    2021     3          7.0     4.333333     4.333333   \n",
       "11262  DeAndre Hopkins    2022     8         14.0     7.500000     7.500000   \n",
       "2575        D.J. Moore    2018    13          7.3     6.000000     5.700000   \n",
       "14847      Ronnie Bell    2023    16          1.0     1.200000     1.000000   \n",
       "5123        John Brown    2019    11          8.7     7.400000     7.600000   \n",
       "1147       Kenny Britt    2017    13          2.3     4.000000     4.600000   \n",
       "9619       K.J. Osborn    2021     1          1.0     1.000000     1.000000   \n",
       "\n",
       "       rec_3wk_avg  rec_5wk_avg  rec_7wk_avg  rec_yds_3wk_avg  ...  \\\n",
       "15225     4.300000     4.800000     4.600000        61.700000  ...   \n",
       "13401     0.333333     0.333333     0.333333         0.333333  ...   \n",
       "7249      7.300000     0.600000     0.600000        89.300000  ...   \n",
       "8675      0.000000     0.000000     0.000000         0.000000  ...   \n",
       "11262     0.500000     0.500000     0.500000         0.500000  ...   \n",
       "2575      6.300000     5.000000     4.600000        89.300000  ...   \n",
       "14847     0.300000     0.800000     0.333333         6.700000  ...   \n",
       "5123      4.700000     4.800000     4.700000        69.000000  ...   \n",
       "1147      2.000000     2.200000     2.100000        31.700000  ...   \n",
       "9619      0.000000     0.000000     0.000000         0.000000  ...   \n",
       "\n",
       "       rec_7wk_delta  rec_yds_3wk_delta  rec_yds_5wk_delta  rec_yds_7wk_delta  \\\n",
       "15225      -0.600000         -20.700000         -13.000000         -14.100000   \n",
       "13401       0.333333           0.333333           0.333333           0.333333   \n",
       "7249        0.600000         -63.300000           0.600000           0.600000   \n",
       "8675        0.000000           0.000000           0.000000           0.000000   \n",
       "11262       0.500000           0.500000           0.500000           0.500000   \n",
       "2575       -0.600000         -45.300000         -30.800000         -22.000000   \n",
       "14847       0.333333           5.300000           1.800000           0.333333   \n",
       "5123        4.300000          68.000000          64.000000          67.700000   \n",
       "1147       -0.100000         -21.700000         -20.800000         -20.000000   \n",
       "9619        0.000000           0.000000           0.000000           0.000000   \n",
       "\n",
       "       rec_air_yards_3wk_delta  rec_air_yards_5wk_delta  \\\n",
       "15225                -3.300000               -16.400000   \n",
       "13401                 0.333333                 0.333333   \n",
       "7249                -71.300000                 0.600000   \n",
       "8675                  0.000000                 0.000000   \n",
       "11262                 0.500000                 0.500000   \n",
       "2575                 81.700000                80.400000   \n",
       "14847                -2.700000                -1.000000   \n",
       "5123                 37.000000                56.400000   \n",
       "1147                 -7.300000               -26.000000   \n",
       "9619                  0.000000                 0.000000   \n",
       "\n",
       "       rec_air_yards_7wk_delta  fpts_3wk_delta  fpts_5wk_delta  fpts_7wk_delta  \n",
       "15225               -16.900000        2.856250        2.856250        2.856250  \n",
       "13401                 0.333333        0.133333        0.133333        0.133333  \n",
       "7249                  0.600000       -2.300000        5.720000        5.720000  \n",
       "8675                  0.000000        1.766667        1.766667        1.766667  \n",
       "11262                 0.500000       10.750000       10.750000       10.750000  \n",
       "2575                 84.400000       -4.800000       -3.800000       -1.800000  \n",
       "14847                 0.333333        6.500000        5.000000       -2.316667  \n",
       "5123                 55.100000       18.800000       17.200000       17.900000  \n",
       "1147                -35.000000       -4.200000       -3.300000       -3.700000  \n",
       "9619                  0.000000        0.800000        0.800000        0.800000  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# feature engineering - rolling averages\n",
    "\n",
    "# === 1. Setup refined base mapping ===\n",
    "refined_base_mapping = {\n",
    "    'receptions': 'rec_',\n",
    "    'targets': 'tgt_',\n",
    "    'receiving_yards': 'rec_yds_',\n",
    "    'fpts': 'fpts_',\n",
    "    'receiving_air_yards': 'rec_air_yards_'\n",
    "}\n",
    "\n",
    "# === 2. Define stricter regex to match ONLY valid rolling columns ===\n",
    "rolling_avg_pattern = re.compile(r'^.*_\\d+wk_(avg|delta|z)$')\n",
    "\n",
    "# === 3. Exclude known non-numeric or categorical suffixes ===\n",
    "non_numeric_suffixes = ('_bin', '_tier', '_clipped', '_flag')\n",
    "\n",
    "# === 4. Identify safe rolling average columns ===\n",
    "rolling_avg_cols = [\n",
    "    col for col in wr_df.columns\n",
    "    if rolling_avg_pattern.search(col)\n",
    "    and not any(col.endswith(suffix) for suffix in non_numeric_suffixes)\n",
    "]\n",
    "\n",
    "# === 5. Match rolling columns to base stats ===\n",
    "rolling_to_base_pairs = []\n",
    "for base_stat, base_prefix in refined_base_mapping.items():\n",
    "    for col in rolling_avg_cols:\n",
    "        if col.startswith(base_prefix):\n",
    "            rolling_to_base_pairs.append((col, base_stat))\n",
    "\n",
    "# === 6. Apply group-wise expanding mean imputation ===\n",
    "wr_df.sort_values(by=['name', 'season', 'week'], inplace=True)\n",
    "\n",
    "for rolling_col, base_stat in rolling_to_base_pairs:\n",
    "    base_cols_matching = [\n",
    "        col for col in wr_df.columns\n",
    "        if col.startswith(refined_base_mapping[base_stat])\n",
    "        and np.issubdtype(wr_df[col].dtype, np.number)\n",
    "    ]\n",
    "\n",
    "    for base_col in base_cols_matching:\n",
    "        if base_col in wr_df.columns and rolling_col in wr_df.columns:\n",
    "            progressive_avg = (\n",
    "                wr_df.groupby(['name', 'season'])[base_col]\n",
    "                .transform(lambda x: x.expanding().mean())\n",
    "            )\n",
    "            wr_df[rolling_col] = wr_df[rolling_col].fillna(progressive_avg)\n",
    "\n",
    "# === 7. Optional: Visual summary ===\n",
    "print(\"\\n📊 Summary of remaining nulls in rolling average columns:\")\n",
    "missing_summary_rolling = wr_df[rolling_avg_cols].isna().sum().sort_values(ascending=False)\n",
    "display(missing_summary_rolling)\n",
    "\n",
    "print(\"\\n👀 Sample rows where fallback expanding mean likely applied:\")\n",
    "rolling_imputed_rows = wr_df[wr_df[rolling_avg_cols].isna().sum(axis=1) == 0]\n",
    "display(rolling_imputed_rows[['name', 'season', 'week'] + rolling_avg_cols].sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3515b517-06bb-4d7d-99d2-064c0db5e2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "55c473bf-1f9d-4871-9741-47fd3dc49138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Null values in rolling average columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tgt_3wk_avg                0\n",
       "tgt_5wk_avg                0\n",
       "fpts_5wk_delta             0\n",
       "fpts_3wk_delta             0\n",
       "rec_air_yards_7wk_delta    0\n",
       "rec_air_yards_5wk_delta    0\n",
       "rec_air_yards_3wk_delta    0\n",
       "rec_yds_7wk_delta          0\n",
       "rec_yds_5wk_delta          0\n",
       "rec_yds_3wk_delta          0\n",
       "rec_7wk_delta              0\n",
       "rec_5wk_delta              0\n",
       "rec_3wk_delta              0\n",
       "tgt_7wk_delta              0\n",
       "tgt_5wk_delta              0\n",
       "tgt_3wk_delta              0\n",
       "fpts_7wk_avg               0\n",
       "fpts_5wk_avg               0\n",
       "fpts_3wk_avg               0\n",
       "rec_air_yards_7wk_avg      0\n",
       "rec_air_yards_5wk_avg      0\n",
       "rec_air_yards_3wk_avg      0\n",
       "rec_yds_7wk_avg            0\n",
       "rec_yds_5wk_avg            0\n",
       "rec_yds_3wk_avg            0\n",
       "rec_7wk_avg                0\n",
       "rec_5wk_avg                0\n",
       "rec_3wk_avg                0\n",
       "tgt_7wk_avg                0\n",
       "fpts_7wk_delta             0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Sample rows where at least one rolling column was imputed with 0.0:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>season</th>\n",
       "      <th>week</th>\n",
       "      <th>tgt_3wk_avg</th>\n",
       "      <th>tgt_5wk_avg</th>\n",
       "      <th>tgt_7wk_avg</th>\n",
       "      <th>rec_3wk_avg</th>\n",
       "      <th>rec_5wk_avg</th>\n",
       "      <th>rec_7wk_avg</th>\n",
       "      <th>rec_yds_3wk_avg</th>\n",
       "      <th>...</th>\n",
       "      <th>rec_7wk_delta</th>\n",
       "      <th>rec_yds_3wk_delta</th>\n",
       "      <th>rec_yds_5wk_delta</th>\n",
       "      <th>rec_yds_7wk_delta</th>\n",
       "      <th>rec_air_yards_3wk_delta</th>\n",
       "      <th>rec_air_yards_5wk_delta</th>\n",
       "      <th>rec_air_yards_7wk_delta</th>\n",
       "      <th>fpts_3wk_delta</th>\n",
       "      <th>fpts_5wk_delta</th>\n",
       "      <th>fpts_7wk_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6284</th>\n",
       "      <td>Andy Isabella</td>\n",
       "      <td>2020</td>\n",
       "      <td>9</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>12.3</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-7.4</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-26.7</td>\n",
       "      <td>-18.4</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>-2.128571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15573</th>\n",
       "      <td>Collin Johnson</td>\n",
       "      <td>2024</td>\n",
       "      <td>16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.900000</td>\n",
       "      <td>-5.900000</td>\n",
       "      <td>-5.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12265</th>\n",
       "      <td>Nico Collins</td>\n",
       "      <td>2022</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.566667</td>\n",
       "      <td>-1.566667</td>\n",
       "      <td>-1.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12669</th>\n",
       "      <td>Trent Sherfield</td>\n",
       "      <td>2022</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21.7</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.7</td>\n",
       "      <td>-10.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-15.7</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.600000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-3.883333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16317</th>\n",
       "      <td>John Metchie</td>\n",
       "      <td>2024</td>\n",
       "      <td>18</td>\n",
       "      <td>4.7</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.700000</td>\n",
       "      <td>-10.3</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>-6.900000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>DeVante Parker</td>\n",
       "      <td>2018</td>\n",
       "      <td>9</td>\n",
       "      <td>4.3</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>2.325000</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>58.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>-1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>Richie James</td>\n",
       "      <td>2018</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.300000</td>\n",
       "      <td>-5.300000</td>\n",
       "      <td>-5.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>DaeSean Hamilton</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-4.625000</td>\n",
       "      <td>-4.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>Josh Reynolds</td>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.300000</td>\n",
       "      <td>-4.300000</td>\n",
       "      <td>-4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16074</th>\n",
       "      <td>Jake Bobo</td>\n",
       "      <td>2024</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>-2.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name  season  week  tgt_3wk_avg  tgt_5wk_avg  tgt_7wk_avg  \\\n",
       "6284      Andy Isabella    2020     9          2.7     3.000000     2.000000   \n",
       "15573    Collin Johnson    2024    16          1.0     0.500000     0.500000   \n",
       "12265      Nico Collins    2022     3          6.0     3.000000     3.000000   \n",
       "12669   Trent Sherfield    2022     6          3.0     2.600000     1.950000   \n",
       "16317      John Metchie    2024    18          4.7     4.000000     4.000000   \n",
       "2704     DeVante Parker    2018     9          4.3     2.325000     2.325000   \n",
       "3622       Richie James    2018     5          0.0     0.000000     0.000000   \n",
       "2583   DaeSean Hamilton    2018    12          2.0     1.750000     1.750000   \n",
       "977       Josh Reynolds    2017    11          1.0     0.666667     0.666667   \n",
       "16074         Jake Bobo    2024     4          2.0     1.000000     1.000000   \n",
       "\n",
       "       rec_3wk_avg  rec_5wk_avg  rec_7wk_avg  rec_yds_3wk_avg  ...  \\\n",
       "6284           1.0          1.8     0.285714             12.3  ...   \n",
       "15573          0.0          0.0     0.000000              0.0  ...   \n",
       "12265          0.0          0.0     0.000000              0.0  ...   \n",
       "12669          2.0          1.8     0.000000             21.7  ...   \n",
       "16317          3.0          2.6     2.600000             27.0  ...   \n",
       "2704           2.7          0.0     0.000000             58.0  ...   \n",
       "3622           0.0          0.0     0.000000              0.0  ...   \n",
       "2583           1.3          0.0     0.000000             16.0  ...   \n",
       "977            0.0          0.0     0.000000              0.0  ...   \n",
       "16074          0.0          0.0     0.000000              0.0  ...   \n",
       "\n",
       "       rec_7wk_delta  rec_yds_3wk_delta  rec_yds_5wk_delta  rec_yds_7wk_delta  \\\n",
       "6284        0.285714               -2.3               -7.4           0.285714   \n",
       "15573       0.000000                0.0                0.0           0.000000   \n",
       "12265       0.000000                0.0                0.0           0.000000   \n",
       "12669       0.000000              -15.7              -10.4           0.000000   \n",
       "16317      -0.600000               -3.0                0.0          -3.700000   \n",
       "2704        0.000000              -50.0                0.0           0.000000   \n",
       "3622        0.000000                0.0                0.0           0.000000   \n",
       "2583        0.000000               -3.0                0.0           0.000000   \n",
       "977         0.000000                0.0                0.0           0.000000   \n",
       "16074       0.000000                0.0                0.0           0.000000   \n",
       "\n",
       "       rec_air_yards_3wk_delta  rec_air_yards_5wk_delta  \\\n",
       "6284                     -26.7                    -18.4   \n",
       "15573                      0.0                      0.0   \n",
       "12265                      0.0                      0.0   \n",
       "12669                    -15.7                     -4.0   \n",
       "16317                    -10.3                     -2.6   \n",
       "2704                     -75.0                      0.0   \n",
       "3622                       0.0                      0.0   \n",
       "2583                       6.7                      0.0   \n",
       "977                        0.0                      0.0   \n",
       "16074                      0.0                      0.0   \n",
       "\n",
       "       rec_air_yards_7wk_delta  fpts_3wk_delta  fpts_5wk_delta  fpts_7wk_delta  \n",
       "6284                  0.285714       -0.200000       -3.000000       -2.128571  \n",
       "15573                 0.000000       -5.900000       -5.900000       -5.900000  \n",
       "12265                 0.000000       -1.566667       -1.566667       -1.566667  \n",
       "12669                 0.000000       -1.600000       -1.000000       -3.883333  \n",
       "16317                -6.900000       -0.300000        0.000000       -1.200000  \n",
       "2704                  0.000000       -5.000000       -1.500000       -1.500000  \n",
       "3622                  0.000000       -5.300000       -5.300000       -5.300000  \n",
       "2583                  0.000000       -0.300000       -4.625000       -4.625000  \n",
       "977                   0.000000       -4.300000       -4.300000       -4.300000  \n",
       "16074                 0.000000       -2.900000       -2.900000       -2.900000  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for any remaining missing values in rolling columns\n",
    "rolling_cols = [col for col in wr_df.columns if rolling_avg_pattern.search(col) or col.endswith(('_3wk_avg', '_5wk_avg', '_7wk_avg'))]\n",
    "missing_summary_rolling = wr_df[rolling_cols].isna().sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\n📊 Null values in rolling average columns:\")\n",
    "display(missing_summary_rolling)\n",
    "\n",
    "# Show a few rows with 0.0 values (likely imputed fallback)\n",
    "rolling_imputed_rows = wr_df[wr_df[rolling_cols].eq(0.0).any(axis=1)]\n",
    "print(\"\\n🔍 Sample rows where at least one rolling column was imputed with 0.0:\")\n",
    "display(rolling_imputed_rows[['name', 'season', 'week'] + rolling_cols].sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef39b6c-c425-4576-b756-b72ee3061a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "38ad2483-c15a-4e02-b173-90fd7dccadc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step14_after_rolling_imputations.csv\n"
     ]
    }
   ],
   "source": [
    "# Export the dataframe after recalculating value_ratio_dk and related features\n",
    "wr_df.to_csv(\"step14_after_rolling_imputations.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step14_after_rolling_imputations.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc8d96-26b7-40e2-8163-98b9c69f213e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "777ddcdd-7b16-42c6-8eb7-6e3790664fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_rz                                   : 16330\n",
      "rec_rz                                   : 16330\n",
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "rec_7wk_avg_z                            : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "yards                                    : 9277\n",
      "avg_cushion_performance_bin              : 9252\n",
      "catch_percentage_performance_bin         : 9250\n",
      "avg_separation_performance_bin           : 9250\n",
      "avg_intended_air_yards_performance_bin   : 9250\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "percent_share_of_intended_air_yards_performance_bin : 9250\n",
      "value_ratio_dk_log_performance_bin       : 2833\n",
      "z_value_ratio_fd                         : 2833\n",
      "z_fpts_diff_fd                           : 2833\n",
      "value_ratio_fd_log_performance_bin       : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "z_value_ratio_dk                         : 2833\n",
      "z_fpts_diff_dk                           : 2833\n",
      "fpts_diff_dk                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "expected_fpts_fd                         : 2193\n",
      "dk_salary                                : 2193\n",
      "expected_fpts_dk                         : 2193\n",
      "fd_salary                                : 2193\n",
      "fpts_lag_1                               : 2075\n",
      "tgt_lag_1                                : 1748\n",
      "rec_air_yards_lag_1                      : 1748\n",
      "rec_yds_lag_1                            : 1748\n",
      "rec_lag_1                                : 1748\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts_performance_bin                     : 780\n",
      "rost                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share_performance_bin             : 323\n",
      "wopr                                     : 286\n",
      "target_share                             : 286\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_7wk                                 : 19\n",
      "fpts_5wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 57\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727794b1-7802-49b8-957b-ce1049497ee6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "436f756a-90f4-47ca-99aa-273a509d3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_rz    0\n",
      "rec_rz    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# feature engineering - redzone imputations\n",
    "\n",
    "# Red Zone Feature Imputation\n",
    "red_zone_cols = ['tgt_rz', 'rec_rz']\n",
    "\n",
    "# Fill missing values with 0.0\n",
    "wr_df[red_zone_cols] = wr_df[red_zone_cols].fillna(0.0)\n",
    "\n",
    "# Optional: add flags to trace what was imputed\n",
    "for col in red_zone_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# Verify cleanup\n",
    "print(wr_df[red_zone_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ff4f61-4f8c-4988-b6a3-bbf5e8f856b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "96b5d19c-beef-4447-9b02-e5c775596970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_3wk_avg              0\n",
      "rec_5wk_avg              0\n",
      "rec_7wk_avg              0\n",
      "tgt_3wk_avg              0\n",
      "tgt_5wk_avg              0\n",
      "tgt_7wk_avg              0\n",
      "fpts_3wk_avg             0\n",
      "fpts_5wk_avg             0\n",
      "fpts_7wk_avg             0\n",
      "rec_air_yards_3wk_avg    0\n",
      "rec_air_yards_5wk_avg    0\n",
      "rec_air_yards_7wk_avg    0\n",
      "rec_yds_3wk_avg          0\n",
      "rec_yds_5wk_avg          0\n",
      "rec_yds_7wk_avg          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# feature engineering - rolling averages \n",
    "\n",
    "# Define all multi-week average columns to impute\n",
    "multiweek_avg_cols = [\n",
    "    'rec_3wk_avg', 'rec_5wk_avg', 'rec_7wk_avg',\n",
    "    'tgt_3wk_avg', 'tgt_5wk_avg', 'tgt_7wk_avg',\n",
    "    'fpts_3wk_avg', 'fpts_5wk_avg', 'fpts_7wk_avg',\n",
    "    'rec_air_yards_3wk_avg', 'rec_air_yards_5wk_avg', 'rec_air_yards_7wk_avg',\n",
    "    'rec_yds_3wk_avg', 'rec_yds_5wk_avg', 'rec_yds_7wk_avg'\n",
    "]\n",
    "\n",
    "# Fill NaNs with 0.0\n",
    "wr_df[multiweek_avg_cols] = wr_df[multiweek_avg_cols].fillna(0.0)\n",
    "\n",
    "# Add missingness flags\n",
    "for col in multiweek_avg_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# Verify cleanup\n",
    "print(wr_df[multiweek_avg_cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64049cd4-0624-4ba5-9065-176f7ced93ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c12bb472-7b52-4d81-a0f8-a0fc09c0f6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step15_after_multiweek_avg_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# ✅ Export CSV after Step 15\n",
    "wr_df.to_csv(\"step15_after_multiweek_avg_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step15_after_multiweek_avg_imputation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d82bcb-3f16-4651-be3e-d800004c1863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "603e5d59-3e44-44f0-b4d7-6421992d58d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "rec_7wk_avg_z                            : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "yards                                    : 9277\n",
      "avg_cushion_performance_bin              : 9252\n",
      "avg_intended_air_yards                   : 9250\n",
      "percent_share_of_intended_air_yards_performance_bin : 9250\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "avg_separation_performance_bin           : 9250\n",
      "avg_intended_air_yards_performance_bin   : 9250\n",
      "catch_percentage_performance_bin         : 9250\n",
      "z_value_ratio_fd                         : 2833\n",
      "value_ratio_dk_log_performance_bin       : 2833\n",
      "z_fpts_diff_fd                           : 2833\n",
      "value_ratio_fd_log_performance_bin       : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "z_value_ratio_dk                         : 2833\n",
      "z_fpts_diff_dk                           : 2833\n",
      "fpts_diff_dk                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "expected_fpts_fd                         : 2193\n",
      "dk_salary                                : 2193\n",
      "expected_fpts_dk                         : 2193\n",
      "fd_salary                                : 2193\n",
      "fpts_lag_1                               : 2075\n",
      "rec_lag_1                                : 1748\n",
      "rec_air_yards_lag_1                      : 1748\n",
      "rec_yds_lag_1                            : 1748\n",
      "tgt_lag_1                                : 1748\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts_performance_bin                     : 780\n",
      "rost                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share_performance_bin             : 323\n",
      "wopr                                     : 286\n",
      "target_share                             : 286\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_7wk                                 : 19\n",
      "fpts_5wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 55\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de7483-d714-4f6f-bbc3-d60a4fea0ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "16d06923-0244-4f0f-a185-1ddef974e080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after performance bin imputation:\n",
      "avg_cushion_performance_bin                            0\n",
      "catch_percentage_performance_bin                       0\n",
      "avg_separation_performance_bin                         0\n",
      "avg_intended_air_yards_performance_bin                 0\n",
      "percent_share_of_intended_air_yards_performance_bin    0\n",
      "value_ratio_dk_log_performance_bin                     0\n",
      "value_ratio_fd_log_performance_bin                     0\n",
      "fpts_performance_bin                                   0\n",
      "target_share_performance_bin                           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
      "/tmp/ipykernel_1130712/582805702.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Performance Bin Imputations\n",
    "\n",
    "# Define performance bin columns\n",
    "performance_bin_cols = [\n",
    "    'avg_cushion_performance_bin',\n",
    "    'catch_percentage_performance_bin',\n",
    "    'avg_separation_performance_bin',\n",
    "    'avg_intended_air_yards_performance_bin',\n",
    "    'percent_share_of_intended_air_yards_performance_bin',\n",
    "    'value_ratio_dk_log_performance_bin',\n",
    "    'value_ratio_fd_log_performance_bin',\n",
    "    'fpts_performance_bin',\n",
    "    'target_share_performance_bin',\n",
    "]\n",
    "\n",
    "# Impute missing values with -1 (sentinel)\n",
    "wr_df[performance_bin_cols] = wr_df[performance_bin_cols].fillna(-1)\n",
    "\n",
    "# Add imputation flags\n",
    "for col in performance_bin_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].eq(-1).astype(int)\n",
    "\n",
    "# Confirm no missing values remain in those columns\n",
    "print(\"\\n✅ Missing values after performance bin imputation:\")\n",
    "print(wr_df[performance_bin_cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1951b8da-1844-46bb-af16-e78bd336ab42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "0db410c7-ed81-416c-9d42-e8d68c146225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📁 CSV export complete: step16_after_performance_bin_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export updated dataframe\n",
    "wr_df.to_csv(\"step16_after_performance_bin_imputation.csv\", index=False)\n",
    "print(\"\\n📁 CSV export complete: step16_after_performance_bin_imputation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0713e4-efc8-41c1-88f5-d47ffde35cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "01271133-8b88-4386-89a5-e3b53eaedc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "rec_7wk_avg_z                            : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "yards                                    : 9277\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "z_value_ratio_fd                         : 2833\n",
      "z_value_ratio_dk                         : 2833\n",
      "fpts_diff_dk                             : 2833\n",
      "z_fpts_diff_dk                           : 2833\n",
      "z_fpts_diff_fd                           : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "expected_fpts_dk                         : 2193\n",
      "expected_fpts_fd                         : 2193\n",
      "dk_salary                                : 2193\n",
      "fd_salary                                : 2193\n",
      "fpts_lag_1                               : 2075\n",
      "tgt_lag_1                                : 1748\n",
      "rec_lag_1                                : 1748\n",
      "rec_yds_lag_1                            : 1748\n",
      "rec_air_yards_lag_1                      : 1748\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts                                     : 780\n",
      "rost                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "wopr                                     : 286\n",
      "air_yards_share                          : 286\n",
      "target_share_z                           : 286\n",
      "target_share                             : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 46\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255d4601-18ef-4106-95d9-248fa99513ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "ad3e3035-22c4-406f-b13b-2b08fb40136a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after z-score imputation:\n",
      "z_value_ratio_fd    0\n",
      "z_value_ratio_dk    0\n",
      "z_fpts_diff_fd      0\n",
      "z_fpts_diff_dk      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/3229050487.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-4.0).astype(int)\n",
      "/tmp/ipykernel_1130712/3229050487.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-4.0).astype(int)\n",
      "/tmp/ipykernel_1130712/3229050487.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-4.0).astype(int)\n",
      "/tmp/ipykernel_1130712/3229050487.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].eq(-4.0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Z-Score Imputations\n",
    "\n",
    "# Define all z-score columns for imputation\n",
    "z_score_cols = [\n",
    "    'z_value_ratio_fd', 'z_value_ratio_dk',\n",
    "    'z_fpts_diff_fd', 'z_fpts_diff_dk'\n",
    "]\n",
    "\n",
    "# Impute missing z-score values with -4.0\n",
    "wr_df[z_score_cols] = wr_df[z_score_cols].fillna(-4.0)\n",
    "\n",
    "# Add imputation flags for each z-score column\n",
    "for col in z_score_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].eq(-4.0).astype(int)\n",
    "\n",
    "# ✅ Confirm no remaining missing values\n",
    "print(\"\\n✅ Missing values after z-score imputation:\")\n",
    "print(wr_df[z_score_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9f695-1fed-4d80-b7db-4f0cd8eb094c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "41910f1a-4111-4425-ab82-c972f2688ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step17_after_z_score_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export after z-score imputation\n",
    "wr_df.to_csv(\"step17_after_z_score_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step17_after_z_score_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbec4dc9-8e02-4179-be32-46efcaf86954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "e3c00121-46ee-4905-82d3-1ed7e6b08228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "rec_7wk_avg_z                            : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "yards                                    : 9277\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "fpts_diff_dk                             : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "fd_salary                                : 2193\n",
      "expected_fpts_fd                         : 2193\n",
      "expected_fpts_dk                         : 2193\n",
      "dk_salary                                : 2193\n",
      "fpts_lag_1                               : 2075\n",
      "rec_yds_lag_1                            : 1748\n",
      "rec_air_yards_lag_1                      : 1748\n",
      "tgt_lag_1                                : 1748\n",
      "rec_lag_1                                : 1748\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "rost                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "air_yards_share                          : 286\n",
      "target_share_z                           : 286\n",
      "target_share                             : 286\n",
      "wopr                                     : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 42\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f02793c-fa81-43a6-ad4c-5c6e707c21ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "ba4251a7-58d3-460d-98a1-7bafd23fd1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after lag feature imputation:\n",
      "fpts_lag_1             0\n",
      "tgt_lag_1              0\n",
      "rec_lag_1              0\n",
      "rec_yds_lag_1          0\n",
      "rec_air_yards_lag_1    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/2056373407.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2056373407.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2056373407.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2056373407.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2056373407.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Lag Feature Imputations\n",
    "\n",
    "# Define lag columns to impute\n",
    "lag_cols = [\n",
    "    'fpts_lag_1', 'tgt_lag_1', 'rec_lag_1',\n",
    "    'rec_yds_lag_1', 'rec_air_yards_lag_1'\n",
    "]\n",
    "\n",
    "# Impute missing values with 0.0\n",
    "wr_df[lag_cols] = wr_df[lag_cols].fillna(0.0)\n",
    "\n",
    "# Add flags to trace what was imputed\n",
    "for col in lag_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# ✅ Confirm cleanup\n",
    "print(\"\\n✅ Missing values after lag feature imputation:\")\n",
    "print(wr_df[lag_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46618fb-45c3-406c-9542-d6dbbac6eb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "ba10a939-fd5f-4055-b31e-bd0b5e40a503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step18_after_lag_feature_imputation.csv\n"
     ]
    }
   ],
   "source": [
    "# Export after imputing lag features\n",
    "wr_df.to_csv(\"step18_after_lag_feature_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step18_after_lag_feature_imputation.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5fc3fb-61f6-4a03-bf52-76c5d9f7028c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "f2934613-9b02-474b-90d2-506b01c7c33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rec_3wk                                  : 14678\n",
      "rec_5wk                                  : 13571\n",
      "rec_7wk                                  : 12894\n",
      "fpts_7wk_avg_z                           : 10268\n",
      "rec_air_yards_7wk_avg_clipped            : 9961\n",
      "rec_7wk_avg_z                            : 9961\n",
      "tgt_7wk_avg_z                            : 9961\n",
      "rec_yds_7wk_avg_z                        : 9961\n",
      "rec_air_yards_7wk_avg_clipped_z          : 9961\n",
      "yards                                    : 9277\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "fpts_diff_fd                             : 2833\n",
      "fpts_diff_dk                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "expected_fpts_fd                         : 2193\n",
      "expected_fpts_dk                         : 2193\n",
      "dk_salary                                : 2193\n",
      "fd_salary                                : 2193\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts                                     : 780\n",
      "rost                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "wopr                                     : 286\n",
      "air_yards_share                          : 286\n",
      "target_share_z                           : 286\n",
      "target_share                             : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 37\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57214a0a-5714-4b7f-aa68-64b4555a680c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "92d5e146-0621-4d7e-b015-38d1531e8755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after feature imputation:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3969777923.py:17: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - rec, fpts, avg Feature Imputations\n",
    "\n",
    "# Define next group of columns for zero imputation\n",
    "next_zero_impute_cols = [\n",
    "    'rec_3wk', 'rec_5wk', 'rec_7wk',\n",
    "    'fpts_7wk_avg_z',\n",
    "    'rec_air_yards_7wk_avg_clipped', 'rec_air_yards_7wk_avg_clipped_z',\n",
    "    'rec_7wk_avg_z', 'tgt_7wk_avg_z', 'rec_yds_7wk_avg_z'\n",
    "]\n",
    "\n",
    "# Impute with 0.0\n",
    "wr_df[next_zero_impute_cols] = wr_df[next_zero_impute_cols].fillna(0.0)\n",
    "\n",
    "# Add is_missing flags\n",
    "for col in next_zero_impute_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "\n",
    "# ✅ Confirm cleanup\n",
    "print(\"\\n✅ Missing values after feature imputation:\")\n",
    "print(wr_df[flag_col].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fddc5f-e4fd-4fe3-a210-715981b01413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "094f5b72-fddb-4052-ab68-f7c38c6f54ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step19_after_7wk_and_air_yard_clip_zero_imputation\n"
     ]
    }
   ],
   "source": [
    "# Export after z-score imputation\n",
    "wr_df.to_csv(\"step19_after_7wk_and_air_yard_clip_zero_imputation.csv\")\n",
    "print(\"✅ CSV export complete: step19_after_7wk_and_air_yard_clip_zero_imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db616cb5-b041-476e-9cc0-9fb70cd7585d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "70785e3c-c9ba-47b9-aff5-ee87af5736de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yards                                    : 9277\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "avg_intended_air_yards                   : 9250\n",
      "fpts_diff_dk                             : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "fd_salary                                : 2193\n",
      "expected_fpts_dk                         : 2193\n",
      "expected_fpts_fd                         : 2193\n",
      "dk_salary                                : 2193\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "rost                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "fpts                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "wopr                                     : 286\n",
      "target_share                             : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 28\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000aaf2d-7e13-4ad3-8e63-6f08946c4d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c82c1e19-d7c0-45c1-aa8c-a3218dca206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after salary/expected fpts imputation:\n",
      "dk_salary           0\n",
      "fd_salary           0\n",
      "expected_fpts_dk    0\n",
      "expected_fpts_fd    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/3758791546.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3758791546.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3758791546.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/3758791546.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - dk and fd salary and expected fpts\n",
    "\n",
    "# Define columns to impute\n",
    "salary_and_expected_cols = [\n",
    "    'dk_salary', 'fd_salary',\n",
    "    'expected_fpts_dk', 'expected_fpts_fd'\n",
    "]\n",
    "\n",
    "# Fill with 0.0\n",
    "wr_df[salary_and_expected_cols] = wr_df[salary_and_expected_cols].fillna(0.0)\n",
    "\n",
    "# Add flags for traceability\n",
    "for col in salary_and_expected_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# Confirm cleanup\n",
    "print(\"\\n✅ Missing values after salary/expected fpts imputation:\")\n",
    "print(wr_df[salary_and_expected_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd1e7a-3db4-4310-b3a7-76f5751062fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "67c68af7-d7ef-42f4-aa04-1137f6039cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step20_after_salary_expected_fpts_zero_imputation\n"
     ]
    }
   ],
   "source": [
    "# Export after salary and expected fpts imputation\n",
    "wr_df.to_csv(\"step20_after_salary_expected_fpts_zero_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step20_after_salary_expected_fpts_zero_imputation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff4062-fc9c-4d10-927a-6ce246e71866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "6cb6b43d-f5fe-4f38-b5ec-5825436bdd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yards                                    : 9277\n",
      "avg_intended_air_yards                   : 9250\n",
      "percent_share_of_intended_air_yards      : 9250\n",
      "fpts_diff_dk                             : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "rost                                     : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "target_share                             : 286\n",
      "wopr                                     : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 24\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018b13f1-a6b1-4790-9f85-96f9b53837c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d439ba5c-643c-46a6-b199-36ac20047f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after air yard metrics imputation:\n",
      "avg_intended_air_yards                 0\n",
      "percent_share_of_intended_air_yards    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/1574854742.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/1574854742.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Air Yard Metrics Imputation\n",
    "\n",
    "# Define columns to impute\n",
    "air_yard_cols = [\n",
    "    'avg_intended_air_yards',\n",
    "    'percent_share_of_intended_air_yards'\n",
    "]\n",
    "\n",
    "# Impute missing with 0.0\n",
    "wr_df[air_yard_cols] = wr_df[air_yard_cols].fillna(0.0)\n",
    "\n",
    "# Add is_missing flags\n",
    "for col in air_yard_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# Confirm cleanup\n",
    "print(\"\\n✅ Missing values after air yard metrics imputation:\")\n",
    "print(wr_df[air_yard_cols].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed576e0-a34d-4706-9ed8-e183317e0582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "59df0824-907a-4154-bfd2-38e7dc5888cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step21_after_air_yard_metrics_zero_imputation\n"
     ]
    }
   ],
   "source": [
    "# Export CSV\n",
    "wr_df.to_csv(\"step21_after_air_yard_metrics_zero_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step21_after_air_yard_metrics_zero_imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b669f9d-1afb-465c-b83a-ba89e6263552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "42c596f2-b74f-44e6-a21b-5c854eea9ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yards                                    : 9277\n",
      "fpts_diff_dk                             : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "value_ratio_dk_3wk                       : 1633\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "value_ratio_dk_5wk                       : 1496\n",
      "value_ratio_dk_7wk                       : 1484\n",
      "fpts                                     : 780\n",
      "rost                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share                             : 286\n",
      "air_yards_share                          : 286\n",
      "wopr                                     : 286\n",
      "target_share_z                           : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 22\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610e5df-2a81-45eb-84b5-f4f343f25087",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f36fe1f0-c527-4f50-971a-faff4818835b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after value_ratio_dk_nwk imputation:\n",
      "value_ratio_dk_3wk    0\n",
      "value_ratio_dk_5wk    0\n",
      "value_ratio_dk_7wk    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/1600902865.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/1600902865.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/1600902865.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Value Ratio DK N-week Imputation\n",
    "\n",
    "# Define columns\n",
    "value_ratio_dk_nwk_cols = [\n",
    "    'value_ratio_dk_3wk', \n",
    "    'value_ratio_dk_5wk', \n",
    "    'value_ratio_dk_7wk'\n",
    "]\n",
    "\n",
    "# Impute with 0.0\n",
    "wr_df[value_ratio_dk_nwk_cols] = wr_df[value_ratio_dk_nwk_cols].fillna(0.0)\n",
    "\n",
    "# Add trace flags\n",
    "for col in value_ratio_dk_nwk_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# ✅ Confirm cleanup\n",
    "print(\"\\n✅ Missing values after value_ratio_dk_nwk imputation:\")\n",
    "print(wr_df[value_ratio_dk_nwk_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0548db-d4ca-4d4f-8b03-bfa15e8679b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "aae81a61-dd0b-4a9b-9c9d-1e2acd383f58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step22_after_value_ratio_dk_nwk_zero_imputation\n"
     ]
    }
   ],
   "source": [
    "# 📁 Export after value_ratio_dk_nwk imputation\n",
    "wr_df.to_csv(\"step22_after_value_ratio_dk_nwk_zero_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step22_after_value_ratio_dk_nwk_zero_imputation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c794e113-95a6-44dc-8398-b191025cc003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "e7f776b9-c4cb-46bf-a034-e13409f8aa43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yards                                    : 9277\n",
      "fpts_diff_dk                             : 2833\n",
      "fpts_diff_fd                             : 2833\n",
      "receiving_rat                            : 2774\n",
      "rolling_fpts_diff_fd                     : 1543\n",
      "rolling_fpts_diff_dk                     : 1543\n",
      "fpts                                     : 780\n",
      "rost                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share                             : 286\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "wopr                                     : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 19\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95881510-0336-4bcf-8d98-346f44315d26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4e527f4b-94e0-47df-9782-624501d50208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Missing values after diff/rolling/rat imputation:\n",
      "fpts_diff_dk            0\n",
      "fpts_diff_fd            0\n",
      "rolling_fpts_diff_dk    0\n",
      "rolling_fpts_diff_fd    0\n",
      "receiving_rat           0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1130712/2131273331.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2131273331.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2131273331.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2131273331.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
      "/tmp/ipykernel_1130712/2131273331.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  wr_df[flag_col] = wr_df[col].isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# Differential & Rolling Performance Metrics\n",
    "\n",
    "next_group_cols = [\n",
    "    'fpts_diff_dk', 'fpts_diff_fd',\n",
    "    'rolling_fpts_diff_dk', 'rolling_fpts_diff_fd',\n",
    "    'receiving_rat'  # same logic: 0 implies no receptions or rate unrecorded\n",
    "]\n",
    "\n",
    "# Impute with 0.0\n",
    "wr_df[next_group_cols] = wr_df[next_group_cols].fillna(0.0)\n",
    "\n",
    "# Add is_missing flags\n",
    "for col in next_group_cols:\n",
    "    flag_col = f'is_missing_{col}'\n",
    "    wr_df[flag_col] = wr_df[col].isna().astype(int)\n",
    "\n",
    "# Confirm cleanup\n",
    "print(\"\\n✅ Missing values after diff/rolling/rat imputation:\")\n",
    "print(wr_df[next_group_cols].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13c0ad6-6d1d-4afe-acb5-d5602860c28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "03627844-20b9-458a-9495-9bfd1d17b2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV export complete: step23_after_diff_rolling_rat_zero_imputation\n"
     ]
    }
   ],
   "source": [
    "# ✅ Export after diff/rolling/receiving_rate imputation\n",
    "wr_df.to_csv(\"step23_after_diff_rolling_rat_zero_imputation.csv\", index=False)\n",
    "print(\"✅ CSV export complete: step23_after_diff_rolling_rat_zero_imputation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f020087-65a6-44fc-bef4-584d6addaf02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "15afcd13-2444-4ab6-a3f1-5975e44d0e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yards                                    : 9277\n",
      "rost                                     : 780\n",
      "fpts                                     : 780\n",
      "fpts_above_pos_avg                       : 780\n",
      "pos_avg_fpts                             : 763\n",
      "racr                                     : 328\n",
      "target_share                             : 286\n",
      "target_share_z                           : 286\n",
      "air_yards_share                          : 286\n",
      "wopr                                     : 286\n",
      "fpts_3wk                                 : 119\n",
      "spread                                   : 39\n",
      "fpts_5wk                                 : 19\n",
      "fpts_7wk                                 : 19\n",
      "\n",
      "🟡 Total columns with missing values: 14\n"
     ]
    }
   ],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859acb2-2cd8-4126-95e4-c6bf5d77c1f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1467b8ca-b8fe-4178-997e-693d8071c72f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c71ff0-f95b-460e-acb2-62836f44626e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9749c-aca4-43d0-8680-1e13855ced44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ba75bb-2cd7-4e1a-bb7c-f34345d821fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c15c7e8-89d5-4210-bf9f-879b10fdf5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549aa9b5-fc2e-4da5-a4c0-635bcddc48cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92538626-9018-44d3-90ce-2c488e31cdd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e547994-e739-45fc-b23c-b11282ae1276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880eee9-4ef8-4c4a-a9c5-ba4037238a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ce648-7787-4b03-863a-ca8a2456d13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760d283-bf87-43c8-a7f2-dad1fdac9842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe Spot Check #\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = wr_df.isna().sum()\n",
    "missing_summary = missing_summary[missing_summary > 0].sort_values(ascending=False)\n",
    "\n",
    "for col, count in missing_summary.items():\n",
    "    print(f\"{col:<40} : {count}\")\n",
    "    \n",
    "# Total number of columns with missing values\n",
    "print(f\"\\n🟡 Total columns with missing values: {len(missing_summary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e086b-ef93-486c-833f-ef7c96ca0548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00eef8c-a7d3-4071-97fd-2bc314cc818c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356d41e4-a06e-49f7-8689-6c3252dac110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa655c3-fedb-415f-884d-28e27dbd7b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb6d6c0-b357-48ff-8d75-a44f8b9171c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf2fcc-9cea-47ec-bb07-78986576815d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac7d71-1188-4971-aa38-262ef7710d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc9a60-b21e-4643-9a1c-1ba68f614520",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187d087-1e95-4079-be42-72291d7cdad1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b356100-586f-4cdb-8046-384cbc918e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7600a1-df06-45ec-85eb-c3072ca47bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End: feature engineering - imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3133cb-e759-4bf3-9690-703de510e5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28924c0-3bdb-4cff-b026-12b7bdf9c6b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913e098-aea8-49eb-9b2a-883cb5ef4514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inspect the hit_value_dk target column ---\n",
    "print(\"🔍 Value Counts:\")\n",
    "print(df[\"hit_value_dk\"].value_counts(dropna=False))\n",
    "\n",
    "print(\"\\n🔍 Data Type:\", df[\"hit_value_dk\"].dtype)\n",
    "print(\"❓ Missing Values:\", df[\"hit_value_dk\"].isnull().sum())\n",
    "\n",
    "proportion = df[\"hit_value_dk\"].mean()\n",
    "print(f\"\\n✅ Proportion of hits (1's): {proportion:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0dca75-455f-4d0b-a9d0-4b418975c998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94447a-42a0-4b47-b9a3-4f354df1fec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: Determine Targets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f51aa64-5c3b-4fa9-9fc3-0eb3c4ca382e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check potential targets\n",
    "targets_to_check = [\n",
    "    \"hit_value_dk\",\n",
    "    \"hit_value_fd\",\n",
    "    \"rec_ge_7\",\n",
    "    \"fpts_performance_bin\",\n",
    "    \"value_ratio_dk_log_performance_bin\"\n",
    "]\n",
    "\n",
    "for col in targets_to_check:\n",
    "    print(f\"\\n📊 {col}\")\n",
    "    print(\"Value Counts:\\n\", df[col].value_counts(dropna=False))\n",
    "    print(\"Data Type:\", df[col].dtype)\n",
    "    print(\"Missing Values:\", df[col].isnull().sum())\n",
    "\n",
    "    if df[col].dropna().nunique() == 2 and df[col].dtype != 'object':\n",
    "        prop = df[col].mean()\n",
    "        print(f\"Proportion of 1's: {prop:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86743123-1486-4562-baa6-662931959812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb02743-2406-47e8-8dda-40c33b1aa6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulates value return tiers using a 3-tier system based on scaled value ratios.\n",
    "\n",
    "# Parameters:\n",
    "# - df: DataFrame containing 'fpts' and salary columns\n",
    "# - lower: Lower threshold (float), e.g., 0.5\n",
    "# - upper: Upper threshold (float), e.g., 2.0\n",
    "# - platform: 'dk' or 'fd' to determine salary column\n",
    "\n",
    "# Returns:\n",
    "# - Simulated tier Series with labels: underperformed, good_return, elite_return\n",
    "\n",
    "def simulate_value_tiers(df, lower, upper, platform=\"dk\"):\n",
    "  \n",
    "    if platform not in [\"dk\", \"fd\"]:\n",
    "        raise ValueError(\"Platform must be 'dk' or 'fd'\")\n",
    "\n",
    "    salary_col = \"dk_salary\" if platform == \"dk\" else \"fd_salary\"\n",
    "    value_ratio_scaled = df[\"fpts\"] / (df[salary_col] / 1000)\n",
    "\n",
    "    # Apply 3-tier binning\n",
    "    bins = [-np.inf, lower, upper, np.inf]\n",
    "    labels = [\"underperformed\", \"good_return\", \"elite_return\"]\n",
    "\n",
    "    simulated_tiers = pd.cut(\n",
    "        value_ratio_scaled,\n",
    "        bins=bins,\n",
    "        labels=labels,\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    # Print output\n",
    "    print(f\"\\n📊 Tier Distribution ({platform.upper()}):\")\n",
    "    print(simulated_tiers.value_counts(dropna=False))\n",
    "\n",
    "    print(f\"\\n📈 Tier Proportions ({platform.upper()}):\")\n",
    "    print(simulated_tiers.value_counts(normalize=True, dropna=False).apply(lambda x: f\"{x:.2%}\"))\n",
    "\n",
    "    return simulated_tiers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af45311-aa88-4e25-b35e-eacf5826246b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1578a17-2800-40e0-8219-79714fde2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold boundaries\n",
    "lower = 0.5\n",
    "upper = 2.0\n",
    "\n",
    "# Run DK and FD simulations using shared thresholds\n",
    "simulate_value_tiers(df, lower=lower, upper=upper, platform=\"dk\")\n",
    "simulate_value_tiers(df, lower=lower, upper=upper, platform=\"fd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3214ec-8273-4207-98aa-776153b549bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad06e26-a105-4c49-8ade-b9bd1d4c9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define threshold boundaries\n",
    "lower = 1.0\n",
    "upper = 2.0\n",
    "\n",
    "# Run DK and FD simulations using shared thresholds\n",
    "simulate_value_tiers(df, lower=lower, upper=upper, platform=\"dk\")\n",
    "simulate_value_tiers(df, lower=lower, upper=upper, platform=\"fd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a797c8-6147-4553-87ad-41dd009c187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: Determine Targets ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190caf46-f61e-4dd7-89af-c0cdde13ecce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce089ce-e905-4f79-a17a-5d98f563766b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: experimental logistic regression modeling  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191465a9-daae-47a0-9691-932f3368310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create  Binary Target\n",
    "dk_tiers = simulate_value_tiers(df, lower=1.0, upper=2.0, platform=\"dk\")\n",
    "is_elite_return_dk = dk_tiers == \"elite_return\"\n",
    "\n",
    "\n",
    "# Select Feature Columns\n",
    "features = [\n",
    "    \"targets\",\n",
    "    \"receptions\",\n",
    "    \"rolling_fpts_diff_dk\",\n",
    "    \"value_ratio_dk_log\",\n",
    "    \"z_fpts_diff_dk\",\n",
    "    \"tgt_ge_7\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308c3ab6-635e-4194-90c3-5103c2189049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fb8254-d617-418f-9da6-01d90fbad314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset features from df into a temporary X variable\n",
    "X = df[features].copy()\n",
    "y = is_elite_return_dk.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f321c7-7a7f-42e6-adfb-5cc268f42357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05e768-5b78-488e-8d15-f0c8bbe5cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the train-test split\n",
    "# train on seasons < 2023\n",
    "# validation on season 2023\n",
    "# final test on 2024\n",
    "\n",
    "\n",
    "# Mask for seasons\n",
    "mask_2024 = df[\"season\"] == 2024\n",
    "mask_2023 = df[\"season\"] == 2023\n",
    "mask_pre_2023 = df[\"season\"] < 2023\n",
    "\n",
    "# Full null check mask\n",
    "mask_all_valid = X.notnull().all(axis=1) & y.notnull()\n",
    "\n",
    "# Training set: Pre-2023\n",
    "mask_train = mask_all_valid & mask_pre_2023\n",
    "X_train = X[mask_train]\n",
    "y_train = y[mask_train]\n",
    "\n",
    "# Validation set: 2023\n",
    "mask_val = mask_all_valid & mask_2023\n",
    "X_val = X[mask_val]\n",
    "y_val = y[mask_val]\n",
    "\n",
    "# Final test set: 2024\n",
    "mask_test = mask_all_valid & mask_2024\n",
    "X_test_final = X[mask_test]\n",
    "y_test_final = y[mask_test]\n",
    "\n",
    "# Summary\n",
    "print(f\"Training samples:         {len(X_train)}\")\n",
    "print(f\"Validation (2023):        {len(X_val)}\")\n",
    "print(f\"Final Test (2024):        {len(X_test_final)}\")\n",
    "\n",
    "print(\"\\nClass Distribution:\")\n",
    "print(\"Train:\\n\", y_train.value_counts(normalize=True).apply(lambda x: f\"{x:.2%}\"))\n",
    "print(\"Val:\\n\", y_val.value_counts(normalize=True).apply(lambda x: f\"{x:.2%}\"))\n",
    "print(\"Test:\\n\", y_test_final.value_counts(normalize=True).apply(lambda x: f\"{x:.2%}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e065b-85de-49b9-ac34-e7846d9c2548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ff0115-ad6e-4f24-bc49-6c48f4471f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and Initialize the Model\n",
    "\n",
    "# Create a logistic regression model object\n",
    "# Use balanced class weights to account for slight imbalance\n",
    "logreg_model = LogisticRegression(\n",
    "    penalty=None,               # No regularization for now (keep math pure)\n",
    "    solver='lbfgs',             # Robust optimizer\n",
    "    class_weight='balanced',    # Compensate for 21/79 class ratio\n",
    "    max_iter=1000,              # Extra room for convergence\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da29bc7-4151-4260-80c9-e8fb7c476d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abd5131-6473-4169-987a-8d522a632723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on training data\n",
    "logreg_model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd50d3b-c7fd-46a1-a3c7-5ba693cdb03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3838cc26-60dd-4698-a1ac-8c8dc4786714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View and Interpret the Model Coefficients\n",
    "# View learned coefficients with their corresponding feature names\n",
    "coefficients = pd.Series(\n",
    "    logreg_model.coef_[0],\n",
    "    index=X_train.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "print(\"🔎 Logistic Regression Coefficients:\")\n",
    "print(coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6cd94-6df7-4dd3-b501-225f1cad278f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6790f495-5a5c-43c1-b288-2084774f98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Among Features\n",
    "# Spot collinearity and detect proxy behavior\n",
    "\n",
    "# Only use the features from the model\n",
    "feature_cols = [\n",
    "    \"value_ratio_dk_log\",\n",
    "    \"z_fpts_diff_dk\",\n",
    "    \"receptions\",\n",
    "    \"targets\",\n",
    "    \"rolling_fpts_diff_dk\",\n",
    "    \"tgt_ge_7\"\n",
    "]\n",
    "\n",
    "# Compute correlation matrix (drop NaNs just in case)\n",
    "corr_matrix = df[feature_cols].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Logistic Regression Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865c8b69-7c72-4872-a13f-e9adb06a3ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5327bf59-620b-4e83-a4a3-064a0d0eb2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply regression to determine the following:\n",
    "# How much z_fpts_diff_dk explains the variation in value_ratio_dk_log\n",
    "\n",
    "# Drop rows with missing values in either variable\n",
    "mask = df[\"value_ratio_dk_log\"].notnull() & df[\"z_fpts_diff_dk\"].notnull()\n",
    "x = df.loc[mask, \"z_fpts_diff_dk\"]\n",
    "y = df.loc[mask, \"value_ratio_dk_log\"]\n",
    "\n",
    "# Add constant for intercept\n",
    "x_with_const = sm.add_constant(x)\n",
    "\n",
    "# Fit linear regression\n",
    "model = sm.OLS(y, x_with_const).fit()\n",
    "\n",
    "# View results\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b360a6-dd67-4996-8f61-1a42a18064b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd07d27-2a28-4ef6-be28-7bf2ab620671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaNs for both features\n",
    "mask = df[\"receptions\"].notnull() & df[\"targets\"].notnull()\n",
    "x = df.loc[mask, \"targets\"]\n",
    "y = df.loc[mask, \"receptions\"]\n",
    "\n",
    "# Add constant for intercept\n",
    "x_with_const = sm.add_constant(x)\n",
    "\n",
    "# Fit linear regression model\n",
    "model_targets_to_rec = sm.OLS(y, x_with_const).fit()\n",
    "\n",
    "# Display summary\n",
    "print(model_targets_to_rec.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e899d6d8-359e-4278-bb30-8d303c06be67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521e213-b623-42b7-8d59-c02fbc16d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised feature list\n",
    "features_revised = [\n",
    "    \"value_ratio_dk_log\",\n",
    "    \"receptions\",\n",
    "    \"rolling_fpts_diff_dk\",\n",
    "    \"tgt_ge_7\"\n",
    "]\n",
    "\n",
    "# Compute correlation matrix (drop NaNs)\n",
    "corr_matrix_revised = df[features_revised].dropna().corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(corr_matrix_revised, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix (Revised Features)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5869f3-88f9-4f92-bab3-57b408c84867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6beb203-73e3-44cc-8d2c-7b2fd86dd8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore more features\n",
    "candidate_features = [\n",
    "    \"catch_percentage\",\n",
    "    \"catch_percentage_scaled\",\n",
    "    \"catch_percentage_scaled_z\",\n",
    "    \"fpts_3wk_avg\",\n",
    "    \"avg_cushion\",\n",
    "    \"avg_separation\",\n",
    "    \"avg_intended_air_yards\",\n",
    "    \"percent_share_of_intended_air_yards\"\n",
    "]\n",
    "\n",
    "# Count non-null values\n",
    "df[candidate_features].notnull().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f572ef78-f7c2-4fe8-8907-4aa7def78cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d2da5-38db-43ab-9790-4e1cd4520c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised features list\n",
    "features_final = [\n",
    "    \"value_ratio_dk_log\",\n",
    "    \"receptions\",\n",
    "    \"rolling_fpts_diff_dk\",\n",
    "    \"tgt_ge_7\",\n",
    "    \"fpts_3wk_avg\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe88ec-fcdc-4721-a4a8-d7fb25595e4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0cf2e3-dc53-473e-a77a-0345032ce8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation check on final feature set\n",
    "df[features_final].dropna().corr()\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(df[features_final].dropna().corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix – Final 5 Features\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5700fd49-51c9-4431-a0d4-8bc850fa609d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f7a0d-7a48-4548-9fe9-45d96d92b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train / Validation / Test Sets\n",
    "# Create season masks\n",
    "mask_2024 = df[\"season\"] == 2024\n",
    "mask_2023 = df[\"season\"] == 2023\n",
    "mask_pre_2023 = df[\"season\"] < 2023\n",
    "\n",
    "# Create modeling mask (no missing values)\n",
    "valid_mask = df[features_final].notnull().all(axis=1) & y.notnull()\n",
    "\n",
    "# Create X and y matrices\n",
    "X = df[features_final]\n",
    "\n",
    "X_train = X[valid_mask & mask_pre_2023]\n",
    "y_train = y[valid_mask & mask_pre_2023]\n",
    "\n",
    "X_val = X[valid_mask & mask_2023]\n",
    "y_val = y[valid_mask & mask_2023]\n",
    "\n",
    "X_test = X[valid_mask & mask_2024]\n",
    "y_test = y[valid_mask & mask_2024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644905f-4879-4c68-b72c-e73fbecaf816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f484f70d-25aa-481e-85d0-de967cbeebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rerun the logistic mode based on modifications to the features\n",
    "\n",
    "# Re-initialize the logistic model (no changes needed here)\n",
    "logreg_model_final = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit model on finalized training data\n",
    "logreg_model_final.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fb6074-8e18-4988-ba58-2abab2271b9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfc05fa-3dce-4209-b17b-9acd5508f056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pulls from the correct model object and feature set\n",
    "pd.Series(\n",
    "    logreg_model_final.coef_[0],\n",
    "    index=features_final\n",
    ").sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cff15d-fd9c-4d76-a823-806c69371c60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c57a7c-8cc0-4f02-8c1c-bd9f6d0b7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised model and features\n",
    "\n",
    "# # --- Define Final Feature Sets ---\n",
    "features_simplified = [\n",
    "    \"receptions\",\n",
    "    \"rolling_fpts_diff_dk\",\n",
    "    \"fpts_3wk_avg\"\n",
    "]\n",
    "\n",
    "# # --- Define Final Feature Sets ---\n",
    "# features_simplified = [\n",
    "#     \"targets\",\n",
    "#     \"rolling_fpts_diff_dk\",\n",
    "#     \"fpts_3wk_avg\"\n",
    "# ]\n",
    "\n",
    "features_with_tgt = features_simplified + [\"tgt_ge_7\"]\n",
    "\n",
    "# add targets back into the training set\n",
    "X_train[\"targets\"] = df.loc[X_train.index, \"targets\"]\n",
    "\n",
    "# --- Prepare Clean Subsets (Drop Rows with Missing Values) ---\n",
    "X_train_simple = X_train[features_simplified].dropna()\n",
    "y_train_simple = y_train.loc[X_train_simple.index]\n",
    "\n",
    "X_train_with_tgt = X_train[features_with_tgt].dropna()\n",
    "y_train_with_tgt = y_train.loc[X_train_with_tgt.index]\n",
    "\n",
    "# --- Initialize and Fit Logistic Models ---\n",
    "logreg_simple = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "logreg_with_tgt = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "logreg_simple.fit(X_train_simple, y_train_simple)\n",
    "logreg_with_tgt.fit(X_train_with_tgt, y_train_with_tgt)\n",
    "\n",
    "# --- View Coefficients ---\n",
    "print(\"🔹 Simplified Model Coefficients:\")\n",
    "print(pd.Series(logreg_simple.coef_[0], index=features_simplified))\n",
    "\n",
    "print(\"\\n🔹 With tgt_ge_7 Included:\")\n",
    "print(pd.Series(logreg_with_tgt.coef_[0], index=features_with_tgt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed27961-a3f1-40eb-a6b2-30278dd84e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35e3292-392f-48f8-9006-a0ed2ccf63af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define minimum thresholds ---\n",
    "min_targets = 15\n",
    "min_receptions = 15\n",
    "\n",
    "# --- Filter main dataframe before model prep ---\n",
    "df_filtered = df[\n",
    "    (df['targets'] >= min_targets) &\n",
    "    (df['receptions'] >= min_receptions)\n",
    "]\n",
    "\n",
    "# --- Update train splits using filtered indices ---\n",
    "X_train_filtered = X_train.loc[df_filtered.index.intersection(X_train.index)]\n",
    "y_train_filtered = y_train.loc[X_train_filtered.index]\n",
    "\n",
    "# --- Drop missing values for selected features ---\n",
    "# features_final = [\"targets\", \"rolling_fpts_diff_dk\", \"fpts_3wk_avg\"]\n",
    "features_receptions = [\"receptions\", \"rolling_fpts_diff_dk\", \"fpts_3wk_avg\"]\n",
    "\n",
    "# --- Drop rows with NaNs in selected features ---\n",
    "X_train_filtered = X_train_filtered[features_final].dropna()\n",
    "y_train_filtered = y_train_filtered.loc[X_train_filtered.index]\n",
    "\n",
    "# --- Refit model ---\n",
    "logreg_filtered = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "logreg_filtered.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# --- View updated coefficients ---\n",
    "print(\"🔍 Coefficients with Fringe Players Excluded:\")\n",
    "print(pd.Series(logreg_filtered.coef_[0], index=features_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b2510-cbbd-4948-a556-33e50cca5e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8836c320-2956-4947-8fc8-f02c551ccbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Filter full dataframe ---\n",
    "df_filtered = df[\n",
    "    (df['targets'] >= 15) &\n",
    "    (df['receptions'] >= 15)\n",
    "].copy()\n",
    "\n",
    "# --- Select only relevant features ---\n",
    "features_receptions = [\"receptions\", \"rolling_fpts_diff_dk\", \"fpts_3wk_avg\"]\n",
    "\n",
    "# --- Rebuild training input/output from scratch ---\n",
    "X_train_filtered = df_filtered.loc[\n",
    "    df_filtered.index.intersection(X_train.index),\n",
    "    features_receptions\n",
    "].dropna()\n",
    "\n",
    "y_train_filtered = y_train.loc[X_train_filtered.index]\n",
    "\n",
    "# --- Fit logistic regression ---\n",
    "logreg_receptions = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "logreg_receptions.fit(X_train_filtered, y_train_filtered)\n",
    "\n",
    "# --- Output coefficients ---\n",
    "print(\"🔍 Final Coefficients using 'receptions' only:\")\n",
    "print(pd.Series(logreg_receptions.coef_[0], index=features_receptions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755a5f8c-d00c-4030-9ef3-b69654df1f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57a535-0767-4aeb-a7a0-acdbefca1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Initial Logistical Regression Model ***\n",
    "# - Not Suitable because it contains Data Leakage *** ###\n",
    "# Model needs adjusting - x- inputs should be from previous rows\n",
    "\n",
    "# --- Configuration ---\n",
    "receptions_threshold = 1\n",
    "targets_threshold = 1\n",
    "features_final = [\"receptions\", \"rolling_fpts_diff_dk\", \"fpts_3wk_avg\"]\n",
    "\n",
    "# --- Filter dataset for meaningful usage (per week) ---\n",
    "df_filtered = df[\n",
    "    (df[\"receptions\"] >= receptions_threshold) &\n",
    "    (df[\"targets\"] >= targets_threshold)\n",
    "].copy()\n",
    "\n",
    "# --- TRAIN / VALIDATION SPLIT ---\n",
    "mask_train = df_filtered[\"season\"] < 2023\n",
    "mask_val = df_filtered[\"season\"] == 2023\n",
    "\n",
    "X_train = df_filtered.loc[mask_train, features_final].dropna()\n",
    "y_train = df_filtered.loc[X_train.index, \"hit_value_dk\"].astype(int)\n",
    "\n",
    "X_val = df_filtered.loc[mask_val, features_final].dropna()\n",
    "y_val = df_filtered.loc[X_val.index, \"hit_value_dk\"].astype(int)\n",
    "\n",
    "# --- Fit logistic regression model ---\n",
    "logreg_model = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver='lbfgs',\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=42\n",
    ")\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Evaluate on validation set ---\n",
    "y_val_pred = logreg_model.predict(X_val)\n",
    "y_val_proba = logreg_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --- Print results ---\n",
    "print(\"✅ Classification Report (2023 Validation Set):\")\n",
    "print(classification_report(y_val, y_val_pred, zero_division=0))\n",
    "\n",
    "print(\"📊 Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "print(\"🎯 ROC AUC Score:\", round(roc_auc_score(y_val, y_val_proba), 3))\n",
    "\n",
    "print(\"\\n📈 Logistic Regression Coefficients:\")\n",
    "print(pd.Series(logreg_model.coef_[0], index=features_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb713cd-18f1-4d18-80ef-ce594ac07407",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: experimental logistic regression modeling  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c60c8e-2cc7-4e94-a038-b125cbd48521",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7285e494-72bd-4329-bb7a-010aa0e7668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: value-based logistic regression classifier model  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad91810-93d6-4e4c-be74-b9c1952e40f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the value-engineered dataset\n",
    "wr_df_updated = pd.read_csv(\"wr_nfl_df_sorted_new_features_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe95164-e7fe-4845-92ba-ce22e46667da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c96e74-3ba8-458a-9585-cbba7fa8ae45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define value-based feature list for modeling\n",
    "value_based_features = [\n",
    "    'O_U', 'Total', 'dk_salary',\n",
    "    'value_ratio_dk', 'value_ratio_dk_3wk', 'value_ratio_dk_5wk', 'value_ratio_dk_7wk',\n",
    "    'value_ratio_dk_75th_percentile_1wk', 'value_ratio_dk_75th_percentile_3wk',\n",
    "    'value_ratio_dk_75th_percentile_5wk', 'value_ratio_dk_75th_percentile_7wk',\n",
    "    'value_ratio_dk_90th_percentile_3wk', 'value_ratio_dk_90th_percentile_5wk',\n",
    "    'value_ratio_dk_90th_percentile_7wk', 'value_ratio_dk_95th_percentile_3wk',\n",
    "    'value_ratio_dk_95th_percentile_5wk', 'value_ratio_dk_95th_percentile_7wk',\n",
    "    'z_value_ratio_dk', 'value_ratio_fd', 'z_value_ratio_fd',\n",
    "    'tgt_ge_5', 'tgt_ge_7', 'rec_ge_5', 'rec_ge_7',\n",
    "    'target_share_ge_20', 'target_share_ge_30',\n",
    "    'over_100_yds'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379df509-0ee9-4833-b864-75e2faf2af39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1c7120-6628-4f7f-9570-b44121fb6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct feature matrix and target vector\n",
    "X_value = wr_df_updated[value_based_features].copy()\n",
    "y_value = wr_df_updated['hit_value_dk'].copy()\n",
    "\n",
    "# Drop rows with missing values\n",
    "X_value_clean = X_value.dropna()\n",
    "\n",
    "# Drop non-numeric column(s) for correlation only\n",
    "X_corr = X_value_clean.drop(columns=['O_U'])\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X_corr.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(18, 12))\n",
    "sns.heatmap(\n",
    "    corr_matrix,\n",
    "    annot=False,\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"Correlation Matrix: Value-Based Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f70898-6509-44f5-99f5-f322109eaff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6742d97-d54d-460e-9699-ce1adacd7f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upated feature list\n",
    "\n",
    "# Define cleaned value-based feature list for modeling\n",
    "value_based_features = [\n",
    "    'Total', 'dk_salary',\n",
    "    'value_ratio_dk', 'value_ratio_dk_3wk', 'value_ratio_dk_5wk', 'value_ratio_dk_7wk',\n",
    "    'value_ratio_dk_90th_percentile_3wk', 'value_ratio_dk_90th_percentile_5wk',\n",
    "    'value_ratio_dk_90th_percentile_7wk',\n",
    "    'z_value_ratio_dk'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226f959-a995-4997-a105-f48c645f7e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb61166-6e04-46cb-994e-1e5f35453e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Construct X and y\n",
    "X_value = wr_df_updated[value_based_features].copy()\n",
    "y_value = wr_df_updated['hit_value_dk'].copy()\n",
    "\n",
    "# Drop rows with missing values for correlation matrix\n",
    "X_value_clean = X_value.dropna()\n",
    "\n",
    "# Compute and plot the updated correlation matrix\n",
    "corr_matrix_updated = X_value_clean.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    corr_matrix_updated,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap='coolwarm',\n",
    "    center=0,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title(\"Updated Correlation Matrix: Cleaned Value-Based Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5fd876-4811-4b53-a499-d16d268f5bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f4b5e3-e191-442a-9c5b-e9f94225f3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of the train, validate, test data\n",
    "\n",
    "# Extract feature matrix and target vector\n",
    "X_all = wr_df_updated[value_based_features].copy()\n",
    "y_all = wr_df_updated['hit_value_dk'].copy()\n",
    "\n",
    "# Include the season column to guide the split\n",
    "season_series = wr_df_updated['season']\n",
    "\n",
    "# Drop rows with missing values in X\n",
    "X_all_clean = X_all.dropna()\n",
    "y_all_clean = y_all.loc[X_all_clean.index]\n",
    "season_clean = season_series.loc[X_all_clean.index]\n",
    "\n",
    "# Create masks for each dataset\n",
    "train_mask = season_clean < 2023\n",
    "val_mask = season_clean == 2023\n",
    "test_mask = season_clean == 2024\n",
    "\n",
    "# Apply the masks\n",
    "X_train = X_all_clean[train_mask]\n",
    "y_train = y_all_clean[train_mask]\n",
    "\n",
    "X_val = X_all_clean[val_mask]\n",
    "y_val = y_all_clean[val_mask]\n",
    "\n",
    "X_test = X_all_clean[test_mask]\n",
    "y_test = y_all_clean[test_mask]\n",
    "\n",
    "# Return the shapes of each split to confirm\n",
    "X_train.shape, X_val.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48be2a41-7fe1-40b5-8854-a2e0559d925c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fc7bc-e85e-48c5-b3f5-1e0f9f92bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "# Initialize and fit logistic regression model on training data only\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Output training set score (accuracy) for quick verification\n",
    "train_accuracy = model.score(X_train, y_train)\n",
    "train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540dfa95-8f08-4202-8d8c-42618453ec66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c25734-0908-450d-bed2-85ddf3bcde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature impportance\n",
    "# Extract feature names and coefficients\n",
    "coefficients = pd.Series(model.coef_[0], index=X_train.columns)\n",
    "\n",
    "# Sort by absolute value for importance\n",
    "coeff_sorted = coefficients.reindex(coefficients.abs().sort_values(ascending=False).index)\n",
    "\n",
    "coeff_sorted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa3113f-aff0-4c7d-888e-72c15b55eb5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10245394-e6c1-4ec8-ba15-0d45019b1d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated features\n",
    "\n",
    "# Remove 'z_value_ratio_dk' from the feature list\n",
    "features_no_zscore = [\n",
    "    'Total', 'dk_salary',\n",
    "    'value_ratio_dk', 'value_ratio_dk_3wk', 'value_ratio_dk_5wk', 'value_ratio_dk_7wk',\n",
    "    'value_ratio_dk_90th_percentile_3wk', 'value_ratio_dk_90th_percentile_5wk',\n",
    "    'value_ratio_dk_90th_percentile_7wk'\n",
    "]\n",
    "\n",
    "# Create new training feature matrix\n",
    "X_train_nz = X_train[features_no_zscore]\n",
    "X_val_nz = X_val[features_no_zscore]\n",
    "\n",
    "# Refit logistic regression model\n",
    "model_nz = LogisticRegression(max_iter=1000)\n",
    "model_nz.fit(X_train_nz, y_train)\n",
    "\n",
    "# Extract and sort coefficients again\n",
    "coefficients_nz = pd.Series(model_nz.coef_[0], index=X_train_nz.columns)\n",
    "coeff_sorted_nz = coefficients_nz.reindex(coefficients_nz.abs().sort_values(ascending=False).index)\n",
    "\n",
    "coeff_sorted_nz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916764b-385d-45d3-b192-917fa0b70ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93864de-89c6-41a8-ac85-075b2cd9c0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final feature list excluding dk_salary and value_ratio_dk\n",
    "features_cleaned = [\n",
    "    'Total',\n",
    "    'value_ratio_dk_3wk', 'value_ratio_dk_5wk', 'value_ratio_dk_7wk',\n",
    "    'value_ratio_dk_90th_percentile_3wk', 'value_ratio_dk_90th_percentile_5wk',\n",
    "    'value_ratio_dk_90th_percentile_7wk'\n",
    "]\n",
    "\n",
    "# Extract cleaned X and y\n",
    "X_all = wr_df_updated[features_cleaned].copy()\n",
    "y_all = wr_df_updated['hit_value_dk'].copy()\n",
    "season_series = wr_df_updated['season']\n",
    "\n",
    "# Drop NaNs\n",
    "X_all_clean = X_all.dropna()\n",
    "y_all_clean = y_all.loc[X_all_clean.index]\n",
    "season_clean = season_series.loc[X_all_clean.index]\n",
    "\n",
    "# Create season-based splits\n",
    "train_mask = season_clean < 2023\n",
    "val_mask = season_clean == 2023\n",
    "\n",
    "X_train = X_all_clean[train_mask]\n",
    "y_train = y_all_clean[train_mask]\n",
    "\n",
    "X_val = X_all_clean[val_mask]\n",
    "y_val = y_all_clean[val_mask]\n",
    "\n",
    "# Fit model on updated training set\n",
    "model_cleaned = LogisticRegression(max_iter=1000)\n",
    "model_cleaned.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on 2023 validation set\n",
    "y_val_pred = model_cleaned.predict(X_val)\n",
    "y_val_prob = model_cleaned.predict_proba(X_val)[:, 1]\n",
    "\n",
    "\n",
    "val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_prob)\n",
    "\n",
    "val_report, val_roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78626ed8-4fd5-43b8-867e-7fecc573d7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76be00-a335-4e17-95b7-b04d6816f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the classification report dictionary for better readability\n",
    "def format_classification_report(report_dict, auc_score):\n",
    "    # Extract main classes\n",
    "    rows = []\n",
    "    for label in ['False', 'True', 'accuracy', 'macro avg', 'weighted avg']:\n",
    "        if label == 'accuracy':\n",
    "            rows.append(['accuracy', '', '', f\"{report_dict['accuracy']:.3f}\", ''])\n",
    "        else:\n",
    "            row = report_dict[label]\n",
    "            rows.append([\n",
    "                label,\n",
    "                f\"{row['precision']:.3f}\",\n",
    "                f\"{row['recall']:.3f}\",\n",
    "                f\"{row['f1-score']:.3f}\",\n",
    "                f\"{row['support']:.0f}\"\n",
    "            ])\n",
    "    # Append AUC\n",
    "    rows.append(['roc_auc', '', '', f\"{auc_score:.3f}\", ''])\n",
    "\n",
    "    # Create a DataFrame for display\n",
    "    return pd.DataFrame(rows, columns=['Metric', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "\n",
    "# Apply formatting\n",
    "formatted_val_report = format_classification_report(val_report, val_roc_auc)\n",
    "formatted_val_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b33edbc-8a8a-40cb-bdaf-7ce0273fc301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecca8722-0f0c-4794-a4dc-84b004fd9062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "# Apply the same cleaned feature set to 2024 test data\n",
    "test_mask = season_clean == 2024\n",
    "X_test = X_all_clean[test_mask]\n",
    "y_test = y_all_clean[test_mask]\n",
    "\n",
    "# Predict and evaluate on 2024 test set\n",
    "y_test_pred = model_cleaned.predict(X_test)\n",
    "y_test_prob = model_cleaned.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute evaluation metrics\n",
    "test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# Format the output\n",
    "def format_classification_report(report_dict, auc_score):\n",
    "    rows = []\n",
    "    for label in ['False', 'True', 'accuracy', 'macro avg', 'weighted avg']:\n",
    "        if label == 'accuracy':\n",
    "            rows.append(['accuracy', '', '', f\"{report_dict['accuracy']:.3f}\", ''])\n",
    "        else:\n",
    "            row = report_dict[label]\n",
    "            rows.append([\n",
    "                label,\n",
    "                f\"{row['precision']:.3f}\",\n",
    "                f\"{row['recall']:.3f}\",\n",
    "                f\"{row['f1-score']:.3f}\",\n",
    "                f\"{row['support']:.0f}\"\n",
    "            ])\n",
    "    rows.append(['roc_auc', '', '', f\"{auc_score:.3f}\", ''])\n",
    "    return pd.DataFrame(rows, columns=['Metric', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "\n",
    "formatted_test_report = format_classification_report(test_report, test_roc_auc)\n",
    "formatted_test_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11665a53-fb07-4b54-9bd3-b0ba39eeda7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04896b8c-c1fd-47e5-adb9-bc9d7b9ccb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PDF summary of training, validation, and test results\n",
    "class ModelSummaryPDF(FPDF):\n",
    "    def header(self):\n",
    "        self.set_font(\"Arial\", \"B\", 14)\n",
    "        self.cell(0, 10, \"WR Hit Value Classification Model Summary\", ln=True, align=\"C\")\n",
    "        self.ln(5)\n",
    "\n",
    "    def add_section(self, title, dataframe):\n",
    "        self.set_font(\"Arial\", \"B\", 12)\n",
    "        self.cell(0, 10, title, ln=True)\n",
    "        self.set_font(\"Arial\", \"\", 10)\n",
    "        self.ln(2)\n",
    "        col_widths = [30, 25, 25, 25, 25]\n",
    "        for i, col in enumerate(dataframe.columns):\n",
    "            self.cell(col_widths[i], 8, col, border=1)\n",
    "        self.ln()\n",
    "        for _, row in dataframe.iterrows():\n",
    "            for i, item in enumerate(row):\n",
    "                self.cell(col_widths[i], 8, str(item), border=1)\n",
    "            self.ln()\n",
    "        self.ln(5)\n",
    "\n",
    "# Format 2023 and 2024 reports again using the existing function\n",
    "formatted_val_report = format_classification_report(val_report, val_roc_auc)\n",
    "formatted_test_report = format_classification_report(test_report, test_roc_auc)\n",
    "\n",
    "# Create and populate PDF\n",
    "pdf = ModelSummaryPDF()\n",
    "pdf.add_page()\n",
    "pdf.add_section(\"Validation Set (2023)\", formatted_val_report)\n",
    "pdf.add_section(\"Test Set (2024)\", formatted_test_report)\n",
    "\n",
    "# Save PDF\n",
    "pdf_output_path = \"wr_value_model_summary.pdf\"\n",
    "pdf.output(pdf_output_path)\n",
    "\n",
    "pdf_output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cc61a6-4754-4c02-9746-056414c0a6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd16de-a829-4cd2-8be0-4bcf3b534a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Value-based salary model: fully validated, documented, and reproducible  ***\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Step 1: Load and define features ---\n",
    "df = pd.read_csv(\"wr_nfl_df_sorted_new_features_final.csv\")\n",
    "\n",
    "value_based_features = [\n",
    "    'Total',\n",
    "    'value_ratio_dk_3wk', 'value_ratio_dk_5wk', 'value_ratio_dk_7wk',\n",
    "    'value_ratio_dk_90th_percentile_3wk', 'value_ratio_dk_90th_percentile_5wk',\n",
    "    'value_ratio_dk_90th_percentile_7wk'\n",
    "]\n",
    "\n",
    "X_all = df[value_based_features].copy()\n",
    "y_all = df['hit_value_dk'].copy()\n",
    "season = df['season']\n",
    "\n",
    "# --- Step 2: Drop rows with missing values ---\n",
    "X_all_clean = X_all.dropna()\n",
    "y_all_clean = y_all.loc[X_all_clean.index]\n",
    "season_clean = season.loc[X_all_clean.index]\n",
    "\n",
    "# --- Step 3: Create season-based splits ---\n",
    "train_mask = season_clean < 2023\n",
    "val_mask = season_clean == 2023\n",
    "test_mask = season_clean == 2024\n",
    "\n",
    "X_train = X_all_clean[train_mask]\n",
    "y_train = y_all_clean[train_mask]\n",
    "\n",
    "X_val = X_all_clean[val_mask]\n",
    "y_val = y_all_clean[val_mask]\n",
    "\n",
    "X_test = X_all_clean[test_mask]\n",
    "y_test = y_all_clean[test_mask]\n",
    "\n",
    "# --- Step 4: Fit model on training data ---\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 5: Evaluate on validation set ---\n",
    "y_val_pred = model.predict(X_val)\n",
    "y_val_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "val_roc_auc = roc_auc_score(y_val, y_val_prob)\n",
    "\n",
    "# --- Step 6: Evaluate on test set ---\n",
    "y_test_pred = model.predict(X_test)\n",
    "y_test_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "test_roc_auc = roc_auc_score(y_test, y_test_prob)\n",
    "\n",
    "# --- Step 7: Format reports (optional) ---\n",
    "def format_classification_report(report_dict, auc_score):\n",
    "    rows = []\n",
    "    for label in ['False', 'True', 'accuracy', 'macro avg', 'weighted avg']:\n",
    "        if label == 'accuracy':\n",
    "            rows.append(['accuracy', '', '', f\"{report_dict['accuracy']:.3f}\", ''])\n",
    "        else:\n",
    "            row = report_dict[label]\n",
    "            rows.append([\n",
    "                label,\n",
    "                f\"{row['precision']:.3f}\",\n",
    "                f\"{row['recall']:.3f}\",\n",
    "                f\"{row['f1-score']:.3f}\",\n",
    "                f\"{row['support']:.0f}\"\n",
    "            ])\n",
    "    rows.append(['roc_auc', '', '', f\"{auc_score:.3f}\", ''])\n",
    "    return pd.DataFrame(rows, columns=['Metric', 'Precision', 'Recall', 'F1-Score', 'Support'])\n",
    "\n",
    "val_report_df = format_classification_report(val_report, val_roc_auc)\n",
    "test_report_df = format_classification_report(test_report, test_roc_auc)\n",
    "\n",
    "# --- Step 8: Optional: Plot correlation matrix ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(X_train.corr(), annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix: Value-Based Features (Train Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Output reports ---\n",
    "print(\"Validation Report (2023):\")\n",
    "print(val_report_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nTest Report (2024):\")\n",
    "print(test_report_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fbc07e-ccd8-4bcb-94c2-213e1b3a07e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: value-based logistic regression classifier model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849e7d95-d63a-4200-97fc-b175a95a9644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a97e0-af9d-4b3c-9ee4-d456aaf55453",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: performance-based logistic regression classifier models ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855fc01-a0aa-44b1-9353-99f7e4a7968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"wr_nfl_df_sorted_new_features_final.csv\")\n",
    "\n",
    "# Get full column list\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Display columns that contain 'rec_', 'tgt_', 'rec_yds_', or 'rec_air_yards_'\n",
    "performance_raw_like = sorted([\n",
    "    col for col in all_columns\n",
    "    if col.startswith(('rec_', 'tgt_', 'rec_yds_', 'rec_air_yards_'))\n",
    "])\n",
    "\n",
    "# view list (optional)\n",
    "# performance_raw_like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe10b5-7d6e-419f-b964-95121725788a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3314bc87-dd21-4b36-83a3-ca7f3cb6b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inclusion criteria for safe performance-based features\n",
    "include_keywords = ['_avg', '_lag_', '_delta', '_z', '_percentile']\n",
    "exclude_keywords = ['fpts', 'hit_', 'value_', 'salary', 'position', 'over_', 'ge_', 'rec_touchdowns', 'Total', 'O_U']\n",
    "\n",
    "# Build candidate feature list\n",
    "initial_perf_features = [\n",
    "    col for col in df.columns\n",
    "    if any(kw in col for kw in include_keywords)\n",
    "    and not any(kw in col for kw in exclude_keywords)\n",
    "]\n",
    "\n",
    "# Sort alphabetically for review\n",
    "initial_perf_features = sorted(initial_perf_features)\n",
    "\n",
    "# view list\n",
    "# initial_perf_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189a8161-d824-4d22-85ce-3868430b473b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00f225-7580-4632-a077-9c1f184f7f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the dataframe using the selected features\n",
    "X_perf = df[initial_perf_features].copy()\n",
    "\n",
    "# Drop rows with missing values\n",
    "X_perf_clean = X_perf.dropna()\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = X_perf_clean.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(18, 14))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix - Performance-Based Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1b60d-3846-4284-9764-1b05ff9a2557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb42f12-23e3-410c-aecf-bbc9476d41c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final approved feature list\n",
    "performance_features = [\n",
    "    'rec_3wk_avg', 'rec_yds_5wk_avg', 'rec_air_yards_7wk_avg', 'tgt_3wk_avg',\n",
    "    'tgt_3wk_delta', 'rec_3wk_delta', 'rec_yds_3wk_delta',\n",
    "    'rec_lag_1', 'tgt_lag_1', 'rec_yds_lag_1', 'rec_air_yards_lag_1',\n",
    "    'targets_75th_percentile_3wk', 'rec_75th_percentile_3wk', 'receiving_yards_75th_percentile_3wk',\n",
    "    'rec_7wk_avg_z', 'rec_air_yards_7wk_avg_clipped_z', 'target_share_z'\n",
    "]\n",
    "\n",
    "# Subset and drop NA for clean correlation\n",
    "X_perf_refined = df[performance_features].dropna()\n",
    "\n",
    "# Correlation matrix\n",
    "corr_matrix_final = X_perf_refined.corr()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix_final, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Final Correlation Matrix - Refined Performance Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a55df-35c4-4a3e-820f-98a21665ec80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b52d6e2-51db-48d4-b40b-212c05e8493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# view the shape of the training, validation, and test sets\n",
    "\n",
    "# Step 1: Define the fixed threshold target\n",
    "fpts_threshold = 10\n",
    "df['hit_perf_fixed'] = (df['fpts'] >= fpts_threshold).astype(int)\n",
    "\n",
    "# Step 2: Prepare feature matrix (X) and target vector (y)\n",
    "X = df[performance_features].copy()\n",
    "y = df['hit_perf_fixed'].copy()\n",
    "\n",
    "# Step 3: Add season column for splitting\n",
    "df_season = df[['season']].copy()\n",
    "X['season'] = df_season\n",
    "y.index = df_season.index  # Align indices\n",
    "\n",
    "# Step 4: Apply train/validate/test split by season\n",
    "X_train = X[X['season'] < 2023].drop(columns='season')\n",
    "y_train = y[X['season'] < 2023]\n",
    "\n",
    "X_val = X[X['season'] == 2023].drop(columns='season')\n",
    "y_val = y[X['season'] == 2023]\n",
    "\n",
    "X_test = X[X['season'] == 2024].drop(columns='season')\n",
    "y_test = y[X['season'] == 2024]\n",
    "\n",
    "# Confirm shape\n",
    "(X_train.shape, y_train.shape), (X_val.shape, y_val.shape), (X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ba0ca-771a-4765-bee3-1b260622490a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a82025c-27a8-442e-89cf-a8fffd6f8b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# train the model - feature analysis\n",
    "\n",
    "# Drop missing values from training set only\n",
    "X_train_clean = X_train.dropna()\n",
    "y_train_clean = y_train.loc[X_train_clean.index]\n",
    "\n",
    "# Fit logistic regression on cleaned training set\n",
    "logreg_perf = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "logreg_perf.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "# Get and display feature importances\n",
    "coefficients = pd.Series(logreg_perf.coef_[0], index=X_train_clean.columns).sort_values(key=abs, ascending=False)\n",
    "coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245e301-8932-4ac0-89bb-99cd195fb22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990680d2-7085-4f87-af51-9f679589f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# Revise the features list based on results\n",
    "# Step 1: Define revised feature list (remove lag features)\n",
    "performance_features_revised = [\n",
    "    'rec_3wk_avg', 'rec_yds_5wk_avg', 'rec_air_yards_7wk_avg', 'tgt_3wk_avg',\n",
    "    'tgt_3wk_delta', 'rec_3wk_delta', 'rec_yds_3wk_delta',\n",
    "    'targets_75th_percentile_3wk', 'rec_75th_percentile_3wk', 'receiving_yards_75th_percentile_3wk',\n",
    "    'rec_7wk_avg_z', 'rec_air_yards_7wk_avg_clipped_z', 'target_share_z'\n",
    "]\n",
    "\n",
    "# Step 2: Subset data and drop NAs for correlation\n",
    "X_perf_revised = df[performance_features_revised].dropna()\n",
    "\n",
    "# Step 3: Compute and display correlation matrix\n",
    "corr_matrix_revised = X_perf_revised.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr_matrix_revised, annot=True, fmt=\".2f\", cmap='coolwarm', center=0)\n",
    "plt.title(\"Correlation Matrix - Revised Performance Features (No Lag)\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b5dafe-b64f-4344-b6df-1af7cfe5f69e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab52d7-63c6-43c3-bfa4-5a2ee8ef5873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# Step 4: Prepare revised feature matrix (X) and target vector (y)\n",
    "X = df[performance_features_revised].copy()\n",
    "y = df['hit_perf_fixed'].copy()\n",
    "\n",
    "# Add season column for splitting\n",
    "X['season'] = df['season']\n",
    "y.index = df.index\n",
    "\n",
    "# Apply train/val/test split by season\n",
    "X_train = X[X['season'] < 2023].drop(columns='season')\n",
    "y_train = y[X['season'] < 2023]\n",
    "\n",
    "X_val = X[X['season'] == 2023].drop(columns='season')\n",
    "y_val = y[X['season'] == 2023]\n",
    "\n",
    "X_test = X[X['season'] == 2024].drop(columns='season')\n",
    "y_test = y[X['season'] == 2024]\n",
    "\n",
    "# Drop missing values in training set\n",
    "X_train_clean = X_train.dropna()\n",
    "y_train_clean = y_train.loc[X_train_clean.index]\n",
    "\n",
    "# Check shapes after dropping NA\n",
    "X_train_clean.shape, y_train_clean.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d61107-7d29-4a9a-9633-e776f5cec5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e282ab8-b506-4de2-9c96-7694e40d2940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# Retrain logistic regression with revised feature set\n",
    "logreg_perf_revised = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "logreg_perf_revised.fit(X_train_clean, y_train_clean)\n",
    "\n",
    "# Display sorted coefficients\n",
    "coefficients_revised = pd.Series(\n",
    "    logreg_perf_revised.coef_[0], index=X_train_clean.columns\n",
    ").sort_values(key=abs, ascending=False)\n",
    "\n",
    "coefficients_revised\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3c662-9447-4c09-a2f9-dff13246ef56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67eb1c-fd02-43f2-991d-cedc20a16177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# Validate the model\n",
    "\n",
    "# Drop missing values in validation set\n",
    "X_val_clean = X_val.dropna()\n",
    "y_val_clean = y_val.loc[X_val_clean.index]\n",
    "\n",
    "# Predict on validation set\n",
    "y_val_pred = logreg_perf_revised.predict(X_val_clean)\n",
    "y_val_proba = logreg_perf_revised.predict_proba(X_val_clean)[:, 1]\n",
    "\n",
    "# Classification report and ROC AUC\n",
    "val_report = classification_report(y_val_clean, y_val_pred, output_dict=True)\n",
    "val_auc = roc_auc_score(y_val_clean, y_val_proba)\n",
    "\n",
    "val_report, val_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2601129e-4a36-4f96-86a9-db3ff2b67f27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e5f8c-3841-4788-8653-520e954558cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# Validate the model\n",
    "\n",
    "# Pretty print a classification report and ROC AUC score\n",
    "def print_classification_summary(report_dict, roc_auc_value):\n",
    "  \n",
    "    df = pd.DataFrame(report_dict).transpose().round(3)\n",
    "    \n",
    "    # Ensure 'support' is always an integer for readability\n",
    "    if 'support' in df.columns:\n",
    "        df['support'] = df['support'].astype(int)\n",
    "    \n",
    "    # Add ROC AUC row to the bottom\n",
    "    df.loc[\"ROC AUC\"] = [\"\", \"\", \"\", round(roc_auc_value, 3)]\n",
    "    \n",
    "    # Convert to string and print\n",
    "    print(df.to_string(index=True))\n",
    "\n",
    "print(\"\\n--- 2023 VALIDATION RESULTS (fpts>= defined threshold ) ---\")\n",
    "print_classification_summary(val_report, val_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f704b-e6fd-431b-8608-5e7d2a86a5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c533056-b104-499d-8396-032fe1b88a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using fpts>= a defined threshold\n",
    "# Test the model\n",
    "\n",
    "# Drop missing values in test set\n",
    "X_test_clean = X_test.dropna()\n",
    "y_test_clean = y_test.loc[X_test_clean.index]\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred = logreg_perf_revised.predict(X_test_clean)\n",
    "y_test_proba = logreg_perf_revised.predict_proba(X_test_clean)[:, 1]\n",
    "\n",
    "# Classification report and ROC AUC\n",
    "test_report = classification_report(y_test_clean, y_test_pred, output_dict=True)\n",
    "test_auc = roc_auc_score(y_test_clean, y_test_proba)\n",
    "\n",
    "# Format the report for readability\n",
    "test_report_df = pd.DataFrame(test_report).transpose()\n",
    "test_report_df['support'] = test_report_df['support'].astype(int)\n",
    "test_report_df_rounded = test_report_df.round(3)\n",
    "test_report_df_rounded.loc[\"ROC AUC\"] = [\"\", \"\", \"\", test_auc]\n",
    "\n",
    "# Print readable output\n",
    "# Print output\n",
    "print(\"\\n--- 2024 TEST RESULTS (fpts>= defined threshold ) ---\")\n",
    "print(test_report_df_rounded.to_string(index=True))\n",
    "print(f\"\\nROC AUC: {test_auc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4476a-6a10-468f-ab10-8d4c622caf85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699d844-2f23-45ff-9b45-67097f31759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_summary(test_report, test_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db0b44-7d33-42e4-a66f-065601dd23c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba106d-553b-4d92-af54-1cc28aaac71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using a percentile-based threshold \n",
    "# create the target\n",
    "\n",
    "# --- Define the percentile threshold for the target (adjustable) ---\n",
    "performance_percentile_threshold = 0.85  # This can be changed to 0.75, 0.90, etc.\n",
    "\n",
    "# --- Filter training data for seasons < 2023 ---\n",
    "training_fpts = df[df['season'] < 2023]['fpts']\n",
    "\n",
    "# --- Calculate the fantasy point value at the desired percentile ---\n",
    "fpts_percentile_value = training_fpts.quantile(performance_percentile_threshold)\n",
    "\n",
    "# --- Create binary target column based on this threshold ---\n",
    "df['hit_perf_percentile'] = (df['fpts'] >= fpts_percentile_value).astype(int)\n",
    "\n",
    "# --- Output the threshold value ---\n",
    "fpts_percentile_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c105872-6937-4a64-9c11-cbd65d950bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c70082-c63a-4258-8342-3b1575fa8fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using a percentile-based threshold \n",
    "# get the shape of the training data\n",
    "\n",
    "# Rebuild feature matrix (X) and updated target (y) after redefining percentile target\n",
    "X = df[performance_features].copy()\n",
    "y = df['hit_perf_percentile'].copy()\n",
    "\n",
    "# Add season for splitting\n",
    "X['season'] = df['season']\n",
    "y.index = df.index\n",
    "\n",
    "# Train: seasons < 2023\n",
    "X_train = X[X['season'] < 2023].drop(columns='season').dropna()\n",
    "y_train = y.loc[X_train.index]\n",
    "\n",
    "# Validation: season == 2023\n",
    "X_val = X[X['season'] == 2023].drop(columns='season').dropna()\n",
    "y_val = y.loc[X_val.index]\n",
    "\n",
    "# Test: season == 2024\n",
    "X_test = X[X['season'] == 2024].drop(columns='season').dropna()\n",
    "y_test = y.loc[X_test.index]\n",
    "\n",
    "# Display the shape of all datasets for verification\n",
    "{\n",
    "    \"Train Features\": X_train.shape,\n",
    "    \"Train Target\": y_train.shape,\n",
    "    \"Validation Features\": X_val.shape,\n",
    "    \"Validation Target\": y_val.shape,\n",
    "    \"Test Features\": X_test.shape,\n",
    "    \"Test Target\": y_test.shape\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c268ac6-6415-465f-a8c7-a26f419e63ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89f330-bd13-461d-8936-affd5e0533e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using a percentile-based threshold \n",
    "# train the model - show the features and coefficients\n",
    "\n",
    "# Train logistic regression model on the 85th percentile target\n",
    "logreg_perf_percentile = LogisticRegression(max_iter=1000, solver='liblinear')\n",
    "logreg_perf_percentile.fit(X_train, y_train)\n",
    "\n",
    "# Display coefficients for interpretability\n",
    "coefficients_percentile = pd.Series(\n",
    "    logreg_perf_percentile.coef_[0],\n",
    "    index=X_train.columns\n",
    ").sort_values(key=abs, ascending=False)\n",
    "\n",
    "coefficients_percentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec4d3e-4339-4c73-8b3a-c495b3645730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a54ca2-433f-4363-a169-3870878629dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using a percentile-based threshold \n",
    "# validate the model\n",
    "\n",
    "# Predict on validation set (2023)\n",
    "y_val_pred = logreg_perf_percentile.predict(X_val)\n",
    "y_val_proba = logreg_perf_percentile.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "val_report_percentile = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "val_auc_percentile = roc_auc_score(y_val, y_val_proba)\n",
    "\n",
    "# Format for readable output\n",
    "val_df_percentile = pd.DataFrame(val_report_percentile).transpose().round(3)\n",
    "val_df_percentile['support'] = val_df_percentile['support'].astype(int)\n",
    "val_df_percentile.loc[\"ROC AUC\"] = [\"\", \"\", \"\", round(val_auc_percentile, 3)]\n",
    "\n",
    "# Print output\n",
    "print(\"\\n--- 2023 VALIDATION RESULTS (85th Percentile Target) ---\")\n",
    "print(val_df_percentile.to_string(index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c51401-af1f-4ee2-b52f-d4a5597d7a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f6d6b0-813c-48d7-9f29-51b501ef28e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_classification_summary(val_report_percentile, val_auc_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbc1796-3373-48fb-ab9d-b4ff7a8ba42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d47b6a1-3712-4920-95c0-f7c227eb6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based model using a percentile-based threshold \n",
    "# test the model\n",
    "\n",
    "# Predict on 2024 test set\n",
    "y_test_pred = logreg_perf_percentile.predict(X_test)\n",
    "y_test_proba = logreg_perf_percentile.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "test_report_percentile = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "test_auc_percentile = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "# Format output\n",
    "test_df_percentile = pd.DataFrame(test_report_percentile).transpose().round(3)\n",
    "test_df_percentile['support'] = test_df_percentile['support'].astype(int)\n",
    "test_df_percentile.loc[\"ROC AUC\"] = [\"\", \"\", \"\", round(test_auc_percentile, 3)]\n",
    "\n",
    "# Print test results\n",
    "print(\"\\n--- 2024 TEST RESULTS (85th Percentile Target) ---\")\n",
    "print(test_df_percentile.to_string(index=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eef703-33ad-4303-af56-2104be095ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: performance-based logistic regression classifier model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f5c907-45c5-4d05-a688-df0acb7328e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a222215-c852-41a3-a7d6-380e2d389c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: Prediction Dataframes  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff6dd4b-8666-46dd-a575-d3b9748c4a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Your helper functions ---\n",
    "def get_current_week():\n",
    "    current_date = datetime.now()\n",
    "    season_start_date = datetime(2024, 9, 4)\n",
    "    return ((current_date - season_start_date).days // 7) + 1\n",
    "\n",
    "def get_year_range(current_year, current_week, start_year=2017):\n",
    "    return list(range(start_year, current_year + 1)) if current_week <= 18 else list(range(start_year, current_year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f245b-1767-4b1b-b7e5-6dde6bc7a3ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7604b061-4935-4a17-995a-3b48c520b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and print all columns from the loaded dataframe\n",
    "df_columns = df.columns.tolist()\n",
    "\n",
    "print(f\"🧠 Total Columns: {len(df_columns)}\\n\")\n",
    "for col in df_columns:\n",
    "    print(col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62434959-5e9f-4e3c-aaf4-d1b603f6cf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa7931-0e09-4665-aebb-e70dbdf6eeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Baseline metadata columns\n",
    "baseline_cols = [\n",
    "    \"season\", \"season_type\", \"week\", \"name\", \"position\", \"recent_team\",\n",
    "    \"player_display_name\", \"team_abbr_x\", \"rost\", \"dk_salary\", \"fd_salary\"\n",
    "]\n",
    "\n",
    "# ✅ Target columns\n",
    "target_cols = [\n",
    "    \"hit_value_dk\",         # Value-based threshold\n",
    "    \"hit_perf_fixed\",       # FPTS ≥ 10 (formerly hit_perf_fpts10)\n",
    "    \"hit_perf_percentile\"   # Top 15% WR performance\n",
    "]\n",
    "\n",
    "# ✅ Optional display columns (for dashboard or inspection)\n",
    "optional_display_cols = [\n",
    "    \"fpts\", \"value_ratio_dk\", \"fpts_3wk_avg\", \n",
    "    \"targets\", \"receptions\", \"receiving_yards\", \n",
    "    \"receiving_air_yards\", \"target_share\", \"catch_percentage\"\n",
    "]\n",
    "\n",
    "# ✅ Feature columns for logistic regression (hit_value_dk)\n",
    "feature_cols = [\n",
    "    \"dk_salary\", \"rost\", \"fpts_3wk_avg\", \"value_ratio_dk\",\n",
    "    \"targets\", \"receptions\", \"receiving_yards\", \n",
    "    \"receiving_air_yards\", \"target_share\", \"catch_percentage\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fae566-317c-4d30-8dec-0986cc408de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename df to wr\n",
    "wr_df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147b68c-8bee-4ff8-8bb4-83ba9ab4c3c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54be578-ca26-47ce-bff3-c1b91a43b100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value-Based Prediction Dataframe\n",
    "\n",
    "# column cleaning\n",
    "for col in feature_cols:\n",
    "    non_numeric = wr_df[col][~wr_df[col].apply(lambda x: isinstance(x, (int, float)))]\n",
    "    if not non_numeric.empty:\n",
    "        print(f\"\\n🚨 Column: {col}\")\n",
    "        print(non_numeric.unique()[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01942784-a0c3-47d5-9d0d-19b459e1c4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9dfa63-69e6-47c6-a060-20724da0ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value-Based Prediction Dataframe\n",
    "# column cleaning and data formatting\n",
    "\n",
    "# Patch: recompute value_ratio_dk if missing or all zero\n",
    "if df[\"value_ratio_dk\"].sum() == 0:\n",
    "    df[\"value_ratio_dk\"] = df[\"fpts\"] / df[\"dk_salary\"]\n",
    "\n",
    "# scale the ratio to get integers and not tiny decimals\n",
    "df[\"value_ratio_dk\"] = df[\"fpts\"] / (df[\"dk_salary\"] / 1000)\n",
    "df[\"value_ratio_dk\"] = df[\"value_ratio_dk\"].replace([np.inf, -np.inf], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7aa2f-3770-43d7-a116-34e139eaa779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccec3f0-617d-49f7-873a-e236e1319ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value-Based Prediction Dataframe\n",
    "\n",
    "# check to ensure cleaning was successful\n",
    "df[\"value_ratio_dk\"].describe()\n",
    "df[\"value_ratio_dk\"].value_counts().head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac4254e-4c76-4dcb-9cd5-04f4cdf9cff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6e745f-43e4-40bb-9574-3592281c88ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ** INPUT REQUIRED **\n",
    "# add filtering option for backtest or live predictions\n",
    "\n",
    "# filter option for backtest or live predictions\n",
    "backtest_mode = True  # Set to False for in-season use\n",
    "\n",
    "# Define current season context\n",
    "current_year = datetime.now().year\n",
    "current_week = get_current_week()\n",
    "years = get_year_range(current_year, current_week)\n",
    "\n",
    "# Filter the main df\n",
    "if backtest_mode:\n",
    "    df = df[df[\"season\"].isin(years)].copy()\n",
    "else:\n",
    "    df = df[df[\"season\"] == current_year].copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8fa39-fd12-46ea-984d-b68ea772bfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fb545-8b5f-4681-a6c0-d9c0df397d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** Create New Dataframe: Value-Based Prediction Dataframe ***\n",
    "\n",
    "# --- Step 1: Filter dataset ---\n",
    "wr_df = wr_df[wr_df[\"position\"] == \"WR\"]\n",
    "wr_df = wr_df.dropna(subset=[\"hit_value_dk\"])\n",
    "\n",
    "\n",
    "# --- Step 2: Filter dataset ---\n",
    "percent_cols = [\"catch_percentage\", \"target_share\", \"rost\"]\n",
    "\n",
    "for col in percent_cols:\n",
    "    if col in wr_df.columns:\n",
    "        wr_df[col] = (\n",
    "            wr_df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace('%', '', regex=False)\n",
    "        )\n",
    "        wr_df[col] = pd.to_numeric(wr_df[col], errors='coerce')\n",
    "\n",
    "\n",
    "# ✅ Now drop rows with any remaining NA in features\n",
    "wr_df = wr_df.dropna(subset=feature_cols)\n",
    "\n",
    "\n",
    "X = wr_df[feature_cols].copy()\n",
    "y = wr_df[\"hit_value_dk\"]\n",
    "\n",
    "# --- Step 3: Standardize features ---\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- Step 4: Train logistic regression ---\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# --- Step 5: Generate predictions ---\n",
    "pred_probs = model.predict_proba(X_scaled)[:, 1]\n",
    "pred_classes = model.predict(X_scaled)\n",
    "\n",
    "# --- Step 6: Add predictions to dataframe ---\n",
    "wr_df[\"pred_prob_value\"] = pred_probs\n",
    "wr_df[\"pred_class_value\"] = pred_classes\n",
    "\n",
    "# --- Step 7: Build prediction dataframe ---\n",
    "prediction_df_value = wr_df[\n",
    "    baseline_cols +\n",
    "    target_cols[:1] +  # Only 'hit_value_dk'\n",
    "    [\"pred_prob_value\", \"pred_class_value\"] +\n",
    "    optional_display_cols\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4485ee0-57ac-4a76-8900-b75b92386bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3bccee-4236-43ae-934c-bb85b568488d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspection of prediction_df_value\n",
    "print(f\"✅ Rows: {len(prediction_df_value)} | Columns: {prediction_df_value.shape[1]}\")\n",
    "display(prediction_df_value.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681faac5-e082-4f64-955b-29614b8bc6f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5ea190-49c6-4a4b-96a0-11f49adb032f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify distribution of new value-based prediction dataframe ***\n",
    "\n",
    "# Distribution of predicted probabilities\n",
    "prediction_df_value[\"pred_prob_value\"].describe()\n",
    "\n",
    "# Value counts for predicted vs. actual\n",
    "print(\"📊 Predicted Classes:\")\n",
    "print(prediction_df_value[\"pred_class_value\"].value_counts())\n",
    "\n",
    "print(\"\\n🎯 Actual Outcomes (hit_value_dk):\")\n",
    "print(prediction_df_value[\"hit_value_dk\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f2a082-9b0b-427a-8aca-d2a4e0bb1989",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe39729-e457-4db0-8cfd-622a602adf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** New Dataframe: performance-based prediction dataframe fpts>= threshold ***\n",
    "\n",
    "# --- Step 1: Filter dataset for FPTS >= 10 target ---\n",
    "wr_df = df.copy()\n",
    "wr_df = wr_df[wr_df[\"position\"] == \"WR\"]\n",
    "wr_df = wr_df.dropna(subset=[\"hit_perf_fixed\"])\n",
    "\n",
    "# --- Step 2: Fix percentage columns ---\n",
    "percent_cols = [\"catch_percentage\", \"target_share\", \"rost\"]\n",
    "for col in percent_cols:\n",
    "    if col in wr_df.columns:\n",
    "        wr_df[col] = (\n",
    "            wr_df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace('%', '', regex=False)\n",
    "        )\n",
    "        wr_df[col] = pd.to_numeric(wr_df[col], errors='coerce')\n",
    "\n",
    "# --- Step 3: Drop rows with missing feature values ---\n",
    "wr_df = wr_df.dropna(subset=feature_cols)\n",
    "\n",
    "# --- Step 4: Build model inputs ---\n",
    "X = wr_df[feature_cols].copy()\n",
    "y = wr_df[\"hit_perf_fixed\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# --- Step 5: Predictions ---\n",
    "wr_df[\"pred_prob_fpts10\"] = model.predict_proba(X_scaled)[:, 1]\n",
    "wr_df[\"pred_class_fpts10\"] = model.predict(X_scaled)\n",
    "\n",
    "# --- Step 6: Build prediction dataframe ---\n",
    "prediction_df_fpts10 = wr_df[\n",
    "    baseline_cols +\n",
    "    target_cols[1:2] +  # Only 'hit_perf_fixed'\n",
    "    [\"pred_prob_fpts10\", \"pred_class_fpts10\"] +\n",
    "    optional_display_cols\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e398d3-5f7c-4683-ab7d-e9920c635493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3fd4e5-67bc-43f6-b99e-572e363b99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based prediction dataframe fpts>= threshold \n",
    "# check the distribution and df\n",
    "\n",
    "# Size and preview\n",
    "print(f\"✅ Rows: {len(prediction_df_fpts10)} | Columns: {prediction_df_fpts10.shape[1]}\")\n",
    "display(prediction_df_fpts10.head())\n",
    "\n",
    "# Distribution of predicted classes\n",
    "print(\"\\n📊 Predicted Classes (FPTS10):\")\n",
    "print(prediction_df_fpts10[\"pred_class_fpts10\"].value_counts())\n",
    "\n",
    "# Distribution of actual outcomes\n",
    "print(\"\\n🎯 Actual Outcomes (hit_perf_fixed):\")\n",
    "print(prediction_df_fpts10[\"hit_perf_fixed\"].value_counts())\n",
    "\n",
    "# Optional: Look at prediction probabilities\n",
    "print(\"\\n📈 Probability Distribution:\")\n",
    "print(prediction_df_fpts10[\"pred_prob_fpts10\"].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa8903-0aca-46f1-9123-a4188f4a122d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa39e1-75a2-454b-acfe-de4c7ae92ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** New Dataframe: performance-based prediction dataframe percentage>= threshold ***\n",
    "# --- Step 1: Filter dataset for 85th percentile target ---\n",
    "wr_df = df.copy()\n",
    "wr_df = wr_df[wr_df[\"position\"] == \"WR\"]\n",
    "wr_df = wr_df.dropna(subset=[\"hit_perf_percentile\"])\n",
    "\n",
    "# --- Step 2: Fix percentage columns ---\n",
    "percent_cols = [\"catch_percentage\", \"target_share\", \"rost\"]\n",
    "for col in percent_cols:\n",
    "    if col in wr_df.columns:\n",
    "        wr_df[col] = (\n",
    "            wr_df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace('%', '', regex=False)\n",
    "        )\n",
    "        wr_df[col] = pd.to_numeric(wr_df[col], errors='coerce')\n",
    "\n",
    "# --- Step 3: Drop NA in features ---\n",
    "wr_df = wr_df.dropna(subset=feature_cols)\n",
    "\n",
    "# --- Step 4: Prepare model inputs ---\n",
    "X = wr_df[feature_cols].copy()\n",
    "y = wr_df[\"hit_perf_percentile\"]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_scaled, y)\n",
    "\n",
    "# --- Step 5: Predictions ---\n",
    "wr_df[\"pred_prob_85pct\"] = model.predict_proba(X_scaled)[:, 1]\n",
    "wr_df[\"pred_class_85pct\"] = model.predict(X_scaled)\n",
    "\n",
    "# --- Step 6: Build prediction dataframe ---\n",
    "prediction_df_85pct = wr_df[\n",
    "    baseline_cols +\n",
    "    target_cols[2:] +  # Only 'hit_perf_percentile'\n",
    "    [\"pred_prob_85pct\", \"pred_class_85pct\"] +\n",
    "    optional_display_cols\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a56ad4-9ff3-4d09-8155-c9d32bd681fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357f70f-1acf-407f-94b5-e7961d824827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance-based prediction dataframe percentage>= threshold \n",
    "# check the distribution and df\n",
    "# ✅ Basic preview\n",
    "print(f\"✅ Rows: {len(prediction_df_85pct)} | Columns: {prediction_df_85pct.shape[1]}\")\n",
    "display(prediction_df_85pct.head())\n",
    "\n",
    "# 📊 Predicted class distribution\n",
    "print(\"\\n📊 Predicted Classes (85th Percentile):\")\n",
    "print(prediction_df_85pct[\"pred_class_85pct\"].value_counts())\n",
    "\n",
    "# 🎯 Actual class distribution\n",
    "print(\"\\n🎯 Actual Outcomes (hit_perf_percentile):\")\n",
    "print(prediction_df_85pct[\"hit_perf_percentile\"].value_counts())\n",
    "\n",
    "# 📈 Probability distribution\n",
    "print(\"\\n📈 Probability Distribution:\")\n",
    "print(prediction_df_85pct[\"pred_prob_85pct\"].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a70353-758a-4a29-ab7a-b630a2a9285a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7349e7-ec00-48c9-b756-4ce8460f8d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to merge the prediction dataframes\n",
    "# adjust column order\n",
    "final_column_order = (\n",
    "    baseline_cols +                        # Core metadata\n",
    "    [\"hit_value_dk\", \"hit_perf_fixed\", \"hit_perf_percentile\"] +  # Backtest-only truth values\n",
    "    [\"pred_prob_value\", \"pred_class_value\",\n",
    "     \"pred_prob_fpts10\", \"pred_class_fpts10\",\n",
    "     \"pred_prob_85pct\", \"pred_class_85pct\"] +                   # All model predictions\n",
    "    optional_display_cols                  # Contextual stats for dashboard/visuals\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097820e7-dcd9-47f6-a565-838f85157867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0762b5b-d649-41e3-9323-33e3ea6f20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** New Merged Prediction Dataframes and csv files *** \n",
    "# --- Merge all three predictions on identifying columns ---\n",
    "merge_keys = [\"season\", \"week\", \"name\", \"recent_team\"]\n",
    "\n",
    "merged_df = (\n",
    "    prediction_df_value\n",
    "    .merge(prediction_df_fpts10, on=merge_keys, suffixes=(\"\", \"_f10\"))\n",
    "    .merge(prediction_df_85pct, on=merge_keys, suffixes=(\"\", \"_p85\"))\n",
    ")\n",
    "\n",
    "# --- Rename final output for backtesting ---\n",
    "wr_logit_predictions_all_models_backtest = merged_df[\n",
    "    final_column_order  # defined earlier to control output structure\n",
    "].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cec46f-30c4-4576-a1e4-c6b1357542be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62085c6-6236-4190-bd87-b93f9e94cf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** New csv file of the merged prediction dataframes **\n",
    "\n",
    "\n",
    "# --- Calculate year range and export filename ---\n",
    "current_year = datetime.now().year\n",
    "current_week = get_current_week()\n",
    "years = get_year_range(current_year, current_week)\n",
    "\n",
    "csv_filename = f\"wr_logit_predictions_all_models_{years[0]}_{years[-1]}.csv\"\n",
    "\n",
    "# --- Export to CSV ---\n",
    "wr_logit_predictions_all_models_backtest.to_csv(csv_filename, index=False)\n",
    "print(f\"✅ Backtest prediction CSV saved: {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb66d1a-326b-49e9-b9f6-6550d4b6097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: Prediction Dataframes ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8b245-29c7-4468-84ee-72ce83b2a45e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40da1712-7974-4192-bd17-f28251d215cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin: Simulate Monte Carlo Trials (Basic Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c52db4a-aa01-462c-9545-3b488d54e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Monte Carlo simulations for player-week predictions.\n",
    "def run_monte_carlo_simulation(df, player_col=\"name\", season_col=\"season\", week_col=\"week\",\n",
    "                                prob_col=\"pred_prob_value\", n_simulations=1000):\n",
    "    results = []\n",
    "\n",
    "    for (player, season), group in df.groupby([player_col, season_col]):\n",
    "        probs = group.sort_values(week_col)[prob_col].values\n",
    "        weeks = group.sort_values(week_col)[week_col].values\n",
    "\n",
    "        # Bernoulli trials: n_simulations x n_weeks\n",
    "        sim_matrix = np.random.rand(n_simulations, len(probs)) < probs\n",
    "        total_hits = sim_matrix.sum(axis=1)\n",
    "\n",
    "        results.append({\n",
    "            \"player\": player,\n",
    "            \"season\": season,\n",
    "            \"weeks_played\": len(weeks),\n",
    "            \"avg_hits\": total_hits.mean(),\n",
    "            \"min_hits\": total_hits.min(),\n",
    "            \"max_hits\": total_hits.max(),\n",
    "            \"std_hits\": total_hits.std(),\n",
    "            \"p_hit_all_weeks\": np.mean(total_hits == len(probs)),\n",
    "            \"p_hit_half_or_more\": np.mean(total_hits >= (len(probs) // 2)),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecec990-091f-4198-9e4b-18d7b70cebea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c37878-3ce3-47e7-8f2a-47353595c394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo for hit value\n",
    "monte_carlo_results_value = run_monte_carlo_simulation(prediction_df_value)\n",
    "print(f\"✅ Monte Carlo complete for {len(monte_carlo_results_value)} players\")\n",
    "display(monte_carlo_results_value.head())\n",
    "monte_carlo_results_value.to_csv(\"wr_monte_carlo_value.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1dfe9a-75f6-4aea-8e9f-5eb0f929cb07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6108555e-aaad-42e8-bada-06f7823cc456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo for hit_perf_fpts\n",
    "monte_carlo_results_fpts10 = run_monte_carlo_simulation(\n",
    "    prediction_df_fpts10,\n",
    "    prob_col=\"pred_prob_fpts10\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Monte Carlo complete for FPTS10: {len(monte_carlo_results_fpts10)} players\")\n",
    "display(monte_carlo_results_fpts10.head())\n",
    "\n",
    "# Optional export\n",
    "monte_carlo_results_fpts10.to_csv(\"wr_monte_carlo_fpts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e62e5f-549b-46e6-81f6-62a4937519fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53f714a-62e8-4363-b4b0-cec81bfe44be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo for hit_perf_percentile\n",
    "monte_carlo_results_85pct = run_monte_carlo_simulation(\n",
    "    prediction_df_85pct,\n",
    "    prob_col=\"pred_prob_85pct\"\n",
    ")\n",
    "\n",
    "print(f\"✅ Monte Carlo complete for 85th Percentile: {len(monte_carlo_results_85pct)} players\")\n",
    "display(monte_carlo_results_85pct.head())\n",
    "\n",
    "# Optional export\n",
    "monte_carlo_results_85pct.to_csv(\"wr_monte_carlo_pct.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811b00b3-7800-4f2b-8567-8ff76827c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique player seasons for monte carlo simulations\n",
    "\n",
    "# For validation only — avoid relying on UI load section\n",
    "logit_df = pd.read_csv(\"wr_logit_predictions_all_models_2017_2024.csv\")\n",
    "n_player_seasons = logit_df[['name', 'season']].drop_duplicates()\n",
    "\n",
    "# output should match\n",
    "mc_value_df = pd.read_csv(\"wr_monte_carlo_value.csv\")\n",
    "\n",
    "print(f\"🔍 Total unique player-seasons in predictions: {len(n_player_seasons)}\")\n",
    "print(f\"📉 Rows in monte_carlo_results_value: {len(mc_value_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff3360-2360-4e52-9c46-891f8c5129dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End: Simulate Monte Carlo Trials (Basic Version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f864455-75ad-409c-85a1-31f02b550119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e177b90-80d0-48a8-baf6-103816c6cfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin: User Interface ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a02123-34d1-4fcf-bcc0-e66b0c955e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fuzzywuzzy[speedup]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5349d-e4dd-4b71-a3b7-6829a76ec725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90237e55-d4dc-4313-a953-a5d79cfda936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import xlsxwriter\n",
    "from fpdf import FPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69b923-a706-4cef-b4b0-b692ce84d998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b6a83-30ad-430f-a1ee-cf1f0cd9a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load logistic regression results\n",
    "logit_df = pd.read_csv(\"wr_logit_predictions_all_models_2017_2024.csv\")\n",
    "\n",
    "mc_dict = {\n",
    "    \"monte_carlo_value\": pd.read_csv(\"wr_monte_carlo_value.csv\").rename(columns={\"player\": \"name\"}),\n",
    "    \"monte_carlo_fpts10\": pd.read_csv(\"wr_monte_carlo_fpts.csv\").rename(columns={\"player\": \"name\"}),\n",
    "    \"monte_carlo_85pct\": pd.read_csv(\"wr_monte_carlo_pct.csv\").rename(columns={\"player\": \"name\"})\n",
    "}\n",
    "\n",
    "\n",
    "# # Load Monte Carlo data into dictionary (you already have this)\n",
    "# mc_dict = {\n",
    "#     \"monte_carlo_value\": pd.read_csv(\"wr_monte_carlo_value.csv\"),\n",
    "#     \"monte_carlo_fpts10\": pd.read_csv(\"wr_monte_carlo_fpts.csv\"),\n",
    "#     \"monte_carlo_85pct\": pd.read_csv(\"wr_monte_carlo_pct.csv\")\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff23e6-5369-45a6-84e9-f2e18affb19d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185598e-f1f9-4355-80ee-b4817ee04c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_groups = {\n",
    "    \"fpts\": [\"fpts\", \"value_ratio_dk\", \"value_ratio_fd\"],\n",
    "    \"touches_athleticism\": [\"targets\", \"receptions\", \"receiving_yards\", \"receiving_yards_after_catch\"],\n",
    "    \"efficiency\": [\"catch_percentage\", \"target_share\", \"receiving_air_yards\"],\n",
    "    \"separation\": [\"avg_cushion\", \"avg_separation\"],\n",
    "    \"zscore_fpts\": [\"fpts_zscore\", \"value_ratio_dk_zscore\", \"value_ratio_fd_zscore\"],\n",
    "    \"rolling_avgs\": [\"fpts_3wk_avg\", \"receptions_3wk_avg\", \"targets_3wk_avg\"],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8005b-3abc-4f60-8554-e0040c6309fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550d604-b598-494e-9115-3aa26a832bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction_lookup_ui_menu(logit_df, mc_dict):\n",
    "    \"\"\"\n",
    "    Unified interface to look up player-season predictions with fuzzy name matching,\n",
    "    and export options to Excel or PDF.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Logit + Monte Carlo Prediction Lookup ===\")\n",
    "    print(\"Type 'exit' at any prompt to quit.\\n\")\n",
    "\n",
    "    while True:\n",
    "        name_input = input(\"Enter player name (e.g., A.J. Brown): \").strip().lower()\n",
    "        if name_input == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Fuzzy match\n",
    "        all_names = logit_df['name'].dropna().unique()\n",
    "        best_match, score = process.extractOne(name_input, all_names)\n",
    "        if score < 80:\n",
    "            print(f\"❌ No good match found. Closest was '{best_match}' (score: {score}). Try again.\\n\")\n",
    "            continue\n",
    "        matched_name = best_match\n",
    "        print(f\"🔍 Best match: {matched_name} (score: {score})\")\n",
    "\n",
    "        season_input = input(\"Enter season (e.g., 2024): \").strip()\n",
    "        if season_input == \"exit\":\n",
    "            break\n",
    "        if not season_input.isdigit():\n",
    "            print(\"⚠️ Invalid season. Try again.\\n\")\n",
    "            continue\n",
    "        season_input = int(season_input)\n",
    "\n",
    "        mode_input = input(\"Select mode ('logit', 'mc', or 'all'): \").strip().lower()\n",
    "        if mode_input == \"exit\":\n",
    "            break\n",
    "        if mode_input not in [\"logit\", \"mc\", \"all\"]:\n",
    "            print(\"⚠️ Invalid mode. Choose from 'logit', 'mc', or 'all'.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Filter logit data\n",
    "        player_logit = logit_df[\n",
    "            (logit_df['name'] == matched_name) &\n",
    "            (logit_df['season'] == season_input)\n",
    "        ]\n",
    "\n",
    "        # Filter Monte Carlo data\n",
    "        player_mc = {}\n",
    "        for key, df in mc_dict.items():\n",
    "            match = df[\n",
    "                (df['name'] == matched_name) &\n",
    "                (df['season'] == season_input)\n",
    "            ]\n",
    "            player_mc[key] = match\n",
    "\n",
    "        # No data found\n",
    "        if player_logit.empty and all(df.empty for df in player_mc.values()):\n",
    "            print(\"❌ No data found for that player and season.\\n\")\n",
    "            continue\n",
    "\n",
    "        # Display logit\n",
    "        if mode_input in [\"logit\", \"all\"] and not player_logit.empty:\n",
    "            print(\"\\n--- Logistic Regression Prediction ---\")\n",
    "            display(player_logit)\n",
    "\n",
    "        # Display Monte Carlo\n",
    "        if mode_input in [\"mc\", \"all\"]:\n",
    "            print(\"\\n--- Monte Carlo Forecasts ---\")\n",
    "            for key, df in player_mc.items():\n",
    "                if not df.empty:\n",
    "                    print(f\"\\n📊 {key.replace('_', ' ').title()}\")\n",
    "                    display(df)\n",
    "                else:\n",
    "                    print(f\"⚠️ No data in {key} for this player-season.\")\n",
    "\n",
    "        # Export options\n",
    "        export = input(\"Export results? (excel/pdf/none): \").strip().lower()\n",
    "        if export == \"exit\":\n",
    "            break\n",
    "\n",
    "        # Excel Export\n",
    "        if export == \"excel\":\n",
    "            filename = f\"{matched_name.replace(' ', '_')}_{season_input}_predictions.xlsx\"\n",
    "            writer = pd.ExcelWriter(filename, engine=\"xlsxwriter\")\n",
    "            if not player_logit.empty:\n",
    "                player_logit.to_excel(writer, sheet_name=\"Logit\", index=False)\n",
    "            for key, df in player_mc.items():\n",
    "                if not df.empty:\n",
    "                    sheet = key[:31]  # Excel sheet name limit\n",
    "                    df.to_excel(writer, sheet_name=sheet, index=False)\n",
    "            writer.close()\n",
    "            print(f\"✅ Exported to Excel: {filename}\")\n",
    "\n",
    "        # PDF Export\n",
    "        elif export == \"pdf\":\n",
    "            filename = f\"{matched_name.replace(' ', '_')}_{season_input}_predictions.pdf\"\n",
    "            pdf = FPDF()\n",
    "            pdf.add_page()\n",
    "            pdf.set_font(\"Arial\", size=12)\n",
    "            pdf.cell(200, 10, txt=f\"{matched_name} - {season_input} Predictions\", ln=True)\n",
    "\n",
    "            if not player_logit.empty:\n",
    "                pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                pdf.cell(200, 10, txt=\"--- Logistic Regression ---\", ln=True)\n",
    "                pdf.set_font(\"Arial\", size=11)\n",
    "                for col in player_logit.columns:\n",
    "                    val = str(player_logit.iloc[0][col])\n",
    "                    pdf.cell(200, 8, txt=f\"{col}: {val}\", ln=True)\n",
    "\n",
    "            for key, df in player_mc.items():\n",
    "                if not df.empty:\n",
    "                    pdf.set_font(\"Arial\", \"B\", 12)\n",
    "                    pdf.cell(200, 10, txt=f\"--- {key.replace('_', ' ').title()} ---\", ln=True)\n",
    "                    pdf.set_font(\"Arial\", size=11)\n",
    "                    for col in df.columns:\n",
    "                        val = str(df.iloc[0][col])\n",
    "                        pdf.cell(200, 8, txt=f\"{col}: {val}\", ln=True)\n",
    "\n",
    "            pdf.output(filename)\n",
    "            print(f\"✅ Exported to PDF: {filename}\")\n",
    "\n",
    "        elif export not in [\"none\", \"\"]:\n",
    "            print(\"⚠️ Invalid export option. Skipped export.\")\n",
    "\n",
    "        print(\"\\n✓ Lookup complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b37bc2-f715-47ec-9f5d-b29f58bc35a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666b85be-4cdb-4f78-b946-a1a53d0838e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_multi_player_visuals(df=wr_df, column_groups=column_groups):\n",
    "    \"\"\"\n",
    "    Compare up to 3 players across a selected stat category.\n",
    "    Supports z-score normalization and works across seasons.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Multi-Player Comparison ===\")\n",
    "    \n",
    "    players = []\n",
    "    for i in range(3):\n",
    "        name = input(f\"Enter player {i+1} name (or press Enter to skip): \").strip()\n",
    "        if name:\n",
    "            players.append(name)\n",
    "    \n",
    "    if not players:\n",
    "        print(\"❌ No players entered.\")\n",
    "        return\n",
    "\n",
    "    season_input = input(\"Enter season (e.g., 2023): \").strip()\n",
    "    if not season_input.isdigit():\n",
    "        print(\"⚠️ Invalid season.\")\n",
    "        return\n",
    "    season_input = int(season_input)\n",
    "\n",
    "    stat_group = input(\"Enter stat group (e.g., fpts, touches_athleticism): \").strip()\n",
    "    if stat_group not in column_groups:\n",
    "        print(\"⚠️ Invalid stat group.\")\n",
    "        return\n",
    "\n",
    "    use_zscore = input(\"Use z-score mode? (yes/no): \").strip().lower() == \"yes\"\n",
    "    stat_cols = column_groups[stat_group]\n",
    "\n",
    "    # Set up plots\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(len(stat_cols), len(players), figsize=(5 * len(players), 4 * len(stat_cols)))\n",
    "    if len(stat_cols) == 1:\n",
    "        axes = [axes]\n",
    "    if len(players) == 1:\n",
    "        axes = [[ax] for ax in axes]\n",
    "\n",
    "    for col_idx, stat in enumerate(stat_cols):\n",
    "        for player_idx, player in enumerate(players):\n",
    "            df_player = df[\n",
    "                (df['name'].str.lower() == player.lower()) &\n",
    "                (df['season'] == season_input)\n",
    "            ]\n",
    "            if df_player.empty:\n",
    "                axes[col_idx][player_idx].set_title(f\"{player} — No Data\")\n",
    "                axes[col_idx][player_idx].axis(\"off\")\n",
    "                continue\n",
    "\n",
    "            plot_data = df_player.copy()\n",
    "            if use_zscore:\n",
    "                mean = plot_data[stat].mean()\n",
    "                std = plot_data[stat].std()\n",
    "                plot_data[stat] = (plot_data[stat] - mean) / std\n",
    "\n",
    "            axes[col_idx][player_idx].plot(plot_data['week'], plot_data[stat], marker='o')\n",
    "            axes[col_idx][player_idx].set_title(f\"{player} — {stat}\")\n",
    "            axes[col_idx][player_idx].set_xlabel(\"Week\")\n",
    "            axes[col_idx][player_idx].set_ylabel(\"Z-Score\" if use_zscore else stat)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecd9859-8764-412a-a00e-b63f3d5118c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77855213-0986-4fb0-a23e-fcbce0d5431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_player_visuals(df=wr_df, column_groups=column_groups):\n",
    "    \"\"\"\n",
    "    Show weekly trendlines for a single player across a stat group.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=== Single Player Trendline Visualization ===\")\n",
    "    \n",
    "    name_input = input(\"Enter player name: \").strip().lower()\n",
    "    season_input = input(\"Enter season (e.g., 2024): \").strip()\n",
    "    stat_group = input(\"Enter stat group (e.g., fpts, touches_athleticism): \").strip()\n",
    "    use_zscore = input(\"Use z-score mode? (yes/no): \").strip().lower() == \"yes\"\n",
    "\n",
    "    if stat_group not in column_groups:\n",
    "        print(\"❌ Invalid stat group.\")\n",
    "        return\n",
    "\n",
    "    season_input = int(season_input)\n",
    "    stat_cols = column_groups[stat_group]\n",
    "\n",
    "    player_df = df[\n",
    "        (df['name'].str.lower() == name_input) &\n",
    "        (df['season'] == season_input)\n",
    "    ]\n",
    "\n",
    "    if player_df.empty:\n",
    "        print(\"❌ No data found for that player/season.\")\n",
    "        return\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, axes = plt.subplots(len(stat_cols), 1, figsize=(8, 4 * len(stat_cols)))\n",
    "\n",
    "    if len(stat_cols) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for idx, col in enumerate(stat_cols):\n",
    "        y_data = player_df[col]\n",
    "        if use_zscore:\n",
    "            y_data = (y_data - y_data.mean()) / y_data.std()\n",
    "\n",
    "        axes[idx].plot(player_df['week'], y_data, marker='o')\n",
    "        axes[idx].set_title(f\"{name_input.title()} — {col} ({'Z-Score' if use_zscore else 'Raw'})\")\n",
    "        axes[idx].set_xlabel(\"Week\")\n",
    "        axes[idx].set_ylabel(col)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091c9b6e-e0b6-4811-ab9f-9c0fa7639b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df57f6c2-7c24-4fe6-a0a7-555e35de3b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_player_dashboard_summary(logit_df, mc_dict):\n",
    "    \"\"\"\n",
    "    Smart interactive dashboard tool for in-season or off-season.\n",
    "    Uses current year/week defaults unless overridden.\n",
    "    \"\"\"\n",
    "    from fuzzywuzzy import process\n",
    "    from datetime import datetime\n",
    "\n",
    "    print(\"\\n=== Player Performance Dashboard ===\")\n",
    "    print(\"Type 'exit' at any prompt to cancel.\\n\")\n",
    "\n",
    "    # --- Set current context ---\n",
    "    current_year = datetime.now().year\n",
    "    current_week = get_current_week()\n",
    "    years = get_year_range(current_year, current_week)\n",
    "    \n",
    "    # Smart season + week defaults\n",
    "    if 1 <= current_week <= 18:\n",
    "        default_season = years[-1]\n",
    "        default_week = current_week\n",
    "    else:\n",
    "        # If offseason, use most recent complete season\n",
    "        default_season = years[-1]  # not years[-2]\n",
    "        default_week = 18\n",
    "\n",
    "\n",
    "    # --- Player Input ---\n",
    "    player_input = input(\"Enter player name: \").strip().lower()\n",
    "    if player_input == \"exit\":\n",
    "        return\n",
    "\n",
    "    all_names = logit_df['name'].dropna().unique()\n",
    "    best_match, score = process.extractOne(player_input, all_names)\n",
    "    if score < 80:\n",
    "        print(f\"❌ No good match found. Closest was '{best_match}' (score: {score})\")\n",
    "        return\n",
    "    matched_name = best_match\n",
    "    print(f\"🔍 Best match: {matched_name} (score: {score})\")\n",
    "\n",
    "    # --- Season Input ---\n",
    "    season_input = input(f\"Enter season (default: {default_season}): \").strip()\n",
    "    if season_input == \"exit\":\n",
    "        return\n",
    "    season = int(season_input) if season_input.isdigit() else default_season\n",
    "\n",
    "    # --- Week Input ---\n",
    "    week_input = input(f\"Enter week (default: {default_week}): \").strip()\n",
    "    if week_input == \"exit\":\n",
    "        return\n",
    "    week = int(week_input) if week_input.isdigit() else default_week\n",
    "\n",
    "    # --- Lookup Logit Row ---\n",
    "    logit_row = logit_df[\n",
    "        (logit_df['name'] == matched_name) &\n",
    "        (logit_df['season'] == season) &\n",
    "        (logit_df['week'] == week)\n",
    "    ]\n",
    "\n",
    "    if logit_row.empty:\n",
    "        print(\"❌ No logistic prediction found for that player and week.\")\n",
    "        return\n",
    "    logit_row = logit_row.iloc[0]\n",
    "\n",
    "    # --- Get Monte Carlo Rows ---\n",
    "    mc_summary = {}\n",
    "    for key, df in mc_dict.items():\n",
    "        df_player = df[\n",
    "            (df['name'].str.lower() == matched_name.lower()) &\n",
    "            (df['season'] == season)\n",
    "        ]\n",
    "        mc_summary[key] = df_player.iloc[0] if not df_player.empty else None\n",
    "\n",
    "    # --- Display Summary ---\n",
    "    print(\"\\n📊 === Player Dashboard Summary ===\")\n",
    "    print(f\"Player: {matched_name} | Week: {week} | Season: {season}\")\n",
    "    print(f\"DK Salary: ${int(logit_row['dk_salary'])}\")\n",
    "    print(f\"Actual FPTS (Week {week}): {logit_row.get('fpts', '—')}\")\n",
    "\n",
    "    print(\"\\n🔢 Logistic Regression Predictions:\")\n",
    "    def classify(prob, threshold=0.5):\n",
    "        if prob >= 0.85: return f\"{prob:.2f} 🔵 High\"\n",
    "        if prob >= threshold: return f\"{prob:.2f} 🟡 Medium\"\n",
    "        return f\"{prob:.2f} 🔴 Low\"\n",
    "\n",
    "    print(f\" - P(Hit DK Value):     {classify(logit_row['pred_prob_value'])} — Predicted: {'✅' if logit_row['pred_class_value'] else '❌'}\")\n",
    "    print(f\" - P(FPTS ≥ 10):        {classify(logit_row['pred_prob_fpts10'])} — Predicted: {'✅' if logit_row['pred_class_fpts10'] else '❌'}\")\n",
    "    print(f\" - P(Elite Tier 85%):   {classify(logit_row['pred_prob_85pct'])} — Predicted: {'✅' if logit_row['pred_class_85pct'] else '❌'}\")\n",
    "\n",
    "    print(\"\\n🔮 Monte Carlo Forecasts (Rest of Season):\")\n",
    "    for key in mc_dict.keys():\n",
    "        mc_row = mc_summary[key]\n",
    "        label = key.replace(\"monte_carlo_\", \"\").upper()\n",
    "        if mc_row is not None:\n",
    "            print(f\"\\n--- {label} ---\")\n",
    "            print(f\" Avg Hits:         {mc_row['avg_hits']:.2f}\")\n",
    "            print(f\" Min–Max Hits:     {mc_row['min_hits']} – {mc_row['max_hits']}\")\n",
    "            print(f\" Std Dev:          {mc_row['std_hits']:.2f}\")\n",
    "            print(f\" P(Hit All Weeks): {mc_row['p_hit_all_weeks']:.2f}\")\n",
    "            print(f\" P(Hit ≥ Half):    {mc_row['p_hit_half_or_more']:.2f}\")\n",
    "        else:\n",
    "            print(f\"\\n--- {label} ---\\nNo Monte Carlo data available.\")\n",
    "\n",
    "    print(\"\\n✅ Summary Complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedbc8c-59d8-440f-8510-9b3cc86ee13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a3d774-c3fb-400b-9233-777a513a1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_main_nfl_model_tools_menu():\n",
    "    \"\"\"\n",
    "    Interactive menu to run NFL model tools from one interface.\n",
    "    \"\"\"\n",
    "\n",
    "    while True:\n",
    "        print(\"\\n=== Main NFL Model Tools Menu ===\")\n",
    "        print(\"1. 🧠 Player Performance Dashboard\")\n",
    "        print(\"2. 🔍 Lookup Predictions (Logit + Monte Carlo)\")\n",
    "        print(\"3. 📈 Single Player Trendline Visualization\")\n",
    "        print(\"4. 👥 Multi-Player Comparison\")\n",
    "        print(\"5. 📊 Season Summary Reports\")\n",
    "        print(\"0. ❌ Exit\")\n",
    "\n",
    "        choice = input(\"Enter your choice: \").strip()\n",
    "\n",
    "        if choice == \"1\":\n",
    "            run_player_dashboard_summary(logit_df, mc_dict)\n",
    "        elif choice == \"2\":\n",
    "            run_prediction_lookup_ui_menu(logit_df, mc_dict)\n",
    "        elif choice == \"3\":\n",
    "            run_single_player_visuals()\n",
    "        elif choice == \"4\":\n",
    "            run_multi_player_visuals()\n",
    "        elif choice == \"5\":\n",
    "            run_season_visuals()\n",
    "        elif choice == \"0\":\n",
    "            print(\"👋 Exiting. See you next time!\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"⚠️ Invalid option. Please enter a number from 0 to 5.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f3c43-fbca-4791-a9b6-4b2681523a84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37ae620-8f6f-4b49-90c1-656b824d3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ** USER INPUT ** ###\n",
    "run_main_nfl_model_tools_menu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f0c5a-5508-4ebb-9af2-b4a67fa93ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### End: User Interface ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d19608-2ac8-4306-a4c9-d45b7fc50532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
