{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82d62d0a-e54f-4cbe-b11b-33a6786328e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulls from the rotowire\n",
    "## *** Important: The NFL start week and the API start week are different; run this on Wednesdays\n",
    "## *** e.g. if you run this on Tuesday it will use the previous week and return an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa98ceeb-37e9-4b8b-a03c-298afba30563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "import nfl_data_py as nfl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784593c2-53f5-4a84-bf70-895a00d4789e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8f9193a-e0e9-4539-8c3d-69726fe033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this imports all modified columns correction and in order \n",
    "\n",
    "# define the template URL for API fetching\n",
    "rotowire_template_url = \"https://www.rotowire.com/betting/nfl/tables/nfl-games.php?week={week}\"\n",
    "\n",
    "# define the columns\n",
    "columns_to_keep = ['gameID', 'year', 'week', 'gameDate', 'gameDateTime', 'homeAway', 'nickname', 'abbr',\n",
    "                   'oppNickname', 'draftkings_moneyline', 'draftkings_spread', 'draftkings_ou', 'draftkings_score', \n",
    "                   'draftkings_oppScore', 'fanduel_moneyline', 'fanduel_spread', 'fanduel_ou', 'fanduel_score', 'fanduel_oppScore',\n",
    "                   'betrivers_moneyline', 'betrivers_spread', 'betrivers_ou', 'betrivers_score', 'betrivers_oppScore',\n",
    "                   'mgm_moneyline', 'mgm_spread', 'mgm_ou', 'mgm_score', 'mgm_oppScore'\n",
    "                  ]\n",
    "\n",
    "# fetch JSON data from API and filter columns\n",
    "def import_rotowire_data(week):\n",
    "    # Format the URL with the given week\n",
    "    url = rotowire_template_url.format(week=week)\n",
    "    \n",
    "    # Fetch the JSON data from the API\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        # Convert the JSON data to a Python object\n",
    "        json_data = response.json()\n",
    "        \n",
    "        if not json_data:\n",
    "            return None\n",
    "        \n",
    "        # Convert the JSON data to a DataFrame\n",
    "        df = pd.DataFrame(json_data)\n",
    "        \n",
    "        # Add the 'week' column manually\n",
    "        df['week'] = week\n",
    "        \n",
    "        # Add the 'year' column derived from the 'gameDate' column\n",
    "        df['gameDate'] = pd.to_datetime(df['gameDate'], errors='coerce')  # Convert to datetime\n",
    "        df['year'] = df['gameDate'].dt.year\n",
    "        \n",
    "        # Drop all columns that are not in 'columns_to_keep'\n",
    "        filtered_df = df[columns_to_keep]\n",
    "\n",
    "        return filtered_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# load missing weeks from local JSON files\n",
    "def load_json_files():\n",
    "    # List to hold data for missing weeks\n",
    "    missing_weeks_data = []\n",
    "\n",
    "    # Find all JSON files that follow the pattern for week files\n",
    "    json_files = glob.glob(\"rotowire_odds_lines_week_*.json\")\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        # Extract the week number from the file name\n",
    "        week_number = int(json_file.split('_')[-1].replace('.json', ''))\n",
    "        \n",
    "        # Read the JSON file\n",
    "        with open(json_file, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "        \n",
    "        # Convert the JSON data to a DataFrame\n",
    "        df = pd.DataFrame(json_data)\n",
    "        \n",
    "        # Add the week number and derive the year from the gameDate\n",
    "        df['week'] = week_number\n",
    "        df['gameDate'] = pd.to_datetime(df['gameDate'], errors='coerce')\n",
    "        df['year'] = df['gameDate'].dt.year\n",
    "        \n",
    "        # Drop all columns that are not in 'columns_to_keep'\n",
    "        filtered_df = df[columns_to_keep]\n",
    "        missing_weeks_data.append(filtered_df)\n",
    "    \n",
    "    return missing_weeks_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b35066a-fd94-441f-b443-72d97c594b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch team totals and O/U data for multiple weeks (imports json files)\n",
    "def teamTotals_overUnder():\n",
    "    all_weeks_data = []\n",
    "\n",
    "    # fetch data for each week (you can adjust the range as needed)\n",
    "    for week in range(1, 18):  # adjust range based on the weeks you want to fetch\n",
    "        df = import_rotowire_data(week)\n",
    "        if df is not None:\n",
    "            all_weeks_data.append(df)\n",
    "\n",
    "    # load missing weeks from local JSON files and append them\n",
    "    missing_weeks_data = load_json_files()\n",
    "    all_weeks_data.extend(missing_weeks_data)\n",
    "\n",
    "    # concatenate all non-empty DataFrames\n",
    "    if all_weeks_data:\n",
    "        full_season_data = pd.concat(all_weeks_data, ignore_index=True)\n",
    "\n",
    "        # sort by 'year' (descending) and 'week' (ascending)\n",
    "        full_season_data = full_season_data.sort_values(by=['year', 'week'], ascending=[False, True])\n",
    "\n",
    "        # display the sorted DataFrame\n",
    "        # display(full_season_data)\n",
    "\n",
    "        # save the sorted DataFrame to a CSV file\n",
    "        # csv_filename = \"teamTotals_overUnder.csv\"\n",
    "        # full_season_data.to_csv(csv_filename, index=False)\n",
    "        # print(f\"Data successfully written to {csv_filename}\")\n",
    "    else:\n",
    "        print(\"No data fetched for any week.\")\n",
    "\n",
    "# Call the function to execute the workflow\n",
    "# teamTotals_overUnder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7b6617-2ce1-4187-b6e1-9ad0d1c9061f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a68af8f-b088-498e-8b3d-5e89e31cf19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch team totals and O/U data for the current week only\n",
    "# *** IMPORTANT***: update the NFL season start date for each new season \n",
    "def get_current_week():\n",
    "    \n",
    "    # Logic to compute the current NFL week\n",
    "    current_date = datetime.now()\n",
    "    \n",
    "    # Define the start date of Week 1 in the NFL season (adjust this as needed)\n",
    "    season_start_date = datetime(2024, 9, 4)\n",
    "    \n",
    "    # Calculate the week number based on the difference from the start date\n",
    "    current_week = ((current_date - season_start_date).days // 7) + 1\n",
    "    return current_week\n",
    "\n",
    "# **** IMPORTANT: the qb_and_team_betting_lines() functions needs the csv file to create the df\n",
    "## *** Important: The NFL start week and the API start week are different; run this on Wednesdays\n",
    "## *** e.g. if you run this on Tuesday it will use the previous week and return an error\n",
    "def teamTotals_overUnder_current_week():\n",
    "    current_week_data = []\n",
    "\n",
    "    # Define the current week (you can adjust this to get the actual current week dynamically)\n",
    "    current_week = get_current_week() \n",
    "\n",
    "    # Fetch data for the current week\n",
    "    df = import_rotowire_data(current_week)\n",
    "    if df is not None:\n",
    "        current_week_data.append(df)\n",
    "\n",
    "    # Concatenate all non-empty DataFrames\n",
    "    if current_week_data:\n",
    "        full_current_week_data = pd.concat(current_week_data, ignore_index=True)\n",
    "\n",
    "        # Sort by 'year' (descending) and 'week' (ascending)\n",
    "        full_current_week_data = full_current_week_data.sort_values(by=['year', 'week'], ascending=[False, True])\n",
    "\n",
    "        # Display the sorted DataFrame\n",
    "        # display(full_current_week_data)\n",
    "\n",
    "        # Save the sorted DataFrame to a CSV file   - DO NOT COMMENT OUT\n",
    "        csv_filename = \"teamTotals_overUnder_current_week.csv\"\n",
    "        full_current_week_data.to_csv(csv_filename, index=False)\n",
    "        print(f\"Data successfully written to {csv_filename}\")\n",
    "    else:\n",
    "        print(\"No data fetched for the current week.\")\n",
    "\n",
    "# Call the new function to fetch data for the current week\n",
    "# teamTotals_overUnder_current_week()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b176233-8dde-411d-9f3d-944504dc9e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "679ddce8-d4a1-442d-966a-f313dbb6bd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Function 1: Fetch QB Player Data from ESPN API\n",
    "def import_ESPN_qb_data(year, seasontype):\n",
    "    \n",
    "    # Define the ESPN API URL with parameters\n",
    "    espn_url = f\"https://site.web.api.espn.com/apis/common/v3/sports/football/nfl/statistics/byathlete?region=us&lang=en&contentorigin=espn&isqualified=false&page=1&limit=50&category=offense%3Apassing&sort=passing.passingYards%3Adesc&season={year}&seasontype={seasontype}\"\n",
    "\n",
    "    # Send the request\n",
    "    response = requests.get(espn_url)\n",
    "\n",
    "    # Check the response status and parse JSON data\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract player information, including shortName, team name, and team abbreviation\n",
    "        player_list = []\n",
    "        for athlete_data in data['athletes']:\n",
    "            athlete = athlete_data['athlete']\n",
    "            \n",
    "            # Extract necessary fields\n",
    "            player_name = athlete.get('displayName', 'N/A')\n",
    "            short_name = athlete.get('shortName', 'N/A')\n",
    "            player_id = athlete.get('id', 'N/A')\n",
    "            team_name = athlete.get('teamName', 'N/A')\n",
    "            team_abbreviation = athlete.get('teamShortName', 'N/A')\n",
    "            \n",
    "            # Adding team details\n",
    "            player_info = {\n",
    "                'name': player_name,\n",
    "                'shortName': short_name,\n",
    "                'id': player_id,\n",
    "                'teams': [\n",
    "                    {\n",
    "                        'name': team_name,\n",
    "                        'abbreviation': team_abbreviation\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            player_list.append(player_info)\n",
    "\n",
    "        return player_list\n",
    "    else:\n",
    "        print(f\"Error: Failed to fetch data, status code {response.status_code}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c8a8a5-54f7-40c2-9d28-0504beab2206",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a15fe6ef-5d66-4188-951e-279775313686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "# Function 2: Merge QB Data with Betting Lines Data\n",
    "def create_qb_betting_lines_df(player_list, csv_file_path):\n",
    " \n",
    "    # Convert player_list to a DataFrame\n",
    "    player_df = pd.DataFrame([{\n",
    "        'name': player['name'],\n",
    "        'shortName': player['shortName'],\n",
    "        'id': player['id'],\n",
    "        'team_name': player['teams'][0]['name'],\n",
    "        'team_abbreviation': player['teams'][0]['abbreviation']\n",
    "    } for player in player_list])\n",
    "\n",
    "    # Load the CSV data\n",
    "    betting_df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # Display the columns of the betting data\n",
    "    # print(\"Betting Data Columns:\\n\", betting_df.columns)\n",
    "\n",
    "    # Proceed with the merge if the data is valid\n",
    "    if not betting_df.empty:\n",
    "        merged_df = pd.merge(player_df, betting_df, how='left', \n",
    "                             left_on=['team_name', 'team_abbreviation'], \n",
    "                             right_on=['nickname', 'abbr'])\n",
    "\n",
    "        # Return the merged data\n",
    "        return merged_df\n",
    "    else:\n",
    "        print(\"Error: Betting data is empty.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee69aea-0931-41f2-80ce-ca398144a27b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15041f85-90b6-41f3-b3fd-e67b8a0423bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year: 2024, Season Type: 2, Current Week: 5\n",
      "teamTotals_overUnder_current_week.csv not found, fetching betting lines data...\n",
      "No data fetched for the current week.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'teamTotals_overUnder_current_week.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Call the main function to execute the workflow\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m qb_betting_lines_df \u001b[38;5;241m=\u001b[39m qb_and_team_betting_lines()\n",
      "Cell \u001b[0;32mIn[22], line 35\u001b[0m, in \u001b[0;36mqb_and_team_betting_lines\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Merge the player data with betting lines data\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qb_player_data:\n\u001b[0;32m---> 35\u001b[0m     qb_betting_lines_merged_data \u001b[38;5;241m=\u001b[39m create_qb_betting_lines_df(qb_player_data, csv_file_path)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Display the merged data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m qb_betting_lines_merged_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36mcreate_qb_betting_lines_df\u001b[0;34m(player_list, csv_file_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m player_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([{\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: player[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortName\u001b[39m\u001b[38;5;124m'\u001b[39m: player[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshortName\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteam_abbreviation\u001b[39m\u001b[38;5;124m'\u001b[39m: player[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mteams\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabbreviation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     12\u001b[0m } \u001b[38;5;28;01mfor\u001b[39;00m player \u001b[38;5;129;01min\u001b[39;00m player_list])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load the CSV data\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m betting_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_file_path)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Display the columns of the betting data\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# print(\"Betting Data Columns:\\n\", betting_df.columns)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Proceed with the merge if the data is valid\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m betting_df\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'teamTotals_overUnder_current_week.csv'"
     ]
    }
   ],
   "source": [
    "# **IMPORTANT: this function outputs the combined qb-betting lines df\n",
    "# **Use this function to make further modifications\n",
    "# Function 3: Main function to execute the full workflow\n",
    "def qb_and_team_betting_lines():\n",
    "    # Get the current NFL week\n",
    "    current_week = get_current_week()\n",
    "    \n",
    "    # Logic to determine the year and season type based on the current week\n",
    "    current_year = datetime.now().year\n",
    "    if current_week <= 18:\n",
    "        year = current_year\n",
    "        seasontype = 2  # Regular season\n",
    "    elif 18 <= current_week <= 21:\n",
    "        year = current_year\n",
    "        seasontype = 3  # Playoffs\n",
    "    else:\n",
    "        year = current_year - 1  # If week is after playoffs, default to last season\n",
    "        seasontype = 2  # Regular season\n",
    "\n",
    "    print(f\"Year: {year}, Season Type: {seasontype}, Current Week: {current_week}\")\n",
    "    \n",
    "    # Check to see if the .csv file is in the directory\n",
    "    csv_file_path = 'teamTotals_overUnder_current_week.csv'\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"{csv_file_path} found.\")\n",
    "    else:\n",
    "        print(f\"{csv_file_path} not found, fetching betting lines data...\")\n",
    "        teamTotals_overUnder_current_week()  # Fetch betting lines if CSV not found\n",
    "\n",
    "    # Fetch the player data\n",
    "    qb_player_data = import_ESPN_qb_data(year, seasontype)\n",
    "\n",
    "    # Merge the player data with betting lines data\n",
    "    if qb_player_data:\n",
    "        qb_betting_lines_merged_data = create_qb_betting_lines_df(qb_player_data, csv_file_path)\n",
    "\n",
    "        # Display the merged data\n",
    "        if qb_betting_lines_merged_data is not None:\n",
    "            print(\"Merged Data:\\n\")\n",
    "            \n",
    "            # Display the merged DataFrame\n",
    "            # display(qb_betting_lines_merged_data)  \n",
    "\n",
    "            # Save the merged DataFrame to a CSV file with the same name as the function\n",
    "            output_csv_file = 'qb_and_team_betting_lines.csv'\n",
    "            qb_betting_lines_merged_data.to_csv(output_csv_file, index=False)\n",
    "            print(f\"Data successfully written to {output_csv_file}\")\n",
    "\n",
    "        # Return the merged DataFrame for further use\n",
    "        return qb_betting_lines_merged_data  \n",
    "    else:\n",
    "        print(\"Error: No player data found.\")\n",
    "        return None\n",
    "\n",
    "# Call the main function to execute the workflow\n",
    "qb_betting_lines_df = qb_and_team_betting_lines()\n",
    "\n",
    "# Optional: Use qb_betting_lines_df for further processing if returned correctly\n",
    "# if qb_betting_lines_df is not None:\n",
    "#     print(\"Successfully merged QB data with betting lines and saved to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3169362-63f0-40fd-a133-2165a992ca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the qb and corresponding betting lines\n",
    "# Modifies the qb_betting_lines_df DataFrame by dropping, renaming, and reordering columns\n",
    "def modify_qb_betting_lines_df(qb_betting_lines_df):\n",
    "\n",
    "    # Step 1: Drop the specified columns\n",
    "    columns_to_drop = ['gameDateTime', 'shortName', 'team_abbreviation', 'nickname', 'id']\n",
    "    qb_betting_lines_df = qb_betting_lines_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "    # Step 2: Rename columns\n",
    "    qb_betting_lines_df = qb_betting_lines_df.rename(columns={\n",
    "        'oppNickname': 'opponent',\n",
    "        'team_name': 'team'\n",
    "    })\n",
    "\n",
    "    # Step 3: Check which columns exist for reordering\n",
    "    column_order = ['year', 'week', 'gameID', 'gameDate', 'name', 'id', 'team', 'abbr', 'homeAway']\n",
    "    existing_columns = [col for col in column_order if col in qb_betting_lines_df.columns]\n",
    "    remaining_columns = [col for col in qb_betting_lines_df.columns if col not in existing_columns]\n",
    "\n",
    "    # Apply the order with the existing columns\n",
    "    qb_betting_lines_df = qb_betting_lines_df[existing_columns + remaining_columns]\n",
    "\n",
    "    # Step 4: Display the modified DataFrame\n",
    "    print(\"Modified DataFrame:\\n\")\n",
    "    display(qb_betting_lines_df)\n",
    "\n",
    "    # Step 5: Save the modified DataFrame to a CSV file\n",
    "    output_csv_file = 'qb_betting_lines.csv'\n",
    "    qb_betting_lines_df.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Modified DataFrame successfully written to {output_csv_file}\")\n",
    "\n",
    "    return qb_betting_lines_df\n",
    "\n",
    "# Call the function to modify the qb_betting_lines_df\n",
    "modified_qb_betting_lines_df = modify_qb_betting_lines_df(qb_betting_lines_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dd535-534d-42a0-996f-352459b1de4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac18de0-9a07-4c16-a3f1-43d697b75cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qb_teamTotal_23pts_OU_45(modified_qb_betting_lines_df):\n",
    "    \"\"\"\n",
    "    Filters the modified DataFrame to return rows where:\n",
    "    - The average projected O/U is at least 45\n",
    "    - The projected team total is at least 23 points.\n",
    "    \n",
    "    It also adds new columns for average O/U and average team total, and returns \n",
    "    the DataFrame with the specified columns and order.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the average O/U for each team\n",
    "    ou_columns = ['draftkings_ou', 'fanduel_ou', 'betrivers_ou', 'mgm_ou']\n",
    "    modified_qb_betting_lines_df['avg_over_under'] = modified_qb_betting_lines_df[ou_columns].mean(axis=1)\n",
    "\n",
    "    # Step 2: Compute the average team total for each team\n",
    "    score_columns = ['draftkings_score', 'fanduel_score', 'betrivers_score', 'mgm_score']\n",
    "    modified_qb_betting_lines_df['avg_team_total'] = modified_qb_betting_lines_df[score_columns].mean(axis=1)\n",
    "\n",
    "    # Step 3: Filter rows where avg O/U is at least 45 and avg team total is at least 23\n",
    "    filtered_df = modified_qb_betting_lines_df[\n",
    "        (modified_qb_betting_lines_df['avg_over_under'] >= 45) &\n",
    "        (modified_qb_betting_lines_df['avg_team_total'] >= 23)\n",
    "    ]\n",
    "\n",
    "    # Step 4: Reorder columns\n",
    "    column_order = ['year', 'week', 'gameID', 'gameDate', 'name', 'team', 'abbr', 'homeAway', 'opponent', 'avg_over_under', 'avg_team_total']\n",
    "    filtered_df = filtered_df[column_order]\n",
    "\n",
    "    # Display the final filtered DataFrame\n",
    "    print(\"Filtered DataFrame with avg O/U >= 45 and avg team total >= 23:\\n\")\n",
    "    display(filtered_df)\n",
    "\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    output_csv_file = 'qb_teamTotal_23pts_OU_45.csv'\n",
    "    filtered_df.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Filtered DataFrame successfully written to {output_csv_file}\")\n",
    "\n",
    "    return filtered_df\n",
    "    \n",
    "# Call the new function on the modified DataFrame\n",
    "qb_team_total_df = qb_teamTotal_23pts_OU_45(modified_qb_betting_lines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a7a49-24e5-4d5a-93b5-bf6038b1cad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23363ede-87e3-4b17-97df-fccc05a8271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qb_teamTotal_23pts_OU_45_7pt_favorite(modified_qb_betting_lines_df):\n",
    "    \"\"\"\n",
    "    Filters the modified DataFrame to return rows where:\n",
    "    - The average projected O/U is at least 45\n",
    "    - The projected team total is at least 23 points\n",
    "    - The team is favored by at least 7 points (spread <= -7).\n",
    "    \n",
    "    It also adds new columns for average O/U, average team total, and average spread,\n",
    "    and returns the DataFrame with the specified columns and order.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Compute the average O/U for each team\n",
    "    ou_columns = ['draftkings_ou', 'fanduel_ou', 'betrivers_ou', 'mgm_ou']\n",
    "    modified_qb_betting_lines_df['avg_over_under'] = modified_qb_betting_lines_df[ou_columns].mean(axis=1)\n",
    "\n",
    "    # Step 2: Compute the average team total for each team\n",
    "    score_columns = ['draftkings_score', 'fanduel_score', 'betrivers_score', 'mgm_score']\n",
    "    modified_qb_betting_lines_df['avg_team_total'] = modified_qb_betting_lines_df[score_columns].mean(axis=1)\n",
    "\n",
    "    # Step 3: Compute the average projected spread for each team\n",
    "    spread_columns = ['draftkings_spread', 'fanduel_spread', 'betrivers_spread', 'mgm_spread']\n",
    "    modified_qb_betting_lines_df['average_spread'] = modified_qb_betting_lines_df[spread_columns].mean(axis=1)\n",
    "\n",
    "    # Step 4: Filter rows based on conditions\n",
    "    filtered_df = modified_qb_betting_lines_df[\n",
    "        (modified_qb_betting_lines_df['avg_over_under'] >= 45) &  # avg O/U at least 45\n",
    "        (modified_qb_betting_lines_df['avg_team_total'] >= 23) &  # avg team total at least 23\n",
    "        (modified_qb_betting_lines_df['average_spread'] <= -7)    # avg spread <= -7 (favored by at least 7 points)\n",
    "    ]\n",
    "\n",
    "    # Step 5: Reorder columns\n",
    "    column_order = ['year', 'week', 'gameID', 'gameDate', 'name', 'team', 'abbr', 'homeAway', 'opponent', 'avg_over_under', 'avg_team_total', 'average_spread']\n",
    "    filtered_df = filtered_df[column_order]\n",
    "\n",
    "    # Display the final filtered DataFrame\n",
    "    print(\"Filtered DataFrame with avg O/U >= 45, avg team total >= 23, and avg spread <= -7:\\n\")\n",
    "    display(filtered_df)\n",
    "\n",
    "    # Save the filtered DataFrame to a CSV file\n",
    "    output_csv_file = 'qb_teamTotal_23pts_OU_45_7pt_fav.csv'\n",
    "    filtered_df.to_csv(output_csv_file, index=False)\n",
    "    print(f\"Filtered DataFrame successfully written to {output_csv_file}\")\n",
    "\n",
    "    return filtered_df\n",
    "    \n",
    "# Call the new function on the modified DataFrame\n",
    "qb_team_total_7pt_fav_df = qb_teamTotal_23pts_OU_45_7pt_favorite(modified_qb_betting_lines_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8bbac8-00e6-4ed8-bacc-1b508d084474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
